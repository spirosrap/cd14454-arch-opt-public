{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Accelerate small language model inference with TensorRT optimization\n",
    "\n",
    "Most PyTorch models run far below GPU hardware capacity due to generic execution patterns that don't leverage hardware-specific optimizations. [TensorRT](https://developer.nvidia.com/tensorrt) addresses this on NVIDIA GPUs by creating optimized execution engines for your model specs.\n",
    "\n",
    "> **Overview**: An AI-powered platform is struggling with GPU utilization efficiency. While their model provides excellent response quality, the current PyTorch inference pipeline operates at a throughput far below the hardware's theoretical capacity.\n",
    "> \n",
    "> **Scenario**: You work for a customer service platform that processes 50,000+ support inquiries daily across multiple languages. Current infrastructure costs are unsustainable due to poor GPU utilization, and response times during peak hours exceed SLA requirements. Your goal is to achieve 3,000+ samples/sec throughput through TensorRT optimization to reduce infrastructure needs from 10 T4 instances to 3.\n",
    "> \n",
    "> **Goal**: Implement a TensorRT's build-time optimization workflow, leverage mixed precision and dynamic batching, and measure performance improvements to understand how hardware acceleration frameworks unlock production deployment efficiency.\n",
    "> \n",
    "> **Tools**: transformers, torch, tensorrt, onnx, pycuda, datasets\n",
    "> \n",
    "> **Estimated Time**: 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Let's establish our environment and verify T4 capabilities for TensorRT optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out and restart notebook\n",
    "# ! pip install transformers torch tensorrt onnx onnxruntime-gpu pycuda datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, set_seed\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import onnx\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"assets/exercise1\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla T4\n",
      "Compute Capability: 7.5\n",
      "Total Memory: 15.6 GB\n",
      "Multiprocessors: 40\n",
      "Tensor Core Support: ✓ Available\n",
      "TensorRT Version: 10.3.0\n",
      "  → Mixed precision (FP16) will provide significant speedup\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility  \n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Verify T4 setup and TensorRT compatibility\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "    print(f\"Total Memory: {gpu_properties.total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Multiprocessors: {gpu_properties.multi_processor_count}\")\n",
    "    \n",
    "    # Check TensorRT compatibility\n",
    "    tensor_cores_available = gpu_properties.major >= 7\n",
    "    print(f\"Tensor Core Support: {'✓ Available' if tensor_cores_available else '✗ Not Available'}\")\n",
    "    print(f\"TensorRT Version: {trt.__version__}\")\n",
    "    \n",
    "    if tensor_cores_available:\n",
    "        print(\"  → Mixed precision (FP16) will provide significant speedup\")\n",
    "else:\n",
    "    print(\"CUDA not available - exercise requires GPU\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **T4 hardware context:** T4 GPUs feature 2,560 CUDA cores, 320 Tensor Cores, and 320 GB/s memory bandwidth. \n",
    "> \n",
    "> The Tensor Cores provide specialized acceleration for FP16 matrix operations, which TensorRT leverages through mixed precision optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load baseline model and data\n",
    "\n",
    "For this exercise, we'll use the following model and dataset, respectively:\n",
    "- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) provides a good balance of model complexity and inference speed for demonstrating TensorRT optimizations\n",
    "- A 2500-samples subset of [IMDB movie reviews dataset](https://huggingface.co/datasets/stanfordnlp/imdb) provides realistic text with natural length variability for demonstrating TensorRT's dynamic batching capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: distilbert-base-uncased\n",
      "Model parameters: 66,955,010\n",
      "Model size: 255.4 MB\n",
      "Model memory: 256.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = \"[PAD]\"\n",
    "\n",
    "# Load model in evaluation mode\n",
    "pytorch_model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "pytorch_model.eval()\n",
    "\n",
    "pytorch_model_params = sum(p.numel() for p in pytorch_model.parameters())\n",
    "pytorch_model_size = sum(p.numel() * p.element_size() for p in pytorch_model.parameters()) / 1024**2\n",
    "pytorch_model_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {pytorch_model_params:,}\")\n",
    "print(f\"Model size: {pytorch_model_size:.1f} MB\")\n",
    "print(f\"Model memory: {pytorch_model_memory:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset for realistic inference benchmarking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 2500 movie reviews (subsampled from 25000 total)\n",
      "Sample length distribution:\n",
      "  Min length: 35 tokens\n",
      "  Max length: 1018 tokens\n",
      "  Average length: 330.0 tokens\n",
      "  Median length: 257.5 tokens\n",
      "Dataset prepared:\n",
      "Total samples: 2500\n",
      "Input shape per sample: torch.Size([2500, 64])\n",
      "DataLoaders created for batch sizes: [16, 32, 64, 128]\n"
     ]
    }
   ],
   "source": [
    "# Load a subset of IMDB movie reviews for benchmarking\n",
    "print(\"Loading IMDB dataset for realistic inference benchmarking...\")\n",
    "dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "sample_texts = dataset['text'][:2500]  # Subsample from 25K test samples for efficient benchmarking\n",
    "\n",
    "print(f\"Dataset loaded: {len(sample_texts)} movie reviews (subsampled from {len(dataset)} total)\")\n",
    "\n",
    "# Analyze length distribution\n",
    "review_lengths = [len(tokenizer.encode(text)) for text in sample_texts[:100]]\n",
    "print(f\"Sample length distribution:\")\n",
    "print(f\"  Min length: {min(review_lengths)} tokens\")\n",
    "print(f\"  Max length: {max(review_lengths)} tokens\") \n",
    "print(f\"  Average length: {np.mean(review_lengths):.1f} tokens\")\n",
    "print(f\"  Median length: {np.median(review_lengths):.1f} tokens\")\n",
    "\n",
    "# Tokenize with different sequence lengths to test dynamic batching\n",
    "def prepare_input_tensors(texts, max_length=64, batch_size=32):\n",
    "    \"\"\"Tokenize texts and create batched tensors\"\"\"\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(\n",
    "        encoded['input_ids'],\n",
    "        encoded['attention_mask']\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return dataloader, encoded['input_ids'].shape\n",
    "\n",
    "# Create dataloaders for different batch sizes\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "dataloaders = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    dataloaders[batch_size], input_shape = prepare_input_tensors(\n",
    "        sample_texts, max_length=64, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "print(f\"Dataset prepared:\")\n",
    "print(f\"Total samples: {len(sample_texts)}\")\n",
    "print(f\"Input shape per sample: {input_shape}\")\n",
    "print(f\"DataLoaders created for batch sizes: {batch_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dataset characteristics impact on optimization**: The IMDB reviews show natural length variability (35-1018 tokens), but we truncate to 64 tokens for consistent fast comparison, but in practice 256+ tokens are more commonly used for this use case.  \n",
    "> \n",
    "> Advanced implementations support variable sequence lengths which further benefit from TensorRT's optimization profiles that can configure pre-optimizing kernels for common lengths while dynamically adapting memory allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Measure baseline PyTorch performance\n",
    "\n",
    "Before optimizing with TensorRT, let's establish baseline performance to measure improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pytorch_model(model, dataloader, num_batches=20, warmup_batches=5):\n",
    "    \"\"\"Measure PyTorch model performance\"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "            if i >= warmup_batches:\n",
    "                break\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            _ = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Benchmark\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            \n",
    "            # Memory tracking\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            baseline_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "            \n",
    "            # Time measurement\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            # Record metrics\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "            times.append(end_time - start_time)\n",
    "            memory_usage.append(peak_memory - baseline_memory)\n",
    "    \n",
    "    return times, memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring PyTorch baseline performance...\n",
      "Batch 16: 680.5 samples/sec (23.5ms) | 33.0 MB\n",
      "Batch 32: 689.6 samples/sec (46.4ms) | 66.0 MB\n",
      "Batch 64: 732.3 samples/sec (87.4ms) | 132.0 MB\n",
      "Batch 128: 720.5 samples/sec (177.7ms) | 257.9 MB\n",
      "\n",
      "Baseline established! Best throughput: 732.3 samples/sec\n"
     ]
    }
   ],
   "source": [
    "# Measure baseline performance across batch sizes\n",
    "baseline_results = {}\n",
    "\n",
    "print(\"Measuring PyTorch baseline performance...\")\n",
    "for batch_size in batch_sizes:\n",
    "    times, memory = benchmark_pytorch_model(pytorch_model, dataloaders[batch_size])\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    throughput = batch_size / avg_time\n",
    "    avg_memory = np.mean(memory)\n",
    "    \n",
    "    baseline_results[batch_size] = {\n",
    "        'avg_time': avg_time,\n",
    "        'throughput': throughput,\n",
    "        'latency_ms': avg_time * 1000,\n",
    "        'memory_mb': avg_memory\n",
    "    }\n",
    "    \n",
    "    print(f\"Batch {batch_size}: {throughput:.1f} samples/sec ({avg_time*1000:.1f}ms) | {avg_memory:.1f} MB\")\n",
    "\n",
    "print(f\"\\nBaseline established! Best throughput: {max(r['throughput'] for r in baseline_results.values()):.1f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Insights from baseline performance analysis:** The baseline shows typical PyTorch scaling patterns where throughput increases modestly with batch size while memory usage doubles until memory bandwidth becomes the bottleneck. \n",
    "> \n",
    "> Notice how batch 128 actually performs worse than batch 64, indicating we're hitting T4's memory limits and experiencing memory pressure that degrades performance. TensorRT addresses this through optimized memory layouts, layer fusion, and precision management that better utilize T4's architectural capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement the TensorRT conversion workflow\n",
    "\n",
    "TensorRT optimization happens in two phases: build-time optimization that analyzes and restructures the model, and runtime execution with the optimized engine.\n",
    "\n",
    "> _**Note on the ONNX intermediate format:**_ [ONNX (Open Neural Network Exchange)](https://onnx.ai/) serves as a standard model format that enables different optimization engines to understand PyTorch architectures. TensorRT is one such engine that specializes in NVIDIA GPU optimization—it reads ONNX models and generates hardware-specific kernels, memory layouts, and execution strategies optimized for your target GPU's architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pytorch_to_onnx(model, input_shape, onnx_path):\n",
    "    \"\"\"Convert PyTorch model to ONNX format\"\"\"\n",
    "    print(\"Step 1: Converting PyTorch → ONNX...\")\n",
    "    \n",
    "    # TODO: Create dummy inputs for ONNX export\n",
    "    # HINT: ONNX export needs sample inputs to trace the computation graph on the expected device\n",
    "    # Use torch.randint to create realistic token IDs (0 to vocab_size) and torch.ones to create attention masks\n",
    "    vocab_size = model.config.vocab_size\n",
    "    dummy_input_ids = torch.randint(0, vocab_size, input_shape, device=device)  # Add your code here\n",
    "    dummy_attention_mask = torch.ones(input_shape, device=device)  # Add your code here\n",
    "    \n",
    "    # Export to ONNX with dynamic axes for batching\n",
    "    # No optimizations done here, we leave them for TensorRT\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_input_ids, dummy_attention_mask),\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input_ids', 'attention_mask'],\n",
    "        output_names=['logits'],\n",
    "        dynamic_axes={\n",
    "            'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "            'attention_mask': {0: 'batch_size', 1: 'sequence_length'},  \n",
    "            'logits': {0: 'batch_size', 1: 'sequence_length'}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(f\"✓ ONNX model saved and verified: {onnx_path}\")\n",
    "    return onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensorrt_engine(onnx_path, engine_path, precision='fp16', min_batch=1, opt_batch=32, max_batch=128):\n",
    "    \"\"\"Build TensorRT engine from ONNX model\"\"\"\n",
    "    print(f\"Step 2: Building TensorRT engine with {precision} precision...\")\n",
    "    \n",
    "    # TODO: Initialize TensorRT logger, builder, network, and parser\n",
    "    # HINT: The network_flags allows you to add support for dynamic shapes\n",
    "    # Reference: https://docs.nvidia.com/deeplearning/tensorrt-rtx/latest/inference-library/python-api-docs.html\n",
    "    logger = trt.Logger(trt.Logger.WARNING)  # Add your code here\n",
    "    builder = trt.Builder(logger)  # Add your code here\n",
    "    network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)  # Add your code here\n",
    "    network = builder.create_network(network_flags)  # Add your code here\n",
    "    parser = trt.OnnxParser(network, logger)  # Add your code here\n",
    "    \n",
    "    # Parse ONNX model\n",
    "    with open(onnx_path, 'rb') as f:\n",
    "        parser.parse(f.read())\n",
    "    \n",
    "    # TODO: Configure builder settings for optimization\n",
    "    # HINT: BuilderConfig controls optimization strategies and precision settings\n",
    "    # At minimum, set the following config attributes: memory pool limit (use 4GB for T4) and fp16 precision if requests\n",
    "    # References: \n",
    "    # - https://developer.nvidia.com/docs/drive/drive-os/6.0.7/public/drive-os-tensorrt/api-reference/docs/python/infer/Core/Builder.html\n",
    "    # - https://developer.nvidia.com/docs/drive/drive-os/6.0.7/public/drive-os-tensorrt/api-reference/docs/python/infer/Core/BuilderConfig.html#tensorrt.IBuilderConfig\n",
    "    config = builder.create_builder_config()  # Add your code here\n",
    "\n",
    "    # Add your code here (to set up config attributes)\n",
    "    \n",
    "    # Set memory limit (use 4GB for T4)\n",
    "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)\n",
    "    \n",
    "    # Configure precision\n",
    "    if precision == 'fp16':\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "    \n",
    "    # TODO: Configure dynamic batching with optimization profiles\n",
    "    # HINT: Optimization profiles tell TensorRT the range of input shapes to optimize for\n",
    "    # For our model, we need to set the shape for 'input_ids' and 'attention_mask'\n",
    "    # References: \n",
    "    # - https://developer.nvidia.com/docs/drive/drive-os/6.0.7/public/drive-os-tensorrt/api-reference/docs/python/infer/Core/Builder.html\n",
    "    # - https://docs.nvidia.com/deeplearning/tensorrt/10.8.0/_static/python-api/infer/Core/OptimizationProfile.html#tensorrt.IOptimizationProfile\n",
    "    profile = builder.create_optimization_profile()  # Add your code here\n",
    "    \n",
    "    # Add your code here (to set up profile attributes) \n",
    "\n",
    "    # Set input shape ranges for dynamic batching\n",
    "    profile.set_shape(\n",
    "        'input_ids',\n",
    "        (min_batch, 64), (opt_batch, 64), (max_batch, 64)  # (min, opt, max) shapes\n",
    "    )\n",
    "    profile.set_shape(\n",
    "        'attention_mask', \n",
    "        (min_batch, 64), (opt_batch, 64), (max_batch, 64)\n",
    "    )\n",
    "    config.add_optimization_profile(profile)\n",
    "    \n",
    "    # Build engine\n",
    "    print(\"Building engine... (this may take 1-2 minutes)\")\n",
    "    start_build = time.time()\n",
    "    \n",
    "    # TODO: Build the serialized engine\n",
    "    # Reference: https://developer.nvidia.com/docs/drive/drive-os/6.0.7/public/drive-os-tensorrt/api-reference/docs/python/infer/Core/Builder.html\n",
    "    serialized_engine = builder.build_serialized_network(network, config)  # Add your code here\n",
    "    build_time = time.time() - start_build\n",
    "    \n",
    "    # Save engine to disk\n",
    "    with open(engine_path, 'wb') as f:\n",
    "        f.write(serialized_engine)\n",
    "    \n",
    "    print(f\"✓ TensorRT engine built in {build_time:.1f}s: {engine_path}\")\n",
    "    return engine_path, build_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Converting PyTorch → ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py:215: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ONNX model saved and verified: assets/exercise1/distilbert.onnx\n",
      "Step 2: Building TensorRT engine with fp32 precision...\n",
      "[08/21/2025-15:41:11] [TRT] [W] ModelImporter.cpp:420: Make sure input input_ids has Int64 binding.\n",
      "Building engine... (this may take 1-2 minutes)\n",
      "✓ TensorRT engine built in 7.9s: assets/exercise1/distilbert_fp32.trt\n",
      "Step 2: Building TensorRT engine with fp16 precision...\n",
      "[08/21/2025-15:41:22] [TRT] [W] ModelImporter.cpp:420: Make sure input input_ids has Int64 binding.\n",
      "Building engine... (this may take 1-2 minutes)\n",
      "[08/21/2025-15:41:22] [TRT] [W] Detected layernorm nodes in FP16.\n",
      "[08/21/2025-15:41:22] [TRT] [W] Running layernorm after self-attention with FP16 Reduce or Pow may cause overflow. Forcing Reduce or Pow Layers in FP32 precision, or exporting the model to use INormalizationLayer (available with ONNX opset >= 17) can help preserving accuracy.\n",
      "✓ TensorRT engine built in 51.4s: assets/exercise1/distilbert_fp16.trt\n",
      "\n",
      "=== Conversion Summary ===\n",
      "PyTorch → ONNX: ✓\n",
      "ONNX → TensorRT FP32: ✓ (7.9s)\n",
      "ONNX → TensorRT FP16: ✓ (51.4s)\n"
     ]
    }
   ],
   "source": [
    "# Convert model to ONNX and build TensorRT engines\n",
    "onnx_path = os.path.join(output_dir, \"distilbert.onnx\")\n",
    "fp32_engine_path = os.path.join(output_dir, \"distilbert_fp32.trt\")\n",
    "fp16_engine_path = os.path.join(output_dir, \"distilbert_fp16.trt\")\n",
    "\n",
    "# Conversion pipeline\n",
    "input_shape = (32, 64)  # (batch_size, sequence_length)\n",
    "convert_pytorch_to_onnx(pytorch_model, input_shape, onnx_path)\n",
    "\n",
    "# Build engines for both precisions\n",
    "fp32_engine, fp32_build_time = build_tensorrt_engine(\n",
    "    onnx_path, fp32_engine_path, precision='fp32', \n",
    "    min_batch=16, opt_batch=32, max_batch=128\n",
    ")\n",
    "fp16_engine, fp16_build_time = build_tensorrt_engine(\n",
    "    onnx_path, fp16_engine_path, precision='fp16',\n",
    "    min_batch=16, opt_batch=32, max_batch=128  \n",
    ")\n",
    "\n",
    "print(f\"\\n=== Conversion Summary ===\")\n",
    "print(f\"PyTorch → ONNX: ✓\")\n",
    "print(f\"ONNX → TensorRT FP32: ✓ ({fp32_build_time:.1f}s)\")\n",
    "print(f\"ONNX → TensorRT FP16: ✓ ({fp16_build_time:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What happens at TensorRT build-time?** TensorRT's build phase analyzes the model architecture and generates hardware-optimized kernels. \n",
    "> \n",
    "> This one-time cost _(~60 seconds)_ pays dividends through runtime performance gains. \n",
    "> \n",
    "> The build process includes layer fusion (combining multiple operations into single GPU kernels), precision optimization (using FP16 where safe), and memory layout planning.  specifically for your target hardware.\n",
    "> \n",
    "> _**About the build warnings:**_ The warnings you see are normal and demonstrate TensorRT's intelligent optimization where you get FP16 performance benefits with FP32 accuracy protection where it matters most:\n",
    "> - _\"Make sure input input_ids has Int64 binding\"_ --> TensorRT automatically handles token ID type conversions for optimal GPU execution\n",
    "> \n",
    "> - _\"Detected layernorm nodes in FP16\"_ --> TensorRT keeps numerically sensitive operations (LayerNorm, softmax) in FP32 while running other layers in FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the TensorRT inference engine\n",
    "\n",
    "Now let's implement the TensorRT inference engine with dynamic batch size support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorRTInference:\n",
    "    \"\"\"TensorRT inference engine wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, engine_path):\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        \n",
    "        # Load and deserialize engine\n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        \n",
    "        runtime = trt.Runtime(self.logger)\n",
    "        self.engine = runtime.deserialize_cuda_engine(engine_data)\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Pre-allocate for maximum batch size to avoid memory issues\n",
    "        max_batch_size = 128\n",
    "        seq_length = 64\n",
    "        \n",
    "        # Allocate input memory\n",
    "        input_ids_size = max_batch_size * seq_length\n",
    "        attention_mask_size = max_batch_size * seq_length\n",
    "        output_size = max_batch_size * 2  # Binary classification\n",
    "        \n",
    "        # TODO: Allocate pinned host memory for efficient async GPU transfers\n",
    "        # HINT: You can use PyCUDA for this\n",
    "        # Reference: https://documen.tician.de/pycuda/driver.html#pagelocked-host-memory\n",
    "        self.input_ids_host = cuda.pagelocked_empty(input_ids_size, np.int64)  # Add your code here\n",
    "        self.attention_mask_host = cuda.pagelocked_empty(attention_mask_size, np.int64)  # Add your code here\n",
    "        self.output_host = cuda.pagelocked_empty(output_size, np.float32)  # Add your code here\n",
    "        \n",
    "        # Device memory\n",
    "        self.input_ids_device = cuda.mem_alloc(self.input_ids_host.nbytes)\n",
    "        self.attention_mask_device = cuda.mem_alloc(self.attention_mask_host.nbytes)\n",
    "        self.output_device = cuda.mem_alloc(self.output_host.nbytes)\n",
    "        \n",
    "        # Create CUDA stream\n",
    "        self.stream = cuda.Stream()\n",
    "\n",
    "        # Get TensorRT's built-in memory reporting\n",
    "        self.engine_memory_mb = self.engine.device_memory_size / 1024**2\n",
    "    \n",
    "    def infer(self, input_ids, attention_mask):\n",
    "        \"\"\"Run inference with dynamic batch size\"\"\"\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # TODO: Set dynamic input shapes for current batch\n",
    "        # HINT: TensorRT needs to know the actual input shape for each inference, for both inputs: 'input_ids' and 'attention_mask'\n",
    "        # Set it via self.context\n",
    "        # Reference: https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/work-dynamic-shapes.html\n",
    "\n",
    "        # Add your code here\n",
    "        self.context.set_input_shape(\"input_ids\", input_ids.shape)\n",
    "        self.context.set_input_shape(\"attention_mask\", attention_mask.shape)\n",
    "       \n",
    "        # Copy inputs to pre-allocated host memory\n",
    "        input_ids_flat = input_ids.cpu().numpy().ravel()\n",
    "        attention_mask_flat = attention_mask.cpu().numpy().ravel()\n",
    "        \n",
    "        self.input_ids_host[:len(input_ids_flat)] = input_ids_flat\n",
    "        self.attention_mask_host[:len(attention_mask_flat)] = attention_mask_flat\n",
    "\n",
    "        # Transfer input data to GPU\n",
    "        cuda.memcpy_htod_async(self.input_ids_device, self.input_ids_host, self.stream)\n",
    "        cuda.memcpy_htod_async(self.attention_mask_device, self.attention_mask_host, self.stream)\n",
    "        \n",
    "        # TODO: Bind tensor memory addresses for TensorRT execution\n",
    "        # HINT: TensorRT needs to know where to find input/output tensors in GPU memory via their tensor address\n",
    "        # Set three entries (input_ids, attention_mask, logits) for self.context\n",
    "        # Reference: https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-1001/api/python_api/infer/Core/ExecutionContext.html\n",
    "\n",
    "        # Add your code here\n",
    "        self.context.set_tensor_address(\"input_ids\", self.input_ids_device)\n",
    "        self.context.set_tensor_address(\"attention_mask\", self.attention_mask_device)\n",
    "        self.context.set_tensor_address(\"logits\", self.output_device)\n",
    "        \n",
    "        # Run inference\n",
    "        self.context.execute_async_v3(stream_handle=self.stream.handle)\n",
    "        \n",
    "        # Transfer input data to GPU asynchronously\n",
    "        # HINT: You can use PyCUDA for this, look for device-to-host (_d_to_h_) transfer with an _async prefix\n",
    "        # Reference: https://documen.tician.de/pycuda/driver.html#unstructured-memory-transfers\n",
    "\n",
    "        # Add your code here\n",
    "        cuda.memcpy_dtoh_async(self.output_host, self.output_device, self.stream)\n",
    "\n",
    "        self.stream.synchronize()\n",
    "        \n",
    "        # Reshape output\n",
    "        output_elements = batch_size * 2  # Number of elements\n",
    "        output = self.output_host[:output_elements].reshape(batch_size, 2)\n",
    "        \n",
    "        return torch.tensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TensorRT engines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15558/2646212795.py:40: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by get_device_memory_size_v2 instead.\n",
      "  self.engine_memory_mb = self.engine.device_memory_size / 1024**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TensorRT engines loaded and ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize TensorRT inference engines\n",
    "print(\"Loading TensorRT engines...\")\n",
    "trt_fp32_engine = TensorRTInference(fp32_engine_path)\n",
    "trt_fp16_engine = TensorRTInference(fp16_engine_path)\n",
    "print(\"✓ TensorRT engines loaded and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TensorRT runtime engine efficiency**: TensorRT's runtime engine uses pre-allocated memory pools and asynchronous GPU operations to minimize inference overhead. \n",
    "> \n",
    "> - The dynamic shape setting allows the same engine to handle different batch sizes efficiently without rebuilding. \n",
    "> - The pre-allocated memory approach eliminates allocation overhead during inference, which is crucial for achieving consistent low-latency performance in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Perform comprehensive performance benchmarking\n",
    "\n",
    "Let's benchmark all configurations to measure TensorRT's optimization impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_tensorrt_engine(trt_engine, dataloader, num_batches=20, warmup_batches=3):\n",
    "    \"\"\"Benchmark TensorRT engine performance\"\"\"\n",
    "    times = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    # Warmup\n",
    "    for i, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "        if i >= warmup_batches:\n",
    "            break\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        _ = trt_engine.infer(input_ids, attention_mask)\n",
    "    \n",
    "    # Benchmark\n",
    "    for i, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "            \n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        \n",
    "        # Time measurement  \n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        outputs = trt_engine.infer(input_ids, attention_mask)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        # Record metrics\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    # TODO: Define memory usage\n",
    "    # HINT: Use TensorRT's built-in memory reporting, and convert to MB \n",
    "    # Reference: https://docs.nvidia.com/deeplearning/tensorrt/latest/_static/python-api/infer/Core/Engine.html\n",
    "    memory_usage_engine = [trt_engine.engine.device_memory_size / 1024**2]  # Add your code here\n",
    "    memory_usage = memory_usage_engine * len(times)  # Create one entry per time measurement to match sizes\n",
    "    \n",
    "    return times, memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking TensorRT configurations...\n",
      "\n",
      "Benchmarking TensorRT FP32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15558/4219651796.py:35: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by get_device_memory_size_v2 instead.\n",
      "  memory_usage_engine = [trt_engine.engine.device_memory_size / 1024**2]  # Add your code here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 16: 652.8 samples/sec (24.5ms) | 264.0 MB\n",
      "  Batch 32: 696.3 samples/sec (46.0ms) | 264.0 MB\n",
      "  Batch 64: 683.1 samples/sec (93.7ms) | 264.0 MB\n",
      "  Batch 128: 679.7 samples/sec (188.3ms) | 264.0 MB\n",
      "\n",
      "Benchmarking TensorRT FP16...\n",
      "  Batch 16: 3487.6 samples/sec (4.6ms) | 138.0 MB\n",
      "  Batch 32: 3825.7 samples/sec (8.4ms) | 138.0 MB\n",
      "  Batch 64: 3672.9 samples/sec (17.4ms) | 138.0 MB\n",
      "  Batch 128: 3662.0 samples/sec (35.0ms) | 138.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Benchmark all configurations\n",
    "configurations = [\n",
    "    (\"PyTorch FP32\", \"baseline\"),\n",
    "    (\"TensorRT FP32\", trt_fp32_engine), \n",
    "    (\"TensorRT FP16\", trt_fp16_engine)\n",
    "]\n",
    "\n",
    "all_results = {\"PyTorch FP32\": baseline_results}\n",
    "\n",
    "print(\"Benchmarking TensorRT configurations...\")\n",
    "\n",
    "for config_name, engine in configurations[1:]:  # Skip baseline (already measured)\n",
    "    print(f\"\\nBenchmarking {config_name}...\")\n",
    "    config_results = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        times, memory = benchmark_tensorrt_engine(engine, dataloaders[batch_size])\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        throughput = batch_size / avg_time\n",
    "        avg_memory = np.mean(memory)\n",
    "        \n",
    "        config_results[batch_size] = {\n",
    "            'avg_time': avg_time,\n",
    "            'throughput': throughput, \n",
    "            'latency_ms': avg_time * 1000,\n",
    "            'memory_mb': avg_memory\n",
    "        }\n",
    "        \n",
    "        print(f\"  Batch {batch_size}: {throughput:.1f} samples/sec ({avg_time*1000:.1f}ms) | {avg_memory:.1f} MB\")\n",
    "    \n",
    "    all_results[config_name] = config_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TensorRT Optimization Impact Analysis ===\n",
      "\n",
      "Throughput Analysis: \n",
      "\n",
      "\t--- Batch Size: 16 ---\n",
      "\t\tTensorRT FP32 optimization: 0.96x throughput improvement\n",
      "\t\tTensorRT FP16 optimization: 5.12x throughput improvement\n",
      "\t\t→ Mixed precision benefit: 5.34x additional gain\n",
      "\n",
      "\t--- Batch Size: 32 ---\n",
      "\t\tTensorRT FP32 optimization: 1.01x throughput improvement\n",
      "\t\tTensorRT FP16 optimization: 5.55x throughput improvement\n",
      "\t\t→ Mixed precision benefit: 5.49x additional gain\n",
      "\n",
      "\t--- Batch Size: 64 ---\n",
      "\t\tTensorRT FP32 optimization: 0.93x throughput improvement\n",
      "\t\tTensorRT FP16 optimization: 5.02x throughput improvement\n",
      "\t\t→ Mixed precision benefit: 5.38x additional gain\n",
      "\n",
      "\t--- Batch Size: 128 ---\n",
      "\t\tTensorRT FP32 optimization: 0.94x throughput improvement\n",
      "\t\tTensorRT FP16 optimization: 5.08x throughput improvement\n",
      "\t\t→ Mixed precision benefit: 5.39x additional gain\n",
      "\n",
      "Memory Analysis: \n",
      "\n",
      "\tPyTorch FP32 Model (Weights only): 256.5 MB\n",
      "\tPyTorch peak activation memory: from 33.0 MB for batch_size 128 to 257.9 MB for batch_size 128 (scales with batch size)\n",
      "\tTensorRT FP32 Engine (Weights + Activation Workspace): 264.0 MB\n",
      "\tTensorRT FP16 Engine (Weights + Activation Workspace): 138.0 MB\n",
      "\n",
      "\t→ The FP16 engine reduces the static memory footprint by 73.2% for batch_size 128.\n"
     ]
    }
   ],
   "source": [
    "# Calculate optimization improvements for each batch size\n",
    "print(f\"\\n=== TensorRT Optimization Impact Analysis ===\")\n",
    "\n",
    "# Throughput Analysis\n",
    "print(\"\\nThroughput Analysis: \")\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\n\\t--- Batch Size: {batch_size} ---\")\n",
    "\n",
    "    baseline_batch = baseline_results[batch_size]\n",
    "    trt_fp32_batch = all_results[\"TensorRT FP32\"][batch_size]\n",
    "    trt_fp16_batch = all_results[\"TensorRT FP16\"][batch_size]\n",
    "\n",
    "    fp32_improvement = trt_fp32_batch['throughput'] / baseline_batch['throughput']\n",
    "    fp16_improvement = trt_fp16_batch['throughput'] / baseline_batch['throughput']\n",
    "\n",
    "    print(f\"\\t\\tTensorRT FP32 optimization: {fp32_improvement:.2f}x throughput improvement\")\n",
    "    print(f\"\\t\\tTensorRT FP16 optimization: {fp16_improvement:.2f}x throughput improvement\") \n",
    "    print(f\"\\t\\t→ Mixed precision benefit: {fp16_improvement/fp32_improvement:.2f}x additional gain\")\n",
    "\n",
    "# Memory Analysis\n",
    "print(\"\\nMemory Analysis: \")\n",
    "trt_fp32_memory = trt_fp32_engine.engine_memory_mb\n",
    "trt_fp16_memory = trt_fp16_engine.engine_memory_mb\n",
    "\n",
    "print(f\"\\n\\tPyTorch FP32 Model (Weights only): {pytorch_model_memory:.1f} MB\")\n",
    "print(f\"\\tPyTorch peak activation memory: from {baseline_results[batch_sizes[0]]['memory_mb']:.1f} MB for batch_size {batch_sizes[-1]} to {baseline_results[batch_sizes[-1]]['memory_mb']:.1f} MB for batch_size {batch_sizes[-1]} (scales with batch size)\")\n",
    "print(f\"\\tTensorRT FP32 Engine (Weights + Activation Workspace): {trt_fp32_memory:.1f} MB\")\n",
    "print(f\"\\tTensorRT FP16 Engine (Weights + Activation Workspace): {trt_fp16_memory:.1f} MB\")\n",
    "\n",
    "pytorch_max_memory = pytorch_model_memory + baseline_results[batch_sizes[-1]]['memory_mb']\n",
    "reduction_percentage = (pytorch_max_memory - trt_fp16_memory) / pytorch_max_memory * 100\n",
    "print(f\"\\n\\t→ The FP16 engine reduces the static memory footprint by {reduction_percentage:.1f}% for batch_size {batch_sizes[-1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TensorRT memory management**: Unlike PyTorch's dynamic memory allocation that scales with batch size, TensorRT pre-allocates memory based on your optimization profile's maximum batch size (128 in our case). \n",
    "> \n",
    "> This strategy trades memory efficiency for performance consistency—no allocation overhead during inference, but you pay the memory cost of your largest expected batch size regardless of actual usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "> **TODO: TensorRT Optimization Analysis**\n",
    ">\n",
    "> Using your benchmark results from the previous analysis, answer these questions to test your understanding of TensorRT's core optimization mechanisms:\n",
    "> \n",
    "> \n",
    "> 1. **Memory Management Understanding**\n",
    ">    - Given that TensorRT pre-allocates memory for the maximum batch size, what are the production implications if you set max_batch=256 but typically run batches of 32?\n",
    ">    - In what deployment scenarios would TensorRT's approach be better than PyTorch's dynamic allocation, and when might PyTorch's approach be preferable?\n",
    ">    - Answer: _Setting max_batch=256 while running batches of 32 wastes 7x memory allocation, reducing GPU efficiency and increasing costs. TensorRT's pre-allocation works best for predictable production workloads where eliminating allocation overhead matters more than memory efficiency. PyTorch's dynamic allocation is better for research environments, memory-constrained deployments, or unpredictable traffic patterns where flexibility outweighs performance._\n",
    "> \n",
    "> \n",
    "> 2. **Mixed Precision Impact** \n",
    ">    - Compare the performance improvement from TensorRT FP32 vs TensorRT FP16. Why do you see this difference?\n",
    ">    - What specific T4 hardware feature enables the larger performance boost with FP16?\n",
    ">    - Answer: _TensorRT FP32 shows minimal improvement because it only optimizes execution without changing computation precision. TensorRT FP16 delivers major gains by leveraging T4's Tensor Cores—specialized hardware units for accelerated FP16 matrix operations that aren't used in FP32 mode. This unlocks dedicated hardware acceleration for transformer operations._\n",
    "> \n",
    "> 3. **Build-time vs Runtime**\n",
    ">    - TensorRT required a build step while PyTorch loads instantly. When does this trade-off make business sense?\n",
    ">    - Why can't PyTorch achieve similar runtime performance without this build step?\n",
    ">    - Answer: _The build step makes sense for high-throughput production where you amortize the 60-second cost across thousands of inferences. PyTorch uses generic CUDA kernels for broad compatibility, while TensorRT analyzes your specific architecture to generate fused, hardware-optimized kernels tailored to your GPU—analysis that's too intensive for load time._\n",
    "> \n",
    "> 4. **Dynamic Batching Behavior**\n",
    ">    - Your optimization profile was set to min=16, opt=32, max=128. How does TensorRT handle batch sizes within this range?\n",
    ">    - What would happen if you tried to run a batch size outside this range?\n",
    ">    - Answer: _Within 16-128, TensorRT efficiently handles any batch size with pre-optimized kernels, performing best at the optimal size of 32. Batch sizes above 128 would likely error or force expensive re-optimization. Sizes below 16 work but may be suboptimal since kernels weren't optimized for very small batches._\n",
    ">\n",
    "> 5. **Bonus challenge: What if `max_length` of the tokenizer changed from 64 to 256?**\n",
    ">     - How would you expect this to impact the performance *improvement factor* of the TensorRT FP16 engine over the baseline PyTorch model? Would the speedup become larger, smaller, or stay the same?\n",
    ">     - _HINT:_ Think about how a longer sequence length affects the computational workload (i.e., the size of the matrix multiplications). Which environment—the generic PyTorch framework or the hardware-specific TensorRT engine—is better at capitalizing on more intensive, parallelizable work?\n",
    ">     - Answer: _The speedup would likely increase. Longer sequences create more intensive matrix multiplications in attention mechanisms, and TensorRT's hardware optimizations scale better with larger computational workloads than PyTorch's generic execution. The additional parallelizable work could increase speedup from ~5x to 6-8x._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've experienced TensorRT's complete optimization workflow and measured its real-world performance impact on T4 hardware.\n",
    "\n",
    "TensorRT demonstrates how hardware acceleration frameworks transform model deployment feasibility through systematic build-time optimization and runtime efficiency gains. TensorRT success factors include:\n",
    "- Hardware-specific optimization unlocks true GPU potential\n",
    "- Dynamic batching adapts to variable workload patterns\n",
    "- One-time build cost scales across all production inferences\n",
    "- Mixed precision leverages T4 Tensor Cores effectively\n",
    "\n",
    "##### **Next optimization challenges to explore:**\n",
    "\n",
    "- Explore **TensorRT-LLM** to leverage LLM-specific optimizations for faster autoregressive generation.\n",
    "\n",
    "- Implement **dynamic shape optimization** to handle variable sequence lengths efficiently without reallocating memory.\n",
    "\n",
    "- Optimize **layer fusion** (like attention and activation layers) to minimize kernel launches and improve throughput.\n",
    "\n",
    "- Use **TensorRT’s auto-tuning** to select the most efficient kernels for each layer type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
