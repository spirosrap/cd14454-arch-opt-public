{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Apply cross-platform hardware optimizations with ONNX Runtime\n",
    "\n",
    "[ONNX Runtime](https://onnxruntime.ai/) stands as the industry standard for hardware-agnostic AI deployment, enabling developers to optimize models across diverse hardware platforms—from NVIDIA GPUs to Intel CPUs to mobile processors—using a single framework with consistent APIs, while still leveraging hardware-specific acceleration libraries for production-grade performance.\n",
    "\n",
    "> **Overview:** You need to deploy a model across multiple cloud providers (AWS, Azure, GCP) and on-premise servers with different hardware configurations, with consistent performance and without vendor lock-in.\n",
    "> \n",
    "> **Scenario:** Your startup's image classification service processes 100,000+ images daily across heterogeneous infrastructure. The DevOps team reports suboptimal performance: your PyTorch model achieves only 60% GPU utilization, while CPU-only deployments are bottlenecked by poor threading configuration. You need to maximize performance on each platform while avoiding vendor lock-in and maintaining deployment simplicity.\n",
    "> \n",
    "> **Goal:** Explore ONNX Runtime's cross-platform optimization capabilities including I/O binding, thread management, memory optimization, and execution provider selection to achieve consistent performance across diverse hardware environments.\n",
    "> \n",
    "> **Tools:** torch, torchvision, onnx, onnxruntime-gpu, numpy, pillow\n",
    "> \n",
    "> **Estimated Time:** 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Let's establish your cross-platform testing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out and restart\n",
    "# ! pip install onnx onnxruntime-gpu==1.19.2 torchvision pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"assets/exercise3\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CROSS-PLATFORM DEPLOYMENT ENVIRONMENT ===\n",
      "PyTorch version: 2.5.0+cu124\n",
      "ONNX version: 1.18.0\n",
      "ONNX Runtime version: 1.19.2\n",
      "\n",
      "=== NVIDIA T4 HARDWARE ANALYSIS ===\n",
      "GPU: Tesla T4\n",
      "Compute Capability: 7.5\n",
      "Total Memory: 15.6 GB\n",
      "CUDA Cores: ~2,560\n",
      "Tensor Cores: ~320 (2nd gen)\n",
      "Memory Bandwidth: ~320 GB/s\n",
      "\n",
      "=== AVAILABLE EXECUTION PROVIDERS ===\n",
      "✓ TensorrtExecutionProvider\n",
      "✓ CUDAExecutionProvider\n",
      "✓ CPUExecutionProvider\n",
      "\n",
      "TensorRT Support: ✓ Available\n",
      "\n",
      "✓ Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CROSS-PLATFORM DEPLOYMENT ENVIRONMENT ===\")\n",
    "\n",
    "# Check PyTorch and ONNX versions\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX version: {onnx.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "\n",
    "# Verify GPU availability for cross-platform testing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    \n",
    "    print(f\"\\n=== NVIDIA T4 HARDWARE ANALYSIS ===\")\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "    print(f\"Total Memory: {gpu_properties.total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA Cores: ~2,560\")\n",
    "    print(f\"Tensor Cores: ~320 (2nd gen)\")\n",
    "    print(f\"Memory Bandwidth: ~320 GB/s\")\n",
    "    \n",
    "    # Check available execution providers\n",
    "    print(f\"\\n=== AVAILABLE EXECUTION PROVIDERS ===\")\n",
    "    available_providers = ort.get_available_providers()\n",
    "    for provider in available_providers:\n",
    "        print(f\"✓ {provider}\")\n",
    "    \n",
    "    # Verify TensorRT availability\n",
    "    tensorrt_available = 'TensorrtExecutionProvider' in available_providers\n",
    "    print(f\"\\nTensorRT Support: {'✓ Available' if tensorrt_available else '✗ Not Available'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"CUDA not available - some optimizations will be CPU-only\")\n",
    "\n",
    "print(\"\\n✓ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What do we mean with execution providers?** ONNX Runtime's execution provider system enables hardware abstraction by separating model optimization logic from hardware-specific implementation. \n",
    "> \n",
    "> Each provider _(CPU, CUDA, TensorRT, ...)_ uses the same ONNX graph but applies different optimization strategies—CPU providers focus on vectorization and threading, CUDA providers leverage parallel processing, while TensorRT providers add graph-level optimization and kernel fusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load model\n",
    "\n",
    "For this exercise, we'll use [EfficientNet-B0](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html) as it represents modern CNN architectures commonly deployed across platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EfficientNet-B0 for cross-platform optimization...\n",
      "Model loaded: EfficientNet\n",
      "Total parameters: 5,288,548\n",
      "Model size: 20.2 MB\n",
      "\n",
      "Sample input shape: torch.Size([32, 3, 224, 224])\n",
      "Input tensor size: 18.4 MB\n",
      "Output shape: torch.Size([32, 1000])\n",
      "Model successfully processes input ✓\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained EfficientNet model\n",
    "print(\"Loading EfficientNet-B0 for cross-platform optimization...\")\n",
    "\n",
    "# Load pre-trained EfficientNet-B0 model\n",
    "efficientnet_model = models.efficientnet_b0(pretrained=True)  # Add your code here\n",
    "\n",
    "# Set model to evaluation mode for inference\n",
    "efficientnet_model.eval()\n",
    "\n",
    "print(f\"Model loaded: {efficientnet_model.__class__.__name__}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in efficientnet_model.parameters()):,}\")\n",
    "print(f\"Model size: {sum(p.numel() * p.element_size() for p in efficientnet_model.parameters()) / 1024**2:.1f} MB\")\n",
    "\n",
    "# Prepare sample input for testing\n",
    "# EfficientNet-B0 expects 224x224 RGB images\n",
    "batch_size = 32\n",
    "input_shape = (batch_size, 3, 224, 224)\n",
    "sample_input = torch.randn(input_shape)\n",
    "\n",
    "print(f\"\\nSample input shape: {sample_input.shape}\")\n",
    "print(f\"Input tensor size: {sample_input.numel() * sample_input.element_size() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Verify model works with sample input\n",
    "with torch.no_grad():\n",
    "    output = efficientnet_model(sample_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Model successfully processes input ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EfficientNet hardware-architecture characteristics**: EfficientNet's compound scaling and mobile-optimized blocks make it an excellent test case for cross-platform optimization, as different hardware platforms will benefit from different optimization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert to ONNX with optimization analysis\n",
    "\n",
    "Convert the PyTorch model to ONNX format and analyze the computational graph for cross-platform deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PYTORCH TO ONNX CONVERSION ===\n",
      "✓ ONNX export successful\n",
      "ONNX model size: 20.2 MB\n",
      "ONNX opset version: 17\n",
      "Total nodes: 239\n",
      "\n",
      "Top operator types:\n",
      "  Conv: 81 nodes\n",
      "  Sigmoid: 65 nodes\n",
      "  Mul: 65 nodes\n",
      "  GlobalAveragePool: 17 nodes\n",
      "  Add: 9 nodes\n"
     ]
    }
   ],
   "source": [
    "def convert_to_onnx_with_analysis(model, sample_input, output_path):\n",
    "    \"\"\"Convert PyTorch model to ONNX with detailed analysis\"\"\"\n",
    "    \n",
    "    print(\"=== PYTORCH TO ONNX CONVERSION ===\")\n",
    "    \n",
    "    # TODO: Export PyTorch model to ONNX format\n",
    "    # HINT: ONNX export creates a hardware-agnostic intermediate representation\n",
    "    # Think about: How do you enable variable batch sizes for flexible deployment?\n",
    "    # Key considerations: opset version compatibility, dynamic shape support, operator coverage\n",
    "    # Reference: https://pytorch.org/docs/stable/onnx.html\n",
    "\n",
    "    # Add your code here\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        sample_input, \n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    onnx_model = onnx.load(output_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    \n",
    "    # Analyze model structure for cross-platform insights\n",
    "    print(f\"✓ ONNX export successful\")\n",
    "    print(f\"ONNX model size: {os.path.getsize(output_path) / 1024**2:.1f} MB\")\n",
    "    print(f\"ONNX opset version: {onnx_model.opset_import[0].version}\")\n",
    "    print(f\"Total nodes: {len(onnx_model.graph.node)}\")\n",
    "    \n",
    "    # Count operator types for provider compatibility analysis\n",
    "    op_counts = {}\n",
    "    for node in onnx_model.graph.node:\n",
    "        op_counts[node.op_type] = op_counts.get(node.op_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nTop operator types:\")\n",
    "    for op_type, count in sorted(op_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"  {op_type}: {count} nodes\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Convert model to ONNX\n",
    "onnx_model_path = os.path.join(output_dir, \"efficientnet_b0.onnx\")\n",
    "onnx_model_path = convert_to_onnx_with_analysis(efficientnet_model, sample_input, onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What insights can we gather from the operator analysis?** The operator breakdown reveals EfficientNet's optimization-friendly architecture—Conv operations dominate the computational graph (ideal for GPU acceleration), while Sigmoid and Mul operations are well-supported across all execution providers. \n",
    "> \n",
    "> This operator distribution indicates the model will benefit significantly from TensorRT's convolution fusion optimizations while maintaining broad compatibility for CPU fallback scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Execution provider performance comparison with default values\n",
    "\n",
    "Let's test different execution providers to understand hardware abstraction trade-offs.\n",
    "\n",
    "> **IMPORTANT**: You may need to install different onnxruntime versions depending on your chosen execution providers. By default, the notebook installs `onnxruntime-gpu` to support CUDA 12.x. For more details and other installation paths, please refer to the [Install ONNX Runtime](https://onnxruntime.ai/docs/install/) guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXECUTION PROVIDER PERFORMANCE COMPARISON ===\n",
      "\n",
      "Testing CPU...\n",
      "  Used: CPUExecutionProvider\n",
      "  Latency: 457.5 ± 12.6 ms\n",
      "  Throughput: 69.9 samples/sec\n",
      "\n",
      "Testing CUDA...\n",
      "  Used: CUDAExecutionProvider\n",
      "  Latency: 36.7 ± 0.1 ms\n",
      "  Throughput: 871.7 samples/sec\n"
     ]
    }
   ],
   "source": [
    "def benchmark_execution_provider(model_path, input_data, provider_config, num_warmup=5, num_runs=20):\n",
    "    \"\"\"Benchmark specific execution provider configuration\"\"\"\n",
    "    \n",
    "    # TODO: Create ONNX Runtime session with specified provider\n",
    "    # HINT: We are passing the providers as a function input\n",
    "    # Reference: https://onnxruntime.ai/docs/execution-providers/\n",
    "    session = ort.InferenceSession(model_path, providers=provider_config)  # Add your code here\n",
    "    \n",
    "    # Get input/output names\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    \n",
    "    # Warm up the session\n",
    "    input_dict = {input_name: input_data}\n",
    "    for _ in range(num_warmup):\n",
    "        _ = session.run([output_name], input_dict)\n",
    "    \n",
    "    # Benchmark inference\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.perf_counter()\n",
    "        outputs = session.run([output_name], input_dict)\n",
    "        end_time = time.perf_counter()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    throughput = input_data.shape[0] / avg_time  # samples/sec\n",
    "    \n",
    "    # Get actual execution provider used\n",
    "    used_providers = session.get_providers()\n",
    "    \n",
    "    return {\n",
    "        'provider_config': provider_config,\n",
    "        'used_providers': used_providers,\n",
    "        'avg_latency_ms': avg_time * 1000,\n",
    "        'std_latency_ms': std_time * 1000,\n",
    "        'throughput_samples_sec': throughput,\n",
    "        'output_shape': outputs[0].shape\n",
    "    }\n",
    "\n",
    "def run_provider_comparison():\n",
    "    \"\"\"Compare performance across different execution providers\"\"\"\n",
    "    \n",
    "    print(\"=== EXECUTION PROVIDER PERFORMANCE COMPARISON ===\")\n",
    "    \n",
    "    # Prepare input data\n",
    "    test_input = sample_input.numpy().astype(np.float32)\n",
    "    \n",
    "    # TODO: Define execution provider configurations to test\n",
    "    # Hint: Each provider configuration is a list of execution providers to try in order; it creates a fallback chains - if 1st provider fails, it falls back to 2nd, then 3rd...\n",
    "    # IMPORTANT: You may need to install specific onnxruntime packages / library versions to test beyond CPUExecutionProvider and CUDAExecutionProvider\n",
    "    # Reference: https://onnxruntime.ai/docs/execution-providers/\n",
    "    provider_configs = [\n",
    "        ['CPUExecutionProvider'],\n",
    "        ['CUDAExecutionProvider', 'CPUExecutionProvider']  # GPU first, fallback to CPU\n",
    "    ] # Add your code here\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, provider_config in enumerate(provider_configs):\n",
    "        provider_name = provider_config[0].replace('ExecutionProvider', '')\n",
    "        print(f\"\\nTesting {provider_name}...\")\n",
    "        \n",
    "        try:\n",
    "            result = benchmark_execution_provider(onnx_model_path, test_input, provider_config)\n",
    "            results[provider_name] = result\n",
    "            \n",
    "            print(f\"  Used: {result['used_providers'][0]}\")\n",
    "            print(f\"  Latency: {result['avg_latency_ms']:.1f} ± {result['std_latency_ms']:.1f} ms\")\n",
    "            print(f\"  Throughput: {result['throughput_samples_sec']:.1f} samples/sec\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed: {str(e)}\")\n",
    "            results[provider_name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run provider comparison\n",
    "provider_results = run_provider_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Execution provider deep-dive for NVIDIA GPUs**: ONNX Runtime provides some official guidance on how to [choose execution providers](https://pkreg101.github.io/onnxruntime/docs/performance/choosing-execution-providers.html). For NVIDIA hardware, ONNX Runtime offers three execution providers with different optimization focuses:\n",
    "> \n",
    "> - **CUDA EP**: Basic GPU acceleration using cuDNN/cuBLAS libraries\n",
    "> - **TensorRT EP**: Advanced optimization with graph analysis and kernel fusion\n",
    "> - **TensorRT RTX EP**: Consumer RTX-optimized version with faster compile times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement ONNX Runtime optimizations across platforms\n",
    "\n",
    "Let's now implement targeted optimization strategies for each deployment environment. First we’ll optimize each environment separately before unifying into a single cross-platform configuration. Why this order?\n",
    "- **GPU optimization**: Focus on maximizing expensive GPU instance utilization by eliminating I/O bottlenecks.  \n",
    "- **CPU optimization**: Focus on threading & memory efficiency where resources are constrained.  \n",
    "- **Unified strategy**: Combine both approaches into a flexible configuration that adapts to the hardware at runtime.\n",
    "\n",
    "This progression helps you see *why* GPU and CPU need different strategies, before you merge them into one deployment recipe.\n",
    "\n",
    "> **IMPORTANT**: For this exercise, the focus is on session-level optimizations that provide 80% of performance benefits with a simple configuration. BUT, each execution provider offers additional configuration parameters for fine-tuning performance:\n",
    "> - [**CUDA Provider**](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html): gpu_mem_limit, cudnn_conv_algo_search, arena_extend_strategy for memory and convolution optimization\n",
    "> - [**TensorRT Providers**](https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html): Workspace size, precision modes, engine caching for compilation optimization\n",
    "> - ...\n",
    "\n",
    "> **EXPERT TIP**: ONNX Runtime provides [automated performance tuning](https://pkreg101.github.io/onnxruntime/docs/performance/performance-tuning-tools.html) tools. Here, you experiment manually to get hands-on understanding of how different configurations utilize specific hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Maximize GPU utilization\n",
    "\n",
    "GPU utilization can be boosted by implementing I/O binding and GPU-focused optimizations. This is because memory transfers and suboptimal threading can leave GPU cores idle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU OPTIMIZATION (Target: >90% GPU utilization) ===\n",
      "Baseline CUDA latency: 36.7 ms (871.7 samples/sec)\n",
      "✓ Optimized latency: 33.6 ms (951.3 samples/sec)\n",
      "✓ Improvement vs baseline: 1.09x throughput\n"
     ]
    }
   ],
   "source": [
    "def optimize_for_gpu(model_path):\n",
    "    \"\"\"Optimize configuration for GPU - focus on maximizing GPU utilization\"\"\"\n",
    "    \n",
    "    print(\"=== GPU OPTIMIZATION (Target: >90% GPU utilization) ===\")\n",
    "    \n",
    "    if 'CUDAExecutionProvider' not in ort.get_available_providers():\n",
    "        print(\"CUDA not available - skipping GPU optimization\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare input on GPU\n",
    "    input_tensor = torch.randn(batch_size, 3, 224, 224, device='cuda', dtype=torch.float32)\n",
    "    \n",
    "    # TODO: Configure session for maximum GPU utilization\n",
    "    # Hint: GPU optimization focuses on eliminating CPU↔GPU transfers and leveraging acceleration\n",
    "    # Reference: https://onnxruntime.ai/docs/api/python/api_summary.html\n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL  # Add your code here\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL  # Add your code here\n",
    "    \n",
    "    # Create session with inputs and outputs\n",
    "\n",
    "    # TODO: Define providers for the optimized session\n",
    "    # HINT: What execution providers supports GPUs? What can you fallback to?\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Add your code here\n",
    "    session = ort.InferenceSession(model_path, sess_options, providers=providers)  # Add your code here\n",
    "    \n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    # Forward once to get output shape\n",
    "    ort_input = input_tensor.cpu().numpy().astype(np.float32)\n",
    "    dummy_out = session.run([output_name], {input_name: ort_input})\n",
    "    output_shape = dummy_out[0].shape\n",
    "    \n",
    "    # Create input OrtValue on GPU  \n",
    "    input_ortvalue = ort.OrtValue.ortvalue_from_numpy(input_tensor.cpu().numpy(), 'cuda', 0)\n",
    "    output_ortvalue = ort.OrtValue.ortvalue_from_shape_and_type(output_shape, np.float32, 'cuda', 0)\n",
    "    \n",
    "    # TODO: Implement I/O binding for GPU memory optimization\n",
    "    # Hint: You need to define binds for both input and output\n",
    "    # Reference: https://onnxruntime.ai/docs/api/python/api_summary.html#iobinding\n",
    "    io_binding = session.io_binding()  \n",
    "    \n",
    "    # Add your code here\n",
    "    io_binding.bind_input(input_name, 'cuda', 0, np.float32, input_ortvalue.shape(), input_ortvalue.data_ptr())\n",
    "    io_binding.bind_output(output_name, 'cuda', 0, np.float32, output_shape, output_ortvalue.data_ptr())\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        session.run_with_iobinding(io_binding)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Benchmark optimized configuration\n",
    "    times = []\n",
    "    for _ in range(15):\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.perf_counter()\n",
    "        session.run_with_iobinding(io_binding)\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    throughput = input_tensor.shape[0] / avg_time\n",
    "    \n",
    "    print(f\"Baseline CUDA latency: {provider_results['CUDA']['avg_latency_ms']:.1f} ms \"\n",
    "        f\"({provider_results['CUDA']['throughput_samples_sec']:.1f} samples/sec)\")\n",
    "    print(f\"✓ Optimized latency: {avg_time * 1000:.1f} ms \"\n",
    "        f\"({throughput:.1f} samples/sec)\")\n",
    "    print(f\"✓ Improvement vs baseline: {throughput / provider_results['CUDA']['throughput_samples_sec']:.2f}x throughput\")\n",
    "    \n",
    "    return {\n",
    "        'latency_ms': avg_time * 1000,\n",
    "        'throughput_samples_sec': throughput,\n",
    "        'optimization': 'I/O binding + GPU-focused configuration'\n",
    "    }\n",
    "\n",
    "# Optimize for GPU scenario\n",
    "gpu_result = optimize_for_gpu(onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO: Examine the GPU benchmark results.** How does I/O binding and session optimization affect throughput compared to the raw CUDA baseline? Did latency improve, worsen, or stay about the same?\n",
    "> \n",
    "> _Write your answer here:_ The GPU optimization using I/O binding and session-level tuning improved throughput from 871.7 to 951.3 samples/sec, a ~9% increase. Latency also decreased slightly (36.7 → 33.6 ms). This shows that optimizing memory transfers and leveraging GPU-specific session options can significantly improve utilization and overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Optimize CPU performance\n",
    "\n",
    "Address poor CPU threading configuration for Standard instances via i) efficient thread utilization without oversubscription, and ii) memory optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CPU OPTIMIZATION (Target: Optimal threading for available vCPUs) ===\n",
      "Baseline CPU latency: 457.5 ms (69.9 samples/sec)\n",
      "✓ Optimized latency: 456.2 ms (70.1 samples/sec)\n",
      "✓ Improvement vs baseline: 1.00x throughput\n"
     ]
    }
   ],
   "source": [
    "def optimize_for_cpu(model_path):\n",
    "    \"\"\"Optimize configuration for CPU instances - focus on threading efficiency\"\"\"\n",
    "    \n",
    "    print(\"\\n=== CPU OPTIMIZATION (Target: Optimal threading for available vCPUs) ===\")\n",
    "    \n",
    "    # Define session\n",
    "    sess_options = ort.SessionOptions()\n",
    "\n",
    "    # Prepare input\n",
    "    input_data = np.random.randn(batch_size, 3, 224, 224).astype(np.float32)\n",
    "\n",
    "    # TODO: Configure optimal threading for limited CPU resources\n",
    "    # Hint: Look for session parameters with the 'thread' pattern\n",
    "    # Note that thread overhead can exceed benefits when CPU cores are limited. How many vCPUs does your environment have?\n",
    "    # Key insight: intra-op = threads per operation, inter-op = parallel operations (needs branches in model)\n",
    "    # Reference: https://onnxruntime.ai/docs/api/python/api_summary.html#sessionoptions\n",
    "\n",
    "    # Add your code here\n",
    "    sess_options.intra_op_num_threads = 2\n",
    "    sess_options.inter_op_num_threads = 1\n",
    "    sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL  # Add your code here\n",
    "    \n",
    "    # TODO: Configure memory optimization for CPU efficiency\n",
    "    # Hint: Look for session parameters with the 'mem' pattern\n",
    "    # Reference: https://onnxruntime.ai/docs/api/python/api_summary.html#sessionoptions\n",
    "\n",
    "    # Add your code here\n",
    "    sess_options.enable_cpu_mem_arena = True\n",
    "    sess_options.enable_mem_pattern = True\n",
    "    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL  # Add your code here\n",
    "    \n",
    "    # TODO: Create CPU-optimized session\n",
    "    # HINT: You only need one provider for this session\n",
    "    session = ort.InferenceSession(model_path, sess_options, providers=['CPUExecutionProvider'])  # Add your code here\n",
    "    \n",
    "    # Benchmark threading configuration\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    input_dict = {input_name: input_data}\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        _ = session.run([output_name], input_dict)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(15):\n",
    "        start_time = time.perf_counter()\n",
    "        outputs = session.run([output_name], input_dict)\n",
    "        end_time = time.perf_counter()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    throughput = input_data.shape[0] / avg_time\n",
    "    \n",
    "    print(f\"Baseline CPU latency: {provider_results['CPU']['avg_latency_ms']:.1f} ms \"\n",
    "        f\"({provider_results['CPU']['throughput_samples_sec']:.1f} samples/sec)\")\n",
    "    print(f\"✓ Optimized latency: {avg_time * 1000:.1f} ms \"\n",
    "        f\"({throughput:.1f} samples/sec)\")\n",
    "    print(f\"✓ Improvement vs baseline: {throughput / provider_results['CPU']['throughput_samples_sec']:.2f}x throughput\")\n",
    "    \n",
    "    return {\n",
    "        'latency_ms': avg_time * 1000,\n",
    "        'throughput_samples_sec': throughput,\n",
    "        'optimization': 'Threading + memory optimization for CPU'\n",
    "    }\n",
    "\n",
    "# Optimize for CPU scenario  \n",
    "cpu_result = optimize_for_cpu(onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO: Examine the CPU benchmark results.** How did threading and memory optimizations affect throughput and latency compared to the CPU baseline? Were the improvements significant? Why did you choose these configuration values?\n",
    "> \n",
    "> _Write your answer here_: CPU optimization using threading and memory tuning had a minimal impact: throughput improved only slightly from 69.9 to 70.1 samples/sec, and latency remained nearly unchanged (457.5 → 456.2 ms). This indicates that the default ONNX Runtime configuration was already near-optimal for this CPU environment, and manual threading adjustments provided little benefit for this workload and batch size. \n",
    "> \n",
    "> The selected conservative threading often outperforms aggressive parallelization due to reduced context switching overhead and thermal management on a CPU with few vCPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Define a balanced multi-platform strategy\n",
    "\n",
    "Let's imagine you tried to create a balanced session configuration that works efficiently across different hardware environments. This **single configuration** finds the sweet spot between:\n",
    "- Using GPU optimizations (with I/O binding),  \n",
    "- Setting CPU optimizations (with threading & memory optimizations)\n",
    "\n",
    "**TODO: Explain what would happen if you tried to use the same session options for both CPU and GPU.** Consider the impact on CPU throughput, GPU throughput, and overall deployment portability.\n",
    "\n",
    "_Write your answers here:_ If we tried to use the same session options for both CPU and GPU, the GPU would likely be fine because it can tolerate defaults like sequential execution and memory arena usage. However, CPU performance could degrade significantly because it relies on proper threading and parallel execution to maximize throughput. Using GPU-optimized options on CPU could reduce throughput, increase latency, and underutilize CPU cores.\n",
    "\n",
    "The best practice is to keep a single ONNX model export for portability, but allow platform-specific session options when performance matters. This ensures the model runs everywhere while still leveraging CPU and GPU optimizations independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "> **TODO: What if we wanted to also deploy on mobile?**\n",
    "> \n",
    "> Reflect on how ONNX Runtime's session should be configured for memory- and power-constrained devices like mobile phones or edge devices. Consider memory usage, optimization level, threading, and power efficiency vs. peak performance.\n",
    "> \n",
    "> _Write your answers here:_ When deploying ONNX Runtime models to mobile devices, memory and power efficiency are critical. To optimize for these constraints:\n",
    "> \n",
    "> - **Disable memory arena** (`enable_cpu_mem_arena=False`) to reduce peak memory usage on devices with limited RAM\n",
    "> - **Use basic optimization level** to ensure broad operator support and avoid device-specific optimizations that may increase memory or compilation overhead\n",
    "> - **Run in single-threaded mode** to preserve battery life, reduce thermal impact, and avoid context-switching overhead\n",
    "> \n",
    "> Overall, mobile deployment prioritizes consistency, stability, and energy efficiency over maximum throughput, making conservative ONNX Runtime configurations the optimal choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you have uncovered ONNX Runtime's cross-platform optimization capabilities for AI. \n",
    "\n",
    "You started from a simple CPU vs GPU baseline, and step by step explored how to optimize sessions for platform-specific execution (GPU utilization, threading/memory on CPU).\n",
    "\n",
    "**Key insight**: A single ONNX model is sufficient for cross-platform deployment; session options should be tuned per platform to achieve optimal performance, while portability is maintained.\n",
    "\n",
    "##### **Next cross-platform optimization challenges to explore:**\n",
    "\n",
    "- **Advanced Provider Configuration**: Explore provider-specific optimization options for specialized scenarios.\n",
    "- **Execution Providers Experimentation**: Explore other execution providers (e.g., TensorRT, OpenVINO, DirectML) for specialized hardware.  \n",
    "- **On-device Benchmark**: Benchmark on **different hardware** (edge device, larger CPU server, or alternative GPU)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
