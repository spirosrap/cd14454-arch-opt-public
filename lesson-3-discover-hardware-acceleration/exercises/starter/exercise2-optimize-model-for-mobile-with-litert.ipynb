{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize a Deep Learning model for mobile deployment with LiteRT\n",
    "\n",
    "Mobile optimization transforms AI from cloud-dependent services into instant, private, and always-available experiences that work in your pocket without internet connectivity.\n",
    "\n",
    "> **Overview**: Optimize a computer vision model for mobile deployment using LiteRT's hardware acceleration and configuration options to achieve efficient on-device inference.\n",
    "> \n",
    "> **Scenario**: You work for a photo editing app startup that needs real-time image filters on smartphones. Your production pipeline currently processes only 12 frames/second on mid-range Android devices, but users expect 30+ FPS for smooth real-time effects. You are tasked with optimizing the core model inference from 25 FPS to 60+ FPS to give your production pipeline sufficient headroom for smooth user experience.\n",
    "> \n",
    "> **Goal**: Use LiteRT delegates, threading policies, and precision optimizations to maximize mobile inference performance while managing power consumption trade-offs.\n",
    "> \n",
    "> **Tools**: tensorflow, litert, pillow\n",
    "> \n",
    "> **Estimated Time**: 10 minutes\n",
    "\n",
    "> _**IMPORTANT SIMULATION NOTE**_: This exercise simulates mobile constraints on desktop hardware. Real mobile performance varies significantly based on chipset architecture, thermal design, and available accelerators (GPU, NPU, Neural Engine).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 1: Setup\n",
    "\n",
    "Let's establish your baseline environment and understand mobile deployment constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out and restart\n",
    "# ! pip install ai-edge-litert tensorflow pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ai_edge_litert.interpreter import Interpreter\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"assets/exercise2\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Should you add the CPU instructions suggested by TensorFlow?** oneDNN custom operations and x86-specific CPU instructions (AVX, FMA, etc.) are used by TensorFlow to accelerate performance on desktop CPU. Since your goal is mobile deployment, these are not applicable. But, consider when optimizing for desktop CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=== MOBILE DEPLOYMENT CONTEXT ===\")\n",
    "print(\"Target devices: Mid-range Android phones (4-6GB RAM)\")\n",
    "print(\"CPU cores: 4-8 cores (ARM Cortex-A55/A75)\")\n",
    "print(\"GPU: Adreno 618/Mali-G72 (limited compute)\")\n",
    "print(\"Power budget: <2W for sustained inference\")\n",
    "print(\"Thermal throttling: Kicks in after ~30 seconds of heavy load\")\n",
    "print()\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mobile deployment reality check**: Unlike desktop/cloud deployment, mobile devices face unique constraints: limited RAM shared across all apps, thermal throttling that reduces performance over time, and battery drain that directly impacts user experience. LiteRT's delegate system addresses these by providing hardware-specific optimizations while maintaining broad device compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create dataset and baseline model\n",
    "\n",
    "For this exercise, we'll create a synthetic dataset that simulates real-world image processing workloads. The model is a pre-trained [MobileNetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2) from TensorFlow Keras.\n",
    "\n",
    "> **A note on LiteRT cross-framework support**: While you use TensorFlow for your model creation, LiteRT also accepts models from PyTorch and JAX. All convert to the same `.tflite` format for unified mobile deployment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_image_dataset(num_samples=1000, image_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Create synthetic image dataset for mobile optimization testing\n",
    "    Simulates photo editing app workload with some realistic image distributions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating synthetic image dataset...\")\n",
    "    \n",
    "    # Generate diverse image patterns to test model performance\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Create realistic image patterns (gradients, textures, edges)\n",
    "        if i % 3 == 0:\n",
    "            # Gradient pattern (common in photos)\n",
    "            img = np.linspace(0, 255, image_size[0] * image_size[1] * 3)\n",
    "            img = img.reshape(image_size[0], image_size[1], 3)\n",
    "        elif i % 3 == 1:\n",
    "            # Texture pattern (high-frequency content)\n",
    "            img = np.random.normal(128, 30, (image_size[0], image_size[1], 3))\n",
    "        else:\n",
    "            # Mixed pattern (realistic photo simulation)\n",
    "            base = np.random.uniform(50, 200, (image_size[0], image_size[1], 3))\n",
    "            noise = np.random.normal(0, 15, (image_size[0], image_size[1], 3))\n",
    "            img = base + noise\n",
    "        \n",
    "        # Normalize to [0, 255] and convert to uint8\n",
    "        img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        images.append(img)\n",
    "        labels.append(i % 1000)  # 1000 classes for ImageNet simulation\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(f\"Dataset created:\")\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Image data type: {images.dtype}\")\n",
    "    print(f\"Memory usage: {images.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def preprocess_for_mobilenet(images):\n",
    "    \"\"\"Preprocess images for MobileNetV2 input requirements\"\"\"\n",
    "    \n",
    "    # Convert to float32 and normalize to [-1, 1] (MobileNetV2 requirement)\n",
    "    processed = images.astype(np.float32)\n",
    "    processed = (processed / 127.5) - 1.0\n",
    "    \n",
    "    print(f\"Preprocessing complete:\")\n",
    "    print(f\"Value range: [{processed.min():.2f}, {processed.max():.2f}]\")\n",
    "    print(f\"Data type: {processed.dtype}\")\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# Create dataset\n",
    "raw_images, labels = create_synthetic_image_dataset(num_samples=500)\n",
    "processed_images = preprocess_for_mobilenet(raw_images)\n",
    "\n",
    "# Create batches for testing different batch sizes\n",
    "batch_sizes = [1, 4, 8, 16]\n",
    "test_batches = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    num_batches = min(20, len(processed_images) // batch_size)\n",
    "    batch_data = processed_images[:num_batches * batch_size]\n",
    "    batch_data = batch_data.reshape(num_batches, batch_size, 224, 224, 3)\n",
    "    test_batches[batch_size] = batch_data\n",
    "    print(f\"Batch size {batch_size}: {batch_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **On using a synthetic dataset**: Real mobile photo processing involves diverse image content - from smooth gradients in sky photos to high-frequency textures in fabric close-ups. your synthetic dataset simulates these patterns at a high-level to support extending the testing to accuracy too (by making sure optimization results generalize across typical user content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_mobilenet_model():\n",
    "    \"\"\"Create and prepare base MobileNetV2 model for optimization\"\"\"\n",
    "    \n",
    "    print(\"Creating base MobileNetV2 model...\")\n",
    "    \n",
    "    # Load pre-trained MobileNetV2\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(224, 224, 3),\n",
    "        alpha=1.0,  # Width multiplier\n",
    "        weights='imagenet',\n",
    "        include_top=True,\n",
    "        classifier_activation='softmax'  # Explicit activation for mobile\n",
    "    )\n",
    "    \n",
    "    # Compile for inference\n",
    "    base_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Model analysis for mobile deployment\n",
    "    model_params = base_model.count_params()\n",
    "    \n",
    "    print(f\"Baseline MobileNetV2 for mobile:\")\n",
    "    print(f\"  Parameters: {model_params:,}\")\n",
    "    print(f\"  Layers: {len(base_model.layers)}\")\n",
    "    print(f\"  Model size (FP32): {model_params * 4 / 1024**2:.1f} MB\")\n",
    "    print(f\"  Expected mobile RAM usage: {model_params * 4 * 3 / 1024**2:.1f} MB\")  # 3x for activations\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "# Create your base model\n",
    "base_mobilenet_model = create_base_mobilenet_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why is the expected mobile RAM usage higher than the model size?** The model's runtime memory requirements do not only include model weights, but also intermediate activations and input/output buffers. For this example, you use x3 as expected model RAM usage just as a general approximation for this type of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understanding LiteRT delegate architecture\n",
    "\n",
    "[LiteRT](https://ai.google.dev/edge/litert) enables flexible, cross-platform hardware acceleration for on-device AI inference, allowing models to efficiently run on mobile CPU as well as GPU or other specialized accelerators. \n",
    "\n",
    "Hardware accelerations are managed through LiteRT's delegate system. Let's understand how this works before implementing optimizations.\n",
    "\n",
    "_NOTE: You can also [create your own LiteRT delegate!](https://ai.google.dev/edge/litert/performance/implementing_delegate)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mobile_litert_ecosystem():\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of LiteRT mobile optimization strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== LITERT MOBILE OPTIMIZATION ECOSYSTEM ===\")\n",
    "    print()\n",
    "    \n",
    "    # Mobile-specific delegate analysis\n",
    "    mobile_delegates = [\n",
    "        (\"GPU Delegate\", \"Android GPU/Metal\", \n",
    "         \"3-5x speedup on compatible ops, FP16 preferred\", \n",
    "         \"High power draw, thermal throttling risk\"),\n",
    "        \n",
    "        (\"XNNPACK Delegate\", \"Optimized CPU kernels\", \n",
    "         \"2-3x CPU speedup, broad compatibility\", \n",
    "         \"Default choice for mobile CPU optimization\"),\n",
    "        \n",
    "        (\"NNAPI Delegate\", \"Android ML accelerators\", \n",
    "         \"Automatic vendor hardware (DSP/NPU)\", \n",
    "         \"Device-dependent performance, compatibility issues\"),\n",
    "        \n",
    "        (\"Core ML Delegate\", \"iOS Neural Engine\", \n",
    "         \"Excellent power efficiency on A12+ chips\", \n",
    "         \"iOS-only, limited operation support\"),\n",
    "        \n",
    "        (\"Qualcomm Delegate\", \"Snapdragon NPU/DSP\", \n",
    "         \"Ultra-low power AI acceleration\", \n",
    "         \"Requires QNN SDK, device-specific\"),\n",
    "    ]\n",
    "    \n",
    "    for delegate, platform, benefits, considerations in mobile_delegates:\n",
    "        print(f\"ðŸ“± {delegate}:\")\n",
    "        print(f\"   Platform: {platform}\")\n",
    "        print(f\"   Benefits: {benefits}\")\n",
    "        print(f\"   Mobile considerations: {considerations}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=== MOBILE OPTIMIZATION STRATEGIES ===\")\n",
    "    \n",
    "    strategies = [\n",
    "        (\"Threading Optimization\", \n",
    "         \"Balance performance vs thermal throttling\",\n",
    "         \"1-2 threads: battery-efficient, 2-4 threads: balanced, 4+ threads: performance\"),\n",
    "        \n",
    "        (\"Precision Optimization\", \n",
    "         \"Reduce memory bandwidth and storage\",\n",
    "         \"FP16: 2x compression, INT8: 4x compression, minimal accuracy loss\"),\n",
    "        \n",
    "        (\"Operator Fusion\", \n",
    "         \"Reduce memory traffic between operations\",  \n",
    "         \"Conv+BatchNorm+ReLU â†’ single fused operation\"),\n",
    "        \n",
    "        (\"Memory Layout\", \n",
    "         \"Optimize for mobile cache hierarchies\",\n",
    "         \"NHWC format preferred for mobile ARM processors\"),\n",
    "    ]\n",
    "    \n",
    "    for strategy, purpose, implementation in strategies:\n",
    "        print(f\"âš¡ {strategy}:\")\n",
    "        print(f\"   Purpose: {purpose}\")\n",
    "        print(f\"   Implementation: {implementation}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=== MOBILE DEPLOYMENT DECISION TREE ===\")\n",
    "    print(\"1. Start with XNNPACK delegate (universal CPU optimization)\")\n",
    "    print(\"2. Add FP16 quantization (2x size reduction, broad compatibility)\")  \n",
    "    print(\"3. Test GPU delegate (if target devices support it)\")\n",
    "    print(\"4. Consider INT8 quantization (if accuracy loss acceptable)\")\n",
    "    print(\"5. Enable specialized delegates (NNAPI/NPU for specific vendors)\")\n",
    "    \n",
    "    return [\"XNNPACK\", \"FP16_Quantization\", \"GPU_Fallback\", \"Thermal_Aware\"]\n",
    "\n",
    "# Analyze mobile optimization landscape\n",
    "optimization_strategies = analyze_mobile_litert_ecosystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **LiteRT delegates work in a hierarchical way**: LiteRT's delegates system automatically partitions your model so computationally intensive operations run on specialized processors (GPU, NPU, Neural Engine) while maintaining compatibility through CPU fallback. \n",
    "> \n",
    "> This means a single LiteRT model can achieve 5x speedups on flagship devices with dedicated AI chips while still running efficiently on budget phones with basic ARM processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export MobileNetV2 with different LiteRT optimizations\n",
    "\n",
    "Let's implement MobileNetV2 conversion with various LiteRT hardware optimizations to create four **_incremental_** deployment configurations:\n",
    "\n",
    "1. **Baseline**: No optimization - measures raw model performance for comparison\n",
    "2. **CPU Optimized (XNNPack)**: Performs operator fusion for balanced CPU performance\n",
    "3. **FP16 Precision (XNNPack)**: Sets half-precision for 50% smaller app downloads and faster loading\n",
    "4. **Multi-thread Performance (XNNPack)**: Explores higher threading levels for latency-sensitive app\n",
    "\n",
    "Each configuration comes with its own trade-off on performance, app size, battery life, and device compatibility.\n",
    "\n",
    "> **How to set up LiteRT delegates:** This tutorial only uses XNNPack (CPU delegate), which is already bundled in TensorFlow Lite and doesnâ€™t require extra files. Other delegates you may explore (platform dependent):\n",
    "> \n",
    "> - GPU delegate (libtensorflowlite_gpu_delegate.so): For mobile GPUs\n",
    "> \n",
    "> - NNAPI delegate (Android only): Maps ops to hardware accelerators\n",
    "> \n",
    "> - Core ML delegate (iOS only): Maps ops to Appleâ€™s Neural Engine\n",
    "> \n",
    "> Each requires a specific `.so` library to be loaded as an app dependency. _See [LiteRT Delegates](https://ai.google.dev/edge/litert/performance/delegates) for more details._\n",
    "> \n",
    "> These delegates are not required for your exercise, but youâ€™ll see **TODOs** in the code where you could import and load them if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define mobile optimization configurations focused on hardware acceleration\n",
    "# HINT: Add the 3 missing configurations outlined in the section intro above to test different mobile hardware acceleration scenarios\n",
    "# Find details on the configuration parameters in the `convert_with_optimization()` function definition.\n",
    "# BONUS: Why not try other configurations too?\n",
    "optimization_configs = [\n",
    "    {\n",
    "        'name': 'baseline_fp32',\n",
    "        'precision': 'fp32',\n",
    "        'delegates': [],\n",
    "        'num_threads': 1\n",
    "    },\n",
    "    # Add your code here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileLiteRTOptimizer:\n",
    "    \"\"\"\n",
    "    MobileNetV2 optimization using LiteRT\n",
    "    Tests different delegates, threading, and precision configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model):\n",
    "        self.base_model = base_model\n",
    "        self.optimized_models = {}\n",
    "    \n",
    "    def convert_with_optimization(self, optimization_config):\n",
    "        \"\"\"\n",
    "        Convert model to LiteRT with specific optimization configuration\n",
    "        \n",
    "        Args:\n",
    "            optimization_config (dict): Configuration including:\n",
    "                - precision: 'fp32', 'fp16'\n",
    "                - delegates: ['xnnpack', 'gpu', ...] - [Full list at https://ai.google.dev/edge/litert/performance/delegates]\n",
    "                - num_threads: int\n",
    "        \"\"\"\n",
    "        \n",
    "        config_name = optimization_config['name']\n",
    "        print(f\"\\nConverting model with {config_name} optimization...\")\n",
    "        \n",
    "        # TODO: Create LiteRT converter (conversion pipeline)\n",
    "        # HINT: Instantiate the converter object with the model\n",
    "        # Reference: https://ai.google.dev/edge/api/tflite/python/tf/lite/TFLiteConverter\n",
    "        converter =  # Add your code here\n",
    "        \n",
    "        # TODO: Configure precision optimizations\n",
    "        # HINT: Look at how the Optimize and TargetSpec options for the converter let you model expected precisions\n",
    "        # In practice, you'd typically support 'int8' too for quantization, but model compression is out-of-scope for this exercise\n",
    "        # Reference: https://ai.google.dev/edge/api/tflite/python/tf/lite/TFLiteConverter\n",
    "        \n",
    "        if optimization_config['precision'] == 'fp16':\n",
    "            # Add your code here\n",
    "        else:  # fp32\n",
    "            # Add your code here\n",
    "        \n",
    "        # Convert model\n",
    "        try:\n",
    "            litert_model = converter.convert()\n",
    "            model_size = len(litert_model)\n",
    "            \n",
    "            print(f\"âœ“ Conversion successful:\")\n",
    "            print(f\"  Model size: {model_size / 1024**2:.1f} MB\")\n",
    "            print(f\"  Precision: {optimization_config['precision']}\")\n",
    "            \n",
    "            self.optimized_models[config_name] = {\n",
    "                'model_content': litert_model,\n",
    "                'size_mb': model_size / 1024**2,\n",
    "                'config': optimization_config\n",
    "            }\n",
    "            \n",
    "            return litert_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Conversion failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_litert_interpreter(self, model_content, config):\n",
    "        \"\"\"\n",
    "        Create LiteRT interpreter with specified configuration and delegates\n",
    "        \"\"\"\n",
    "        try:\n",
    "            experimental_delegates = []\n",
    "            \n",
    "            # TODO: Load the delegate\n",
    "            # HINTS: Since xnnpack is bundled with tensorflow, you don't need to import it\n",
    "            # IMPORTANT: For other delegates, you need to define the delegate variable as the path to the delegate library: \"/path/to/libtensorflowlite_[...].so\"\n",
    "            # Reference: https://ai.google.dev/edge/api/tflite/python/tf/lite/experimental/load_delegate\n",
    "            for delegate_name in config.get('delegates', []):\n",
    "                if delegate_name != 'xnnpack':\n",
    "                    # Add your code here\n",
    "\n",
    "            print(f\"experimental delegates: {experimental_delegates}\")\n",
    "\n",
    "            # TODO: Create base LiteRT interpreter with threading configuration\n",
    "            # Reference: https://ai.google.dev/edge/api/tflite/python/tf/lite/Interpreter\n",
    "            interpreter =  # Add your code here\n",
    "            \n",
    "            # Allocate tensors for inference\n",
    "            interpreter.allocate_tensors()\n",
    "            \n",
    "            print(f\"âœ“ LiteRT interpreter created:\")\n",
    "            print(f\"  Threading: {config.get('num_threads', 1)} threads\")\n",
    "            print(f\"  Target hardware: {config.get('delegate')}\")\n",
    "            print(f\"  Precision: {config.get('precision', 'fp32')}\")\n",
    "            \n",
    "            return interpreter\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Interpreter creation failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize optimizer and create base model\n",
    "optimizer = MobileLiteRTOptimizer(base_mobilenet_model)\n",
    "\n",
    "# Convert models with different optimizations\n",
    "for config in optimization_configs:\n",
    "    optimizer.convert_with_optimization(config)\n",
    "\n",
    "print(f\"\\nOptimization complete! Created {len(optimizer.optimized_models)} model variants.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO: Define the configuration for our scenario**\n",
    "> \n",
    "> Review our scenario and mobile device specs from the intro and step 1 of the notebook. What LITERT configuration would you use for our scenario?\n",
    "> \n",
    "> _Your answer_: _________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Benchmark mobile performance across configurations\n",
    "\n",
    "Let's measure the performance impact of your different LiteRT optimizations.\n",
    "\n",
    "To simulate mobile constraints on your development hardware, you can:\n",
    " - Use CPU-only execution\n",
    " - Set a limited thread count to mimic mobile thermal constraints\n",
    "\n",
    "> _**NOTE**_: This exercise focuses on CPU optimization (threading, precision), which provides guaranteed improvements across all mobile devices. \n",
    "> <br>In production, you'd also benchmark GPU/NNAPI delegates on target hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_mobile_configuration(model_content, config, test_data, num_iterations=50):\n",
    "    \"\"\"\n",
    "    Benchmark LiteRT model with specific configuration\n",
    "    \n",
    "    Args:\n",
    "        model_content: LiteRT model binary\n",
    "        config: Optimization configuration\n",
    "        test_data: Input data for benchmarking\n",
    "        num_iterations: Number of inference iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    config_name = config['name']\n",
    "    print(f\"\\nBenchmarking {config_name}...\")\n",
    "\n",
    "    # Set CPU as available physical device\n",
    "    my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "    tf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU')\n",
    "    \n",
    "    # Create LiteRT interpreter\n",
    "    interpreter = optimizer.create_litert_interpreter(model_content, config)\n",
    "    if interpreter is None:\n",
    "        return None\n",
    "    \n",
    "    # Prepare test batch (single sample for mobile simulation)\n",
    "    test_batch = test_data[0:1]  # Single image for mobile processing\n",
    "    \n",
    "    # Define helper function to run inference\n",
    "    def _invoke_interpreter(batch):\n",
    "        \"\"\"\n",
    "        #Â TODO: Run inference using LiteRT interpreter\n",
    "        # HINT: It's a three-step process that sets the input, invokes the interpreter, and extracts the output\n",
    "        # Reference: https://ai.google.dev/edge/api/tflite/python/tf/lite/Interpreter and https://ai.google.dev/edge/litert/inference#run-python \n",
    "        \"\"\"\n",
    "        output = None\n",
    "\n",
    "        # Add your code here \n",
    "\n",
    "        return output\n",
    "    \n",
    "    # Warm-up runs (important for mobile performance measurement)\n",
    "    for i in range(5):\n",
    "        _ = _invoke_interpreter(test_batch)\n",
    "    \n",
    "    # Benchmark inference\n",
    "    latencies = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Measure single inference time (mobile pattern)\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Run inference using LiteRT interpreter\n",
    "        output = _invoke_interpreter(test_batch)\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        latency = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "        latencies.append(latency)\n",
    "    \n",
    "    # Calculate mobile-relevant statistics\n",
    "    avg_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "    min_latency = np.min(latencies)\n",
    "    max_latency = np.max(latencies)\n",
    "    fps = 1000.0 / avg_latency  # Frames per second (mobile metric)\n",
    "    \n",
    "    results = {\n",
    "        'config_name': config_name,\n",
    "        'avg_latency_ms': avg_latency,\n",
    "        'std_latency_ms': std_latency,\n",
    "        'min_latency_ms': min_latency,\n",
    "        'max_latency_ms': max_latency,\n",
    "        'fps': fps,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    print(f\"Results for {config_name}:\")\n",
    "    print(f\"  Average latency: {avg_latency:.2f} Â± {std_latency:.2f} ms\")\n",
    "    print(f\"  FPS: {fps:.1f}\")\n",
    "    print(f\"  Latency range: [{min_latency:.2f}, {max_latency:.2f}] ms\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def comprehensive_mobile_benchmark():\n",
    "    \"\"\"Run comprehensive benchmark across all optimization configurations\"\"\"\n",
    "    \n",
    "    print(\"=== COMPREHENSIVE MOBILE PERFORMANCE BENCHMARK ===\")\n",
    "    \n",
    "    benchmark_results = {}\n",
    "    test_data = processed_images[:10]  # Use small subset for benchmarking\n",
    "    \n",
    "    for config_name, model_info in optimizer.optimized_models.items():\n",
    "        config = model_info['config']\n",
    "        model_content = model_info['model_content']\n",
    "        \n",
    "        results = benchmark_mobile_configuration(\n",
    "            model_content, \n",
    "            config, \n",
    "            test_data, \n",
    "            num_iterations=30\n",
    "        )\n",
    "        \n",
    "        # Add model size information\n",
    "        results['model_size_mb'] = model_info['size_mb']\n",
    "        \n",
    "        benchmark_results[config_name] = results\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "def analyze_mobile_efficiency(benchmark_results):\n",
    "    \"\"\"Analyze performance and efficiency trade-offs for mobile deployment\"\"\"\n",
    "    \n",
    "    print(\"\\n=== MOBILE EFFICIENCY ANALYSIS ===\")\n",
    "    \n",
    "    baseline_results = benchmark_results['baseline_fp32']\n",
    "    baseline_latency = baseline_results['avg_latency_ms']\n",
    "    baseline_size = baseline_results['model_size_mb']\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    for config_name, results in benchmark_results.items():\n",
    "        latency_improvement = baseline_latency / results['avg_latency_ms']\n",
    "        size_reduction = (baseline_size - results['model_size_mb']) / baseline_size * 100\n",
    "        fps_improvement = results['fps'] / baseline_results['fps']\n",
    "        \n",
    "        analysis[config_name] = {\n",
    "            'latency_improvement': latency_improvement,\n",
    "            'size_reduction_percent': size_reduction,\n",
    "            'fps_improvement': fps_improvement,\n",
    "            'fps': results['fps'],\n",
    "            'latency_ms': results['avg_latency_ms'],\n",
    "            'size_mb': results['model_size_mb']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{config_name.upper()}:\")\n",
    "        print(f\"  Latency improvement: {latency_improvement:.2f}x\")\n",
    "        print(f\"  FPS improvement: {fps_improvement:.2f}x ({results['fps']:.1f} FPS)\")\n",
    "        print(f\"  Model size reduction: {size_reduction:.1f}%\")\n",
    "        print(f\"  Model size: {results['model_size_mb']:.1f} MB\")\n",
    "    \n",
    "    # Mobile deployment analysis\n",
    "    print(\"\\n=== MOBILE DEPLOYMENT TARGET ANALYSIS ===\")\n",
    "    \n",
    "    target_fps = 30  # Real-time photo processing target\n",
    "    battery_efficient_fps = 15  # Battery-conscious processing\n",
    "    \n",
    "    for config_name, analysis_data in analysis.items():\n",
    "        fps = analysis_data['fps']\n",
    "        latency = analysis_data['latency_ms']\n",
    "        \n",
    "        if fps >= target_fps:\n",
    "            deployment_status = \"âœ“ Real-time capable\"\n",
    "        elif fps >= battery_efficient_fps:\n",
    "            deployment_status = \"!  Battery-efficient mode\"\n",
    "        else:\n",
    "            deployment_status = \"âœ— Below mobile requirements\"\n",
    "        \n",
    "        print(f\"{config_name}: {deployment_status} ({fps:.1f} FPS, {latency:.1f}ms)\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "benchmark_results = comprehensive_mobile_benchmark()\n",
    "efficiency_analysis = analyze_mobile_efficiency(benchmark_results)\n",
    "\n",
    "# Calculate power efficiency insights for mobile deployment\n",
    "print(\"\\n=== MOBILE POWER EFFICIENCY INSIGHTS ===\")\n",
    "\n",
    "# Simulate power consumption based on threading and precision optimization\n",
    "# IMPORTANT: The approximation below are for educational simulation; values are device-specific\n",
    "for config_name, results in benchmark_results.items():\n",
    "    config = results['config']\n",
    "    num_threads = config.get('num_threads', 1)\n",
    "    precision = config.get('precision', 'fp32')\n",
    "    \n",
    "    # Estimate relative power consumption (simplified mobile power model)\n",
    "    base_power = 1.0  # Baseline power unit\n",
    "    thread_power = base_power * (1 + 0.4 * (num_threads - 1))  # Threading power overhead\n",
    "    precision_power = thread_power * (0.7 if precision == 'fp16' else 1.0)  # FP16 efficiency\n",
    "    \n",
    "    power_efficiency = results['fps'] / precision_power  # FPS per power unit\n",
    "    \n",
    "    print(f\"{config_name}:\")\n",
    "    print(f\"  Estimated power factor: {precision_power:.2f}x baseline\")\n",
    "    print(f\"  Power efficiency: {power_efficiency:.1f} FPS/power_unit\")\n",
    "    print(f\"  Threading overhead: {((num_threads - 1) * 0.4 * 100):.0f}% additional power\")\n",
    "    print(f\"  Precision benefit: {('30% power reduction' if precision == 'fp16' else 'baseline power')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Did you notice any difference in the mobile vs cloud benchmarking?** In mobile performance benchmarking, you typically focus on single-sample latency rather than batch throughput because mobile apps process user inputs one at a time. \n",
    "> \n",
    "> Additionally, sustained performance matters more than peak performance due to thermal throttling that kicks in after 20-30 seconds of intensive processing on mobile devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.5: Visualize mobile optimization results\n",
    "\n",
    "Let's create comprehensive visualizations to understand the mobile optimization trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mobile_optimization_results():\n",
    "    \"\"\"Create comprehensive visualization of mobile optimization results\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Extract data for visualization\n",
    "    config_names = list(benchmark_results.keys())\n",
    "    fps_values = [benchmark_results[name]['fps'] for name in config_names]\n",
    "    latency_values = [benchmark_results[name]['avg_latency_ms'] for name in config_names]\n",
    "    size_values = [benchmark_results[name]['model_size_mb'] for name in config_names]\n",
    "\n",
    "    # Define colors\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    colors = [cmap(i % 10) for i in range(len(config_names))]\n",
    "    \n",
    "    # 1. FPS Comparison\n",
    "    bars1 = ax1.bar(config_names, fps_values, color=colors, alpha=0.7)\n",
    "    ax1.set_ylabel('Frames Per Second (FPS)')\n",
    "    ax1.set_title('Mobile Performance Comparison')\n",
    "    ax1.axhline(y=30, color='red', linestyle='--', alpha=0.7, label='Real-time target (30 FPS)')\n",
    "    ax1.axhline(y=15, color='orange', linestyle='--', alpha=0.7, label='Battery-efficient (15 FPS)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add FPS values on bars\n",
    "    for bar, fps in zip(bars1, fps_values):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f'{fps:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Latency vs Model Size\n",
    "    ax2.scatter(size_values, latency_values, c=colors, s=100, alpha=0.7)\n",
    "    for i, name in enumerate(config_names):\n",
    "        ax2.annotate(name.replace('_', '\\n'), (size_values[i], latency_values[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    ax2.set_xlabel('Model Size (MB)')\n",
    "    ax2.set_ylabel('Latency (ms)')\n",
    "    ax2.set_title('Mobile Efficiency Trade-off: Size vs Latency')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Improvement factors relative to baseline\n",
    "    baseline_fps = benchmark_results['baseline_fp32']['fps']\n",
    "    baseline_size = benchmark_results['baseline_fp32']['model_size_mb']\n",
    "    \n",
    "    fps_improvements = [fps / baseline_fps for fps in fps_values]\n",
    "    size_reductions = [(baseline_size - size) / baseline_size * 100 for size in size_values]\n",
    "    \n",
    "    x = np.arange(len(config_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars3a = ax3.bar(x - width/2, fps_improvements, width, label='FPS Improvement', \n",
    "                     color='#2ecc71', alpha=0.7)\n",
    "    \n",
    "    ax3.set_xlabel('Configuration')\n",
    "    ax3.set_ylabel('Improvement Factor')\n",
    "    ax3.set_title('Mobile Optimization Improvements vs Baseline')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([name.replace('_', '\\n') for name in config_names], fontsize=8)\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add improvement values on bars\n",
    "    for bar, imp in zip(bars3a, fps_improvements):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{imp:.2f}x', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 4. Power Efficiency Analysis\n",
    "    # Calculate power efficiency scores\n",
    "    power_scores = []\n",
    "    for config_name in config_names:\n",
    "        config = benchmark_results[config_name]['config']\n",
    "        fps = benchmark_results[config_name]['fps']\n",
    "        num_threads = config.get('num_threads', 1)\n",
    "        use_xnnpack = config.get('use_xnnpack', False)\n",
    "        \n",
    "        # Simplified power model\n",
    "        base_power = 1.0\n",
    "        thread_power = base_power * (1 + 0.3 * (num_threads - 1))\n",
    "        optimization_power = thread_power * (0.8 if use_xnnpack else 1.0)\n",
    "        power_efficiency = fps / optimization_power\n",
    "        power_scores.append(power_efficiency)\n",
    "    \n",
    "    bars4 = ax4.bar(config_names, power_scores, color=colors, alpha=0.7)\n",
    "    ax4.set_ylabel('Power Efficiency (FPS/Power Unit)')\n",
    "    ax4.set_title('Mobile Power Efficiency Comparison')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add efficiency values on bars\n",
    "    for bar, score in zip(bars4, power_scores):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, \n",
    "                f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    for ax in [ax1, ax4]:\n",
    "        ax.set_xticklabels([name.replace('_', '\\n') for name in config_names], \n",
    "                          rotation=0, ha='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'mobile_optimization_results.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nVisualization saved to: {output_dir}/mobile_optimization_results.png\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "visualize_mobile_optimization_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Hardware context matters**: Are you surprised about xnnpack not providing speed-ups? This is expected! These results show server CPU performance, after all. On actual mobile ARM processors:\n",
    "> \n",
    "> - Baseline performance would be ~10x slower\n",
    "> - XNNPACK would provide 2-3x acceleration\n",
    "> - Threading benefits would be limited by thermal constraints\n",
    "> \n",
    "> This demonstrates why mobile-specific optimization strategies are essential for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "> **TODO**. Test your understanding of mobile optimization strategies.\n",
    "> \n",
    "> Use your benchmark results and your knowledge of model precision, threading, and delegates to complete the tasks below.\n",
    "> \n",
    "> **PART 1: SCENARIO RECOMMENDATIONS**\n",
    "> For each scenario, choose the LiteRT configuration you think is most appropriate. Justify your choice based on trade-offs such as latency, model size, battery life, accuracy, and device compatibility.\n",
    "> \n",
    "> **1.** Real-time Photo Filter App\n",
    "> \n",
    "> - _Requirements_: Smooth live camera preview (>30 FPS), 2-hour battery life, mid-range Android devices (~2GB RAM).\n",
    "> - _Constraints_: App size <100MB, fast startup, no accuracy loss.\n",
    "> - _Your recommended configuration_: ____________\n",
    "> - _Justification_: __________________________________________\n",
    "> \n",
    "> **2.** Offline Travel Photo Assistant\n",
    "> \n",
    "> - _Requirements_: Works offline, prioritizes 7-day battery life, budget phones (~512MB RAM).\n",
    "> - _Constraints_: Must be very small for emerging markets, occasional slower processing is fine.\n",
    "> - _Your recommended configuration_: ____________\n",
    "> - _Justification_: __________________________________________\n",
    "> \n",
    "> **3.** Enterprise Document Scanner\n",
    "> \n",
    "> - _Requirements_: High accuracy, sustained performance for 8+ hours, premium flagship devices.\n",
    "> - _Constraints_: Speed less important than accuracy, power budget available.\n",
    "> - _Your recommended configuration_: ____________\n",
    "> - _Justification_: __________________________________________\n",
    "> \n",
    "> **PART 2: BONUS QUESTIONS**\n",
    "> \n",
    "> **1.** What differences did you observe when changing the number of threads? How might this affect latency stability and power usage?\n",
    "> <br> _HINT_: Find information at https://ai.google.dev/edge/litert/models/best_practices#tweak_the_number_of_threads \n",
    "> <br>_Your answer_: ____________\n",
    "> \n",
    "> **2.** LiteRT uses a delegate hierarchy (CPU â†’ GPU â†’ NPU) rather than forcing all operations onto the fastest available accelerator. Why might this be important for mobile deployment?\n",
    "> <br>_Your answer_: ____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you have uncovered LiteRT's mobile optimization capabilities by implementing and benchmarking different delegate configurations, threading strategies, and precision optimizations.\n",
    "\n",
    "**Key insight**: Mobile deployment success depends on matching hardware acceleration strategies to specific app requirements and device constraints. LiteRT's delegate system provides the flexibility to optimize for performance, power efficiency, or broad compatibility without changing your model architecture.\n",
    "\n",
    "The optimization principles you have learned&mdash;delegate selection, threading configuration, and precision optimization&mdash;apply broadly to mobile AI deployment and are essential for creating responsive, battery-efficient mobile applications.\n",
    "\n",
    "##### **Next mobile optimization challenges to explore:**\n",
    "\n",
    "- **Advanced quantization**: Implement INT8 post-training quantization with representative datasets\n",
    "- **Custom operations**: Create hardware-specific custom operators for specialized mobile workloads\n",
    "- **Dynamic optimization**: Design runtime performance adaptation based on device capabilities and thermal state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
