{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Accelerate small language model inference with TensorRT optimization\n",
    "\n",
    "Most PyTorch models run far below GPU hardware capacity due to generic execution patterns that don't leverage hardware-specific optimizations. [TensorRT](https://developer.nvidia.com/tensorrt) addresses this on NVIDIA GPUs by creating optimized execution engines for your model specs.\n",
    "\n",
    "> **Overview**: An AI-powered platform is struggling with GPU utilization efficiency. While their model provides excellent response quality, the current PyTorch inference pipeline operates at a throughput far below the hardware's theoretical capacity.\n",
    "> \n",
    "> **Scenario**: You work for a customer service platform that processes 50,000+ support inquiries daily across multiple languages. Current infrastructure costs are unsustainable due to poor GPU utilization, and response times during peak hours exceed SLA requirements. Your goal is to achieve 3,000+ samples/sec throughput through TensorRT optimization to reduce infrastructure needs from 10 T4 instances to 3.\n",
    "> \n",
    "> **Goal**: Implement a TensorRT's build-time optimization workflow, leverage mixed precision and dynamic batching, and measure performance improvements to understand how hardware acceleration frameworks unlock production deployment efficiency.\n",
    "> \n",
    "> **Tools**: transformers, torch, tensorrt, onnx, pycuda, datasets\n",
    "> \n",
    "> **Estimated Time**: 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Let's establish our environment and verify T4 capabilities for TensorRT optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out and restart notebook\n",
    "# ! pip install transformers torch tensorrt onnx onnxruntime-gpu pycuda datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, set_seed\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import onnx\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"assets/exercise1\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility  \n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Verify T4 setup and TensorRT compatibility\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "    print(f\"Total Memory: {gpu_properties.total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Multiprocessors: {gpu_properties.multi_processor_count}\")\n",
    "    \n",
    "    # Check TensorRT compatibility\n",
    "    tensor_cores_available = gpu_properties.major >= 7\n",
    "    print(f\"Tensor Core Support: {'✓ Available' if tensor_cores_available else '✗ Not Available'}\")\n",
    "    print(f\"TensorRT Version: {trt.__version__}\")\n",
    "    \n",
    "    if tensor_cores_available:\n",
    "        print(\"  → Mixed precision (FP16) will provide significant speedup\")\n",
    "else:\n",
    "    print(\"CUDA not available - exercise requires GPU\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **T4 hardware context:** T4 GPUs feature 2,560 CUDA cores, 320 Tensor Cores, and 320 GB/s memory bandwidth. \n",
    "> \n",
    "> The Tensor Cores provide specialized acceleration for FP16 matrix operations, which TensorRT leverages through mixed precision optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load baseline model and data\n",
    "\n",
    "For this exercise, we'll use the following model and dataset, respectively:\n",
    "- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) provides a good balance of model complexity and inference speed for demonstrating TensorRT optimizations\n",
    "- A 2500-samples subset of [IMDB movie reviews dataset](https://huggingface.co/datasets/stanfordnlp/imdb) provides realistic text with natural length variability for demonstrating TensorRT's dynamic batching capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = \"[PAD]\"\n",
    "\n",
    "# Load model in evaluation mode\n",
    "pytorch_model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "pytorch_model.eval()\n",
    "\n",
    "pytorch_model_params = sum(p.numel() for p in pytorch_model.parameters())\n",
    "pytorch_model_size = sum(p.numel() * p.element_size() for p in pytorch_model.parameters()) / 1024**2\n",
    "pytorch_model_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {pytorch_model_params:,}\")\n",
    "print(f\"Model size: {pytorch_model_size:.1f} MB\")\n",
    "print(f\"Model memory: {pytorch_model_memory:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset of IMDB movie reviews for benchmarking\n",
    "print(\"Loading IMDB dataset for realistic inference benchmarking...\")\n",
    "dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "sample_texts = dataset['text'][:2500]  # Subsample from 25K test samples for efficient benchmarking\n",
    "\n",
    "print(f\"Dataset loaded: {len(sample_texts)} movie reviews (subsampled from {len(dataset)} total)\")\n",
    "\n",
    "# Analyze length distribution\n",
    "review_lengths = [len(tokenizer.encode(text)) for text in sample_texts[:100]]\n",
    "print(f\"Sample length distribution:\")\n",
    "print(f\"  Min length: {min(review_lengths)} tokens\")\n",
    "print(f\"  Max length: {max(review_lengths)} tokens\") \n",
    "print(f\"  Average length: {np.mean(review_lengths):.1f} tokens\")\n",
    "print(f\"  Median length: {np.median(review_lengths):.1f} tokens\")\n",
    "\n",
    "# Tokenize with different sequence lengths to test dynamic batching\n",
    "def prepare_input_tensors(texts, max_length=64, batch_size=32):\n",
    "    \"\"\"Tokenize texts and create batched tensors\"\"\"\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(\n",
    "        encoded['input_ids'],\n",
    "        encoded['attention_mask']\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return dataloader, encoded['input_ids'].shape\n",
    "\n",
    "# Create dataloaders for different batch sizes\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "dataloaders = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    dataloaders[batch_size], input_shape = prepare_input_tensors(\n",
    "        sample_texts, max_length=64, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "print(f\"Dataset prepared:\")\n",
    "print(f\"Total samples: {len(sample_texts)}\")\n",
    "print(f\"Input shape per sample: {input_shape}\")\n",
    "print(f\"DataLoaders created for batch sizes: {batch_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dataset characteristics impact on optimization**: The IMDB reviews show natural length variability (35-1018 tokens), but we truncate to 64 tokens for consistent fast comparison, but in practice 256+ tokens are more commonly used for this use case.  \n",
    "> \n",
    "> Advanced implementations support variable sequence lengths which further benefit from TensorRT's optimization profiles that can configure pre-optimizing kernels for common lengths while dynamically adapting memory allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Measure baseline PyTorch performance\n",
    "\n",
    "Before optimizing with TensorRT, let's establish baseline performance to measure improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pytorch_model(model, dataloader, num_batches=20, warmup_batches=5):\n",
    "    \"\"\"Measure PyTorch model performance\"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "            if i >= warmup_batches:\n",
    "                break\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            _ = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Benchmark\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            \n",
    "            # Memory tracking\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            baseline_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "            \n",
    "            # Time measurement\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            # Record metrics\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "            times.append(end_time - start_time)\n",
    "            memory_usage.append(peak_memory - baseline_memory)\n",
    "    \n",
    "    return times, memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure baseline performance across batch sizes\n",
    "baseline_results = {}\n",
    "\n",
    "print(\"Measuring PyTorch baseline performance...\")\n",
    "for batch_size in batch_sizes:\n",
    "    times, memory = benchmark_pytorch_model(pytorch_model, dataloaders[batch_size])\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    throughput = batch_size / avg_time\n",
    "    avg_memory = np.mean(memory)\n",
    "    \n",
    "    baseline_results[batch_size] = {\n",
    "        'avg_time': avg_time,\n",
    "        'throughput': throughput,\n",
    "        'latency_ms': avg_time * 1000,\n",
    "        'memory_mb': avg_memory\n",
    "    }\n",
    "    \n",
    "    print(f\"Batch {batch_size}: {throughput:.1f} samples/sec ({avg_time*1000:.1f}ms) | {avg_memory:.1f} MB\")\n",
    "\n",
    "print(f\"\\nBaseline established! Best throughput: {max(r['throughput'] for r in baseline_results.values()):.1f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Insights from baseline performance analysis:** The baseline shows typical PyTorch scaling patterns where throughput increases modestly with batch size while memory usage doubles until memory bandwidth becomes the bottleneck. \n",
    "> \n",
    "> Notice how batch 128 actually performs worse than batch 64, indicating we're hitting T4's memory limits and experiencing memory pressure that degrades performance. TensorRT addresses this through optimized memory layouts, layer fusion, and precision management that better utilize T4's architectural capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement the TensorRT conversion workflow\n",
    "\n",
    "TensorRT optimization happens in two phases: build-time optimization that analyzes and restructures the model, and runtime execution with the optimized engine.\n",
    "\n",
    "> _**Note on the ONNX intermediate format:**_ [ONNX (Open Neural Network Exchange)](https://onnx.ai/) serves as a standard model format that enables different optimization engines to understand PyTorch architectures. TensorRT is one such engine that specializes in NVIDIA GPU optimization—it reads ONNX models and generates hardware-specific kernels, memory layouts, and execution strategies optimized for your target GPU's architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pytorch_to_onnx(model, input_shape, onnx_path):\n",
    "    \"\"\"Convert PyTorch model to ONNX format\"\"\"\n",
    "    print(\"Step 1: Converting PyTorch → ONNX...\")\n",
    "    \n",
    "    # TODO: Create dummy inputs for ONNX export\n",
    "    # HINT: ONNX export needs sample inputs to trace the computation graph on the expected device\n",
    "    # Use torch.randint to create realistic token IDs (0 to vocab_size) and torch.ones to create attention masks\n",
    "    vocab_size = model.config.vocab_size\n",
    "    dummy_input_ids =  # Add your code here\n",
    "    dummy_attention_mask =  # Add your code here\n",
    "    \n",
    "    # Export to ONNX with dynamic axes for batching\n",
    "    # No optimizations done here, we leave them for TensorRT\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_input_ids, dummy_attention_mask),\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input_ids', 'attention_mask'],\n",
    "        output_names=['logits'],\n",
    "        dynamic_axes={\n",
    "            'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "            'attention_mask': {0: 'batch_size', 1: 'sequence_length'},  \n",
    "            'logits': {0: 'batch_size', 1: 'sequence_length'}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(f\"✓ ONNX model saved and verified: {onnx_path}\")\n",
    "    return onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensorrt_engine(onnx_path, engine_path, precision='fp16', min_batch=1, opt_batch=32, max_batch=128):\n",
    "    \"\"\"Build TensorRT engine from ONNX model\"\"\"\n",
    "    print(f\"Step 2: Building TensorRT engine with {precision} precision...\")\n",
    "    \n",
    "    # TODO: Initialize TensorRT logger, builder, network, and parser\n",
    "    # HINT: The network_flags allows you to add support for dynamic shapes\n",
    "    # Reference: https://docs.nvidia.com/deeplearning/tensorrt-rtx/latest/inference-library/python-api-docs.html\n",
    "    logger =  # Add your code here\n",
    "    builder =  # Add your code here\n",
    "    network_flags =  # Add your code here\n",
    "    network =  # Add your code here\n",
    "    parser =  # Add your code here\n",
    "    \n",
    "    # Parse ONNX model\n",
    "    with open(onnx_path, 'rb') as f:\n",
    "        parser.parse(f.read())\n",
    "    \n",
    "    # TODO: Configure builder settings for optimization\n",
    "    # HINT: BuilderConfig controls optimization strategies and precision settings\n",
    "    # At minimum, set the following config attributes: memory pool limit (use 4GB for T4) and fp16 precision if requests\n",
    "    # References: \n",
    "    # - https://developer.nvidia.com/docs/drive/drive-os/6.0.7/public/drive-os-tensorrt/api-reference/docs/python/infer/Core/Builder.html\n",
    "    # - https://developer.nvidia.com/docs/drive/drive-os/6.0.7/public/drive-os-tensorrt/api-reference/docs/python/infer/Core/BuilderConfig.html#tensorrt.IBuilderConfig\n",
    "    config =  # Add your code here\n",
    "\n",
    "    # Add your code here (to set up config attributes)\n",
    "    \n",
    "    # TODO: Configure dynamic batching with optimization profiles\n",
    "    # HINT: Optimization profiles tell TensorRT the range of input shapes to optimize for\n",
    "    # For our model, we need to set the shape for 'input_ids' and 'attention_mask'\n",
    "    # References: \n",
    "    # - https://developer.nvidia.com/docs/drive/drive-os/6.0.7/public/drive-os-tensorrt/api-reference/docs/python/infer/Core/Builder.html\n",
    "    # - https://docs.nvidia.com/deeplearning/tensorrt/10.8.0/_static/python-api/infer/Core/OptimizationProfile.html#tensorrt.IOptimizationProfile\n",
    "    profile =  # Add your code here\n",
    "    \n",
    "    # Add your code here (to set up profile attributes) \n",
    "\n",
    "    config.add_optimization_profile(profile)\n",
    "    \n",
    "    # Build engine\n",
    "    print(\"Building engine... (this may take 1-2 minutes)\")\n",
    "    start_build = time.time()\n",
    "    \n",
    "    # TODO: Build the serialized engine\n",
    "    # Reference: https://developer.nvidia.com/docs/drive/drive-os/6.0.7/public/drive-os-tensorrt/api-reference/docs/python/infer/Core/Builder.html\n",
    "    serialized_engine =  # Add your code here\n",
    "    build_time = time.time() - start_build\n",
    "    \n",
    "    # Save engine to disk\n",
    "    with open(engine_path, 'wb') as f:\n",
    "        f.write(serialized_engine)\n",
    "    \n",
    "    print(f\"✓ TensorRT engine built in {build_time:.1f}s: {engine_path}\")\n",
    "    return engine_path, build_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to ONNX and build TensorRT engines\n",
    "onnx_path = os.path.join(output_dir, \"distilbert.onnx\")\n",
    "fp32_engine_path = os.path.join(output_dir, \"distilbert_fp32.trt\")\n",
    "fp16_engine_path = os.path.join(output_dir, \"distilbert_fp16.trt\")\n",
    "\n",
    "# Conversion pipeline\n",
    "input_shape = (32, 64)  # (batch_size, sequence_length)\n",
    "convert_pytorch_to_onnx(pytorch_model, input_shape, onnx_path)\n",
    "\n",
    "# Build engines for both precisions\n",
    "fp32_engine, fp32_build_time = build_tensorrt_engine(\n",
    "    onnx_path, fp32_engine_path, precision='fp32', \n",
    "    min_batch=16, opt_batch=32, max_batch=128\n",
    ")\n",
    "fp16_engine, fp16_build_time = build_tensorrt_engine(\n",
    "    onnx_path, fp16_engine_path, precision='fp16',\n",
    "    min_batch=16, opt_batch=32, max_batch=128  \n",
    ")\n",
    "\n",
    "print(f\"\\n=== Conversion Summary ===\")\n",
    "print(f\"PyTorch → ONNX: ✓\")\n",
    "print(f\"ONNX → TensorRT FP32: ✓ ({fp32_build_time:.1f}s)\")\n",
    "print(f\"ONNX → TensorRT FP16: ✓ ({fp16_build_time:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What happens at TensorRT build-time?** TensorRT's build phase analyzes the model architecture and generates hardware-optimized kernels. \n",
    "> \n",
    "> This one-time cost _(~60 seconds)_ pays dividends through runtime performance gains. \n",
    "> \n",
    "> The build process includes layer fusion (combining multiple operations into single GPU kernels), precision optimization (using FP16 where safe), and memory layout planning.  specifically for your target hardware.\n",
    "> \n",
    "> _**About the build warnings:**_ The warnings you see are normal and demonstrate TensorRT's intelligent optimization where you get FP16 performance benefits with FP32 accuracy protection where it matters most:\n",
    "> - _\"Make sure input input_ids has Int64 binding\"_ --> TensorRT automatically handles token ID type conversions for optimal GPU execution\n",
    "> \n",
    "> - _\"Detected layernorm nodes in FP16\"_ --> TensorRT keeps numerically sensitive operations (LayerNorm, softmax) in FP32 while running other layers in FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the TensorRT inference engine\n",
    "\n",
    "Now let's implement the TensorRT inference engine with dynamic batch size support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorRTInference:\n",
    "    \"\"\"TensorRT inference engine wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, engine_path):\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        \n",
    "        # Load and deserialize engine\n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        \n",
    "        runtime = trt.Runtime(self.logger)\n",
    "        self.engine = runtime.deserialize_cuda_engine(engine_data)\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Pre-allocate for maximum batch size to avoid memory issues\n",
    "        max_batch_size = 128\n",
    "        seq_length = 64\n",
    "        \n",
    "        # Allocate input memory\n",
    "        input_ids_size = max_batch_size * seq_length\n",
    "        attention_mask_size = max_batch_size * seq_length\n",
    "        output_size = max_batch_size * 2  # Binary classification\n",
    "        \n",
    "        # TODO: Allocate pinned host memory for efficient async GPU transfers\n",
    "        # HINT: You can use PyCUDA for this\n",
    "        # Reference: https://documen.tician.de/pycuda/driver.html#pagelocked-host-memory\n",
    "        self.input_ids_host =  # Add your code here\n",
    "        self.attention_mask_host =  # Add your code here\n",
    "        self.output_host =  # Add your code here\n",
    "        \n",
    "        # Device memory\n",
    "        self.input_ids_device = cuda.mem_alloc(self.input_ids_host.nbytes)\n",
    "        self.attention_mask_device = cuda.mem_alloc(self.attention_mask_host.nbytes)\n",
    "        self.output_device = cuda.mem_alloc(self.output_host.nbytes)\n",
    "        \n",
    "        # Create CUDA stream\n",
    "        self.stream = cuda.Stream()\n",
    "\n",
    "        # Get TensorRT's built-in memory reporting\n",
    "        self.engine_memory_mb = self.engine.device_memory_size / 1024**2\n",
    "    \n",
    "    def infer(self, input_ids, attention_mask):\n",
    "        \"\"\"Run inference with dynamic batch size\"\"\"\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # TODO: Set dynamic input shapes for current batch\n",
    "        # HINT: TensorRT needs to know the actual input shape for each inference, for both inputs: 'input_ids' and 'attention_mask'\n",
    "        # Set it via self.context\n",
    "        # Reference: https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/work-dynamic-shapes.html\n",
    "\n",
    "        # Add your code here\n",
    "       \n",
    "        # Copy inputs to pre-allocated host memory\n",
    "        input_ids_flat = input_ids.cpu().numpy().ravel()\n",
    "        attention_mask_flat = attention_mask.cpu().numpy().ravel()\n",
    "        \n",
    "        self.input_ids_host[:len(input_ids_flat)] = input_ids_flat\n",
    "        self.attention_mask_host[:len(attention_mask_flat)] = attention_mask_flat\n",
    "\n",
    "        # Transfer input data to GPU\n",
    "        cuda.memcpy_htod_async(self.input_ids_device, self.input_ids_host, self.stream)\n",
    "        cuda.memcpy_htod_async(self.attention_mask_device, self.attention_mask_host, self.stream)\n",
    "        \n",
    "        # TODO: Bind tensor memory addresses for TensorRT execution\n",
    "        # HINT: TensorRT needs to know where to find input/output tensors in GPU memory via their tensor address\n",
    "        # Set three entries (input_ids, attention_mask, logits) for self.context\n",
    "        # Reference: https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-1001/api/python_api/infer/Core/ExecutionContext.html\n",
    "\n",
    "        # Add your code here\n",
    "        \n",
    "        # Run inference\n",
    "        self.context.execute_async_v3(stream_handle=self.stream.handle)\n",
    "        \n",
    "        # Transfer input data to GPU asynchronously\n",
    "        # HINT: You can use PyCUDA for this, look for device-to-host (_d_to_h_) transfer with an _async prefix\n",
    "        # Reference: https://documen.tician.de/pycuda/driver.html#unstructured-memory-transfers\n",
    "\n",
    "        # Add your code here\n",
    "\n",
    "        self.stream.synchronize()\n",
    "        \n",
    "        # Reshape output\n",
    "        output_elements = batch_size * 2  # Number of elements\n",
    "        output = self.output_host[:output_elements].reshape(batch_size, 2)\n",
    "        \n",
    "        return torch.tensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT inference engines\n",
    "print(\"Loading TensorRT engines...\")\n",
    "trt_fp32_engine = TensorRTInference(fp32_engine_path)\n",
    "trt_fp16_engine = TensorRTInference(fp16_engine_path)\n",
    "print(\"✓ TensorRT engines loaded and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TensorRT runtime engine efficiency**: TensorRT's runtime engine uses pre-allocated memory pools and asynchronous GPU operations to minimize inference overhead. \n",
    "> \n",
    "> - The dynamic shape setting allows the same engine to handle different batch sizes efficiently without rebuilding. \n",
    "> - The pre-allocated memory approach eliminates allocation overhead during inference, which is crucial for achieving consistent low-latency performance in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Perform comprehensive performance benchmarking\n",
    "\n",
    "Let's benchmark all configurations to measure TensorRT's optimization impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_tensorrt_engine(trt_engine, dataloader, num_batches=20, warmup_batches=3):\n",
    "    \"\"\"Benchmark TensorRT engine performance\"\"\"\n",
    "    times = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    # Warmup\n",
    "    for i, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "        if i >= warmup_batches:\n",
    "            break\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        _ = trt_engine.infer(input_ids, attention_mask)\n",
    "    \n",
    "    # Benchmark\n",
    "    for i, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "            \n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        \n",
    "        # Time measurement  \n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        outputs = trt_engine.infer(input_ids, attention_mask)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        # Record metrics\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    # TODO: Define memory usage\n",
    "    # HINT: Use TensorRT's built-in memory reporting, and convert to MB \n",
    "    # Reference: https://docs.nvidia.com/deeplearning/tensorrt/latest/_static/python-api/infer/Core/Engine.html\n",
    "    memory_usage_engine =  # Add your code here\n",
    "    memory_usage = memory_usage_engine * len(times)  # Create one entry per time measurement to match sizes\n",
    "    \n",
    "    return times, memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark all configurations\n",
    "configurations = [\n",
    "    (\"PyTorch FP32\", \"baseline\"),\n",
    "    (\"TensorRT FP32\", trt_fp32_engine), \n",
    "    (\"TensorRT FP16\", trt_fp16_engine)\n",
    "]\n",
    "\n",
    "all_results = {\"PyTorch FP32\": baseline_results}\n",
    "\n",
    "print(\"Benchmarking TensorRT configurations...\")\n",
    "\n",
    "for config_name, engine in configurations[1:]:  # Skip baseline (already measured)\n",
    "    print(f\"\\nBenchmarking {config_name}...\")\n",
    "    config_results = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        times, memory = benchmark_tensorrt_engine(engine, dataloaders[batch_size])\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        throughput = batch_size / avg_time\n",
    "        avg_memory = np.mean(memory)\n",
    "        \n",
    "        config_results[batch_size] = {\n",
    "            'avg_time': avg_time,\n",
    "            'throughput': throughput, \n",
    "            'latency_ms': avg_time * 1000,\n",
    "            'memory_mb': avg_memory\n",
    "        }\n",
    "        \n",
    "        print(f\"  Batch {batch_size}: {throughput:.1f} samples/sec ({avg_time*1000:.1f}ms) | {avg_memory:.1f} MB\")\n",
    "    \n",
    "    all_results[config_name] = config_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate optimization improvements for each batch size\n",
    "print(f\"\\n=== TensorRT Optimization Impact Analysis ===\")\n",
    "\n",
    "# Throughput Analysis\n",
    "print(\"\\nThroughput Analysis: \")\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\n\\t--- Batch Size: {batch_size} ---\")\n",
    "\n",
    "    baseline_batch = baseline_results[batch_size]\n",
    "    trt_fp32_batch = all_results[\"TensorRT FP32\"][batch_size]\n",
    "    trt_fp16_batch = all_results[\"TensorRT FP16\"][batch_size]\n",
    "\n",
    "    fp32_improvement = trt_fp32_batch['throughput'] / baseline_batch['throughput']\n",
    "    fp16_improvement = trt_fp16_batch['throughput'] / baseline_batch['throughput']\n",
    "\n",
    "    print(f\"\\t\\tTensorRT FP32 optimization: {fp32_improvement:.2f}x throughput improvement\")\n",
    "    print(f\"\\t\\tTensorRT FP16 optimization: {fp16_improvement:.2f}x throughput improvement\") \n",
    "    print(f\"\\t\\t→ Mixed precision benefit: {fp16_improvement/fp32_improvement:.2f}x additional gain\")\n",
    "\n",
    "# Memory Analysis\n",
    "print(\"\\nMemory Analysis: \")\n",
    "trt_fp32_memory = trt_fp32_engine.engine_memory_mb\n",
    "trt_fp16_memory = trt_fp16_engine.engine_memory_mb\n",
    "\n",
    "print(f\"\\n\\tPyTorch FP32 Model (Weights only): {pytorch_model_memory:.1f} MB\")\n",
    "print(f\"\\tPyTorch peak activation memory: from {baseline_results[batch_sizes[0]]['memory_mb']:.1f} MB for batch_size {batch_sizes[-1]} to {baseline_results[batch_sizes[-1]]['memory_mb']:.1f} MB for batch_size {batch_sizes[-1]} (scales with batch size)\")\n",
    "print(f\"\\tTensorRT FP32 Engine (Weights + Activation Workspace): {trt_fp32_memory:.1f} MB\")\n",
    "print(f\"\\tTensorRT FP16 Engine (Weights + Activation Workspace): {trt_fp16_memory:.1f} MB\")\n",
    "\n",
    "pytorch_max_memory = pytorch_model_memory + baseline_results[batch_sizes[-1]]['memory_mb']\n",
    "reduction_percentage = (pytorch_max_memory - trt_fp16_memory) / pytorch_max_memory * 100\n",
    "print(f\"\\n\\t→ The FP16 engine reduces the static memory footprint by {reduction_percentage:.1f}% for batch_size {batch_sizes[-1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TensorRT memory management**: Unlike PyTorch's dynamic memory allocation that scales with batch size, TensorRT pre-allocates memory based on your optimization profile's maximum batch size (128 in our case). \n",
    "> \n",
    "> This strategy trades memory efficiency for performance consistency—no allocation overhead during inference, but you pay the memory cost of your largest expected batch size regardless of actual usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "> **TODO: TensorRT Optimization Analysis**\n",
    ">\n",
    "> Using your benchmark results from the previous analysis, answer these questions to test your understanding of TensorRT's core optimization mechanisms:\n",
    "> \n",
    "> \n",
    "> 1. **Memory Management Understanding**\n",
    ">    - Given that TensorRT pre-allocates memory for the maximum batch size, what are the production implications if you set max_batch=256 but typically run batches of 32?\n",
    ">    - In what deployment scenarios would TensorRT's approach be better than PyTorch's dynamic allocation, and when might PyTorch's approach be preferable?\n",
    ">    - Answer: ________________\n",
    "> \n",
    "> 2. **Mixed Precision Impact** \n",
    ">    - Compare the performance improvement from TensorRT FP32 vs TensorRT FP16. Why do you see this difference?\n",
    ">    - What specific T4 hardware feature enables the larger performance boost with FP16?\n",
    ">    - Answer: ________________\n",
    "> \n",
    "> 3. **Build-time vs Runtime**\n",
    ">    - TensorRT required a build step while PyTorch loads instantly. When does this trade-off make business sense?\n",
    ">    - Why can't PyTorch achieve similar runtime performance without this build step?\n",
    ">    - Answer: ________________\n",
    "> \n",
    "> 4. **Dynamic Batching Behavior**\n",
    ">    - Your optimization profile was set to min=16, opt=32, max=128. How does TensorRT handle batch sizes within this range?\n",
    ">    - What would happen if you tried to run a batch size outside this range?\n",
    ">    - Answer: ________________\n",
    ">\n",
    "> 5. **Bonus challenge: What if `max_length` of the tokenizer changed from 64 to 256?**\n",
    ">     - How would you expect this to impact the performance *improvement factor* of the TensorRT FP16 engine over the baseline PyTorch model? Would the speedup become larger, smaller, or stay the same?\n",
    ">     - _HINT:_ Think about how a longer sequence length affects the computational workload (i.e., the size of the matrix multiplications). Which environment—the generic PyTorch framework or the hardware-specific TensorRT engine—is better at capitalizing on more intensive, parallelizable work?\n",
    ">     - Answer: ________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've experienced TensorRT's complete optimization workflow and measured its real-world performance impact on T4 hardware.\n",
    "\n",
    "TensorRT demonstrates how hardware acceleration frameworks transform model deployment feasibility through systematic build-time optimization and runtime efficiency gains. TensorRT success factors include:\n",
    "- Hardware-specific optimization unlocks true GPU potential\n",
    "- Dynamic batching adapts to variable workload patterns\n",
    "- One-time build cost scales across all production inferences\n",
    "- Mixed precision leverages T4 Tensor Cores effectively\n",
    "\n",
    "##### **Next optimization challenges to explore:**\n",
    "\n",
    "- Explore **TensorRT-LLM** to leverage LLM-specific optimizations for faster autoregressive generation.\n",
    "\n",
    "- Implement **dynamic shape optimization** to handle variable sequence lengths efficiently without reallocating memory.\n",
    "\n",
    "- Optimize **layer fusion** (like attention and activation layers) to minimize kernel launches and improve throughput.\n",
    "\n",
    "- Use **TensorRT’s auto-tuning** to select the most efficient kernels for each layer type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
