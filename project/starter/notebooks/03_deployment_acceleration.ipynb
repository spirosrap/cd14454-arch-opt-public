{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciMed | Notebook 3: Hardware Acceleration & Production Deployment\n",
    "\n",
    "Welcome to the final phase of UdaciMed's optimization pipeline! In this notebook, you will implement cross-platform hardware acceleration techniques and strategize for the deployment of your optimized model across hardware targets.\n",
    "\n",
    "## Recap: Optimization Journey\n",
    "\n",
    "In [Notebook 2](02_architecture_optimization.ipynb), you have implemented architectural optimizations that brought you closer to your optimization targets.\n",
    "\n",
    "Now, it is time to unlock further performance opportunities with hardware acceleration.\n",
    "\n",
    "> **Your mission**: Transform your optimized model into a production-ready cross-platform deployment that meets production SLAs on this reference hardware, and finalize UdaciMed's deployment strategy across its diverse hardware fleet.\n",
    "\n",
    "### Hardware acceleration\n",
    "\n",
    "You will implement and evaluate **2 core deployment techniques\\*** using [ONNX Runtime](https://onnxruntime.ai/):\n",
    "\n",
    "1. **Mixed Precision (FP16)** - Utilizing 16-bit floating-point numbers to significantly speed up calculations and reduce memory usage on compatible hardware.\n",
    "2. **Dynamic Batching** - Finding the best batch size to maximize throughput for offline tasks while maintaining low latency for real-time requests.\n",
    "\n",
    "Additionally, you will analyze three deployment scenarios: GPU (TensorRT), CPU (OpenVINO), and Edge deployment considerations.\n",
    "\n",
    "_\\* Note that while you are expected to implement both deployment techniques, you can decide whether to keep either or both in your final deployment strategy to best achieve targets._\n",
    "\n",
    "---\n",
    "\n",
    "Through this notebook, you will:\n",
    "\n",
    "- **Convert PyTorch model to ONNX** for cross-platform deployment\n",
    "- **Apply hardware acceleration using ONNX Runtime** on the reference T4 device\n",
    "- **Benchmark end-to-end performance** against SLAs\n",
    "- **Validate clinical safety** across the deployment pipeline\n",
    "- **Analyze alternative deployment strategies** for diverse hardware environments\n",
    "\n",
    "**Let's deliver a production-ready, hardware-accelerated diagnostic deployment!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup the environment\n",
    "\n",
    "First, let's set up the environment and understand our reference hardware capabilities. \n",
    "\n",
    "This ensures our optimization and benchmarking code will run smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any, Literal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project utilities\n",
    "from utils.data_loader import (\n",
    "    load_pneumoniamnist,\n",
    "    get_sample_batch\n",
    ")\n",
    "from utils.model import (\n",
    "    create_baseline_model,\n",
    "    get_model_info\n",
    ")\n",
    "from utils.evaluation import (\n",
    "    evaluate_with_multiple_thresholds\n",
    ")\n",
    "from utils.profiling import (\n",
    "    PerformanceProfiler,\n",
    "    measure_time\n",
    ")\n",
    "from utils.visualization import (\n",
    "    plot_performance_profile,\n",
    "    plot_batch_size_comparison\n",
    ")\n",
    "from utils.architecture_optimization import (\n",
    "    create_optimized_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and analyze hardware capabilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Check tensor core support for mixed precision - crucial for FP16 acceleration\n",
    "    gpu_compute = torch.cuda.get_device_properties(0).major\n",
    "    tensor_core_support = gpu_compute >= 7  # Volta+ architecture\n",
    "    print(f\"Tensor Core Support: {tensor_core_support}\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available - hardware acceleration will be limited\")\n",
    "\n",
    "print(\"Default hardware acceleration environment ready!\")\n",
    "\n",
    "# Verify ONNX Runtime GPU support\n",
    "print(f\"\\nONNX Runtime available providers: {ort.get_available_providers()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Getting ready for acceleration**: The checks above highlight two critical facts for our mission:\n",
    "> 1. Our reference hardware has tensor core support, which can dramatically speed up 16-bit floating-point (FP16) calculations; for other hardware deployments, like CPUs that lack this feature, we would need to rely on different techniques (such as 8-bit integer quantization (INT8)) to achieve similar acceleration.\n",
    "> 2. ONNX Runtime providers are available for our primary targets: CUDAExecutionProvider for GPU and CPUExecutionProvider for CPU. This allows us to benchmark on both platforms. For a true mobile or edge deployment, we would need to use a specialized package like ONNX Runtime Mobile, which is built separately to keep the application lightweight.\n",
    "> \n",
    "> Our task is to meet SLAs on our current device, which means we must **_benchmark against the GPU_** to see if we've met our goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load test data and optimized model with configuration\n",
    "\n",
    "The model is needed for deployment, and the optimization results for comparison.\n",
    "\n",
    "Test data is needed for both conversion and final performance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset loading parameters\n",
    "img_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Load test dataset for final evaluation\n",
    "test_loader = load_pneumoniamnist(\n",
    "    split=\"test\", \n",
    "    download=True, \n",
    "    size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=None\n",
    ")\n",
    "\n",
    "# Get sample batch for profiling\n",
    "sample_images, sample_labels = get_sample_batch(test_loader)\n",
    "sample_images = sample_images.to(device)\n",
    "sample_labels = sample_labels.to(device)\n",
    "\n",
    "print(f\"Test data loaded: {sample_images.shape} batch for hardware acceleration profiling\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Batch size strategy**: Your batch size choice impacts memory usage, latency, and throughput. \n",
    "> \n",
    "> Consider: What batch size best applied for each deployment scenario? Don't forget to review the batch analysis plot from Notebook 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimized model and results from notebook 2\n",
    "\n",
    "# TODO: Define the experiment name\n",
    "experiment_name = # String - Add your value here\n",
    "\n",
    "with open(f'../results/optimization_results_{experiment_name}.pkl', 'rb') as f:\n",
    "    optimization_results = pickle.load(f)\n",
    "\n",
    "print(\"Loaded optimization results from Notebook 2:\")\n",
    "print(f\"   Model: {optimization_results['model_name']}\")\n",
    "print(f\"   Clinical Performance: {optimization_results['clinical_performance']['optimized']['sensitivity']:.1%} sensitivity\")\n",
    "print(f\"   Architecture Speedup: {optimization_results['performance_improvements']['latency_speedup']:.2f}x\")\n",
    "print(f\"   Memory Reduction: {optimization_results['performance_improvements']['memory_reduction_percent']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **HINT: Finding your optimization results**\n",
    "> \n",
    "> Your optimization results from Notebook 2 should be saved as:\n",
    "> - Results file: `../results/optimization_results_{experiment_name}.pkl`\n",
    "> - Model weights: `../results/optimized_model.pth`\n",
    "> \n",
    "> The experiment name typically combines your optimization techniques, like:\n",
    "> - `\"interpolation-removal_depthwise-separable\"`\n",
    "> - `\"channel-reduction_grouped-conv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimization configuration\n",
    "opt_config = optimization_results['optimization_config']\n",
    "optimized_model = None  \n",
    "\n",
    "# TODO: Load the optimized model in the optimized_model variable\n",
    "# HINT: This involves:\n",
    "# > 1. Recreate the baseline model\n",
    "# > 2. Applying the same architectural modifications using the saved optimization configuration\n",
    "# > 3. Loading the trained weights\n",
    "# See https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference for inspiration\n",
    "\n",
    "# Add your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert model with hardware acceleration for production deployment\n",
    "\n",
    "Convert the optimized model to [ONNX (Open Neural Network Exchange)](https://onnx.ai/) with optional hardware accelerations. \n",
    "\n",
    "**IMPORTANT**: You are tasked to implement both hardware optimizations even if you decide to disable them for the final export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your deployment configuration for the ONNX export.\n",
    "# GOAL: Decide whether to use mixed precision (FP16) and/or dynamic batching for the final export.\n",
    "# HINT: Setting use_fp16 to True can significantly improve performance on compatible GPUs (like the T4 with Tensor Cores)\n",
    "# but may introduce a minor, often negligible, loss in precision. We'll validate the clinical impact later.\n",
    "\n",
    "use_fp16 =  # Boolean; Set to True to enable mixed precision, False for standard FP32.\n",
    "use_dynamic_batching =  # Boolean; Set to True to allow variable batch sizes, False for a fixed batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PyTorch model to ONNX format (for cross-platform deployment)\n",
    "\n",
    "def export_model_to_onnx(model: nn.Module, input_tensor: torch.Tensor, \n",
    "                        export_path: str, model_name: str = \"pneumonia_detection\", \n",
    "                        fp16_mode: bool = use_fp16, dynamic_batching: bool = use_dynamic_batching) -> str:\n",
    "    \"\"\"\n",
    "    Export PyTorch model to ONNX format for production deployment.\n",
    "    Apply hardware optimizations if selected.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to export\n",
    "        input_tensor: Sample input tensor for shape inference\n",
    "        export_path: Directory to save the ONNX model\n",
    "        model_name: Name for the exported ONNX file\n",
    "        fp16_mode: If True, exports the model in FP16 (mixed precision)\n",
    "        dynamic_batching: If True, configures the model to accept variable batch sizes\n",
    "        \n",
    "    Returns:\n",
    "        Path to exported ONNX model\n",
    "    \"\"\"\n",
    "    # Define output path, and ensure it exists\n",
    "    onnx_path = f\"{export_path}/{model_name}.onnx\"\n",
    "    Path(export_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Convert PyTorch model to ONNX format for cross-platform deployment following the steps below\n",
    "    # ONNX provides compatibility with TensorRT, OpenVINO, and other inference engines\n",
    "    \n",
    "    # 1. TODO: Set model to evaluation mode\n",
    "    # Add your code here\n",
    "\n",
    "    # 2. TODO: Define the logic for fp16 mode\n",
    "    # HINT: Think about what needs to be converted to half precision (input, model, or both?)\n",
    "    # Add your code here\n",
    "        \n",
    "    print(f\"Exporting model to ONNX format...\")\n",
    "    print(f\"   Input shape: {input_tensor.shape}\")\n",
    "    print(f\"   Input dtype: {input_tensor.dtype}\")\n",
    "    print(f\"   FP16 mode: {fp16_mode}\")\n",
    "    print(f\"   Export path: {onnx_path}\")\n",
    "    \n",
    "    dynamic_axes = None\n",
    "    # 3. TODO: Define the logic for dynamic batching\n",
    "    # HINT: Find the export argument in torch.onnx.export that supports setting dynamic axes\n",
    "    # If you are not setting dynamic batching, how does onnx runtime choose the fixed batch size? Look at the input tensor in this case\n",
    "    # Add your code here\n",
    "\n",
    "    # 4. Export to ONNX format with defined parameters\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        input_tensor,  # Input example\n",
    "        onnx_path,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=16,  # Compatible with most inference engines\n",
    "        do_constant_folding=True,  # Optimize constant operations\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"ONNX export completed: {onnx_path}\")\n",
    "\n",
    "    # Verify ONNX model integrity - sanity check\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(\"   ONNX model verification passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"   WARNING: ONNX verification failed: {str(e)}\")\n",
    "\n",
    "    return onnx_path\n",
    "\n",
    "# Export the mixed precision model to ONNX\n",
    "onnx_model_path = export_model_to_onnx(\n",
    "    model=optimized_model,\n",
    "    input_tensor=sample_images,\n",
    "    export_path=\"../results/onnx_models\",\n",
    "    model_name=\"udacimed_pneumonia_optimized\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploy with ONNX Runtime\n",
    "\n",
    "With our model saved in the ONNX format, we can now load it into the [ONNX Runtime (ORT)](https://onnxruntime.ai/getting-started). \n",
    "\n",
    "ORT is a high-performance inference engine that can execute models on different hardware backends through its **Execution Providers (EPs)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates an ONNX Runtime Inference Session.\n",
    "\n",
    "# TODO: Choose whether the session should run on GPU or not\n",
    "use_gpu =  # Boolean; Add your value here\n",
    "\n",
    "def create_inference_session(model_path: str, use_gpu: bool = use_gpu) -> ort.InferenceSession:\n",
    "    \"\"\"\n",
    "    Creates an ONNX Runtime inference session.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the ONNX model file.\n",
    "        use_gpu: If True, configures the session to use the CUDA Execution Provider.\n",
    "\n",
    "    Returns:\n",
    "        An ONNX Runtime InferenceSession object.\n",
    "    \"\"\"\n",
    "    print(f\"Creating ONNX Runtime session for {'GPU' if use_gpu else 'CPU'}...\")\n",
    "    \n",
    "    # TODO: Define the execution providers\n",
    "    # HINT: The `providers` argument takes a list of strings. For GPU, are you guaranteed that all operations can run on the CUDAExecutionProvider?\n",
    "    # Reference: https://onnxruntime.ai/docs/performance/execution-providers/\n",
    "    \n",
    "    providers = []\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        providers = # Add your code here\n",
    "    else:\n",
    "        providers = ['CPUExecutionProvider']\n",
    "    \n",
    "    # TODO: Create the ONNX Runtime InferenceSession\n",
    "    # HINT: Instantiate an InferenceSession with the correct Execution Provider for the target hardware and any other desired parameters\n",
    "    # Reference: https://onnxruntime.ai/docs/api/python/api_summary.html#inferencesession\n",
    "    session =  # Add your code here\n",
    "    \n",
    "    print(f\"Session created with providers: {session.get_providers()}\")\n",
    "    return session\n",
    "\n",
    "# Create the session for our exported ONNX model.\n",
    "# We will run this on the GPU as it's our primary target device.\n",
    "inference_session = create_inference_session(onnx_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Benchmark model performance on all metrics\n",
    "\n",
    "Now that we have a hardware-accelerated inference session, it's time to measure its performance. \n",
    "\n",
    "Unlike a server-based approach, we will perform direct, client-side benchmarking. This gives us precise measurements of the model's raw inference speed and resource consumption on our target hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to get input details and type\n",
    "\n",
    "def get_input_details(session: ort.InferenceSession) -> Tuple[str, Tuple, np.dtype]:\n",
    "    \"\"\"\n",
    "    Gets the input name, shape, and dtype for an ONNX Runtime session.\n",
    "    \"\"\"\n",
    "    input_details = session.get_inputs()[0]\n",
    "    input_name = input_details.name\n",
    "    \n",
    "    # TODO: Check if the model is FP16 to set the correct numpy dtype\n",
    "    # HINT: Make sure the input type matches the type specified for the session input\n",
    "    # Reference: https://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.InferenceSession.get_inputs\n",
    "    is_fp16 =  # Add your code here\n",
    "    \n",
    "    # Determine the correct numpy dtype\n",
    "    input_dtype = np.float16 if is_fp16 else np.float32\n",
    "    \n",
    "    return input_name, input_details.shape, input_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the main benchmarking function.\n",
    "\n",
    "def benchmark_performance(session: ort.InferenceSession, \n",
    "                          test_data: torch.Tensor,\n",
    "                          batch_sizes: List[int],\n",
    "                          num_runs: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Benchmarks the performance of an ONNX Runtime session.\n",
    "\n",
    "    Args:\n",
    "        session: The ONNX Runtime inference session.\n",
    "        test_data: A batch of test data for inference.\n",
    "        batch_sizes: A list of batch sizes to test.\n",
    "        num_runs: The number of inference runs to average for timing.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the performance results for each batch size.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    \n",
    "    input_name, _, input_dtype = get_input_details(session)\n",
    "    print(f\"Benchmarking with input dtype: {input_dtype}\")\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"--- Benchmarking Batch Size: {batch_size} ---\")\n",
    "        \n",
    "        # Prepare batch data\n",
    "        input_array = test_data[:batch_size].cpu().numpy().astype(input_dtype)\n",
    "        \n",
    "        # Warm-up runs to stabilize GPU clocks and cache\n",
    "        for _ in range(10):\n",
    "            session.run([output_name], {input_name: input_array})\n",
    "            \n",
    "        # Timed runs\n",
    "        latencies = []\n",
    "        \n",
    "        # Perform the timed inference runs\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.perf_counter()\n",
    "            session.run([output_name], {input_name: input_array})\n",
    "            end_time = time.perf_counter()\n",
    "            latencies.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "            \n",
    "        # Measure peak GPU memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            # Run one more inference to capture memory usage after reset\n",
    "            session.run([output_name], {input_name: input_array})\n",
    "            peak_memory_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "        else:\n",
    "            peak_memory_mb = 0  # No GPU memory to measure on CPU\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_latency_ms = np.mean(latencies)\n",
    "        throughput_sps = (batch_size / avg_latency_ms) * 1000  # Samples per second\n",
    "\n",
    "        results[batch_size] = {\n",
    "            'avg_latency_ms': avg_latency_ms,\n",
    "            'throughput_sps': throughput_sps,\n",
    "            'peak_memory_mb': peak_memory_mb\n",
    "        }\n",
    "        print(f\"  Avg Latency: {avg_latency_ms:.3f} ms\")\n",
    "        print(f\"  Throughput: {throughput_sps:,.2f} samples/sec\")\n",
    "        print(f\"  Peak GPU Memory: {peak_memory_mb:.2f} MB\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# TODO: Define the batch size(s) you want to test.\n",
    "# HINT: Powers of two are often optimal for GPU hardware, and 1 is useful for latency\n",
    "batch_sizes_to_test = []  # Add your values here\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark_results = benchmark_performance(\n",
    "    session=inference_session,\n",
    "    test_data=sample_images,\n",
    "    batch_sizes=batch_sizes_to_test\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Assess if production targets are met\n",
    "\n",
    "Final evaluation against all production deployment requirements. Meeting all targets demonstrates successful optimization for UdaciMed's deployment requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define production targets\n",
    "# Note that we are skipping FLOP analysis here because not directly impacted by hardware acceleration\n",
    "PRODUCTION_TARGETS = {\n",
    "    'memory': 100,               # MB - Achievable with mixed precision\n",
    "    'throughput': 2000,          # samples/sec - Target for multi-tenant deployment\n",
    "    'latency': 3,                # ms - Individual inference time for real-time scenarios\n",
    "    'sensitivity': 98,           # % - Clinical safety requirement (non-negotiable)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Extract the best batch configuration from the benchmark results\n",
    "\n",
    "# Initialize variables to hold the best results found.\n",
    "latency_for_target = float('inf')\n",
    "max_throughput = 0\n",
    "best_throughput_bs = None\n",
    "memory_at_max_throughput = 0\n",
    "\n",
    "# Check if the real-time latency scenario (batch size 1) was tested.\n",
    "if 1 in benchmark_results:\n",
    "    latency_for_target = benchmark_results[1]['avg_latency_ms']\n",
    "else:\n",
    "    print(\"WARNING: Batch size 1 not found in results. Real-time latency target cannot be evaluated.\")\n",
    "\n",
    "# Find the batch size that yielded the highest throughput.\n",
    "if benchmark_results:\n",
    "    best_throughput_bs = max(benchmark_results, key=lambda bs: benchmark_results[bs]['throughput_sps'])\n",
    "    max_throughput = benchmark_results[best_throughput_bs]['throughput_sps']\n",
    "    memory_at_max_throughput = benchmark_results[best_throughput_bs]['peak_memory_mb']\n",
    "\n",
    "# Get model file size as another memory metric\n",
    "model_file_size_mb = Path(onnx_model_path).stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(\"\\n--- Performance Analysis ---\")\n",
    "print(f\"Real-time Latency (BS=1): {f'{latency_for_target:.3f} ms' if latency_for_target != float('inf') else 'Not Tested'}\")\n",
    "if best_throughput_bs is not None:\n",
    "    print(f\"Max Throughput: {max_throughput:,.2f} samples/sec (at Batch Size={best_throughput_bs})\")\n",
    "    print(f\"Peak GPU memory at max throughput: {memory_at_max_throughput:.2f} MB\")\n",
    "print(f\"Model file size: {model_file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Define a function to validate the clinical performance using the ONNX session.\n",
    "\n",
    "def validate_clinical_performance(session: ort.InferenceSession, \n",
    "                                  test_loader, \n",
    "                                  threshold: float = 0.5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validates clinical performance (sensitivity) using the ONNX Runtime session.\n",
    "    \"\"\"\n",
    "    print(\"\\nValidating clinical performance on test data...\")\n",
    "    input_name, _, input_dtype = get_input_details(session)\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_inputs, batch_labels in test_loader:\n",
    "        # Prepare input\n",
    "        input_array = batch_inputs.cpu().numpy().astype(input_dtype)\n",
    "        \n",
    "        # Run inference\n",
    "        results = session.run([output_name], {input_name: input_array})\n",
    "        logits = torch.from_numpy(results[0])\n",
    "        \n",
    "        # Process output\n",
    "        probabilities = torch.softmax(logits, dim=1)[:, 1] # Probability of class 1 (pneumonia)\n",
    "        all_predictions.extend(probabilities.cpu().numpy())\n",
    "        all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    predictions = np.array(all_predictions)\n",
    "    labels = np.array(all_labels).flatten()\n",
    "    pred_classes = (predictions > threshold).astype(int)\n",
    "    \n",
    "    tp = np.sum((pred_classes == 1) & (labels == 1))\n",
    "    fn = np.sum((pred_classes == 0) & (labels == 1))\n",
    "    \n",
    "    sensitivity = (tp / (tp + fn)) * 100 if (tp + fn) > 0 else 0\n",
    "    print(f\"Clinical validation completed on {len(labels)} samples.\")\n",
    "    print(f\"  Calculated Sensitivity: {sensitivity:.2f}% (at threshold={threshold})\")\n",
    "    \n",
    "    return {'sensitivity': sensitivity}\n",
    "\n",
    "\n",
    "# TODO: Choose a clinical threshold for classification.\n",
    "# GOAL: Set a decision threshold for classifying a case as pneumonia.\n",
    "# HINT: This value is often determined through clinical studies. A higher threshold\n",
    "# might reduce false positives but could lower sensitivity. We need to ensure we\n",
    "# still meet the sensitivity target with the chosen value.\n",
    "clinical_threshold =  # Float; Add your value here \n",
    "\n",
    "clinical_results = validate_clinical_performance(\n",
    "    session=inference_session,\n",
    "    test_loader=test_loader,\n",
    "    threshold=clinical_threshold\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Manually set the FLOPS target % reduction met given your results from Notebook 2\n",
    "flops_target_reduction = 80\n",
    "flops_achieved_reduction =  # Float (%); Add your value here\n",
    "flp_ok =  # Boolean; Add your value here\n",
    "\n",
    "# Check if targets are met\n",
    "mem_ok = model_file_size_mb < PRODUCTION_TARGETS['memory']\n",
    "lat_ok = latency_for_target < PRODUCTION_TARGETS['latency']\n",
    "thr_ok = max_throughput > PRODUCTION_TARGETS['throughput']\n",
    "sen_ok = clinical_results['sensitivity'] > PRODUCTION_TARGETS['sensitivity']\n",
    "all_ok = all([mem_ok, lat_ok, thr_ok, sen_ok, flp_ok])\n",
    "\n",
    "print(f\"| Metric          | Target                    | Achieved                  | Status  |\")\n",
    "print(f\"|-----------------|---------------------------|---------------------------|---------|\")\n",
    "print(f\"| Memory          | < {PRODUCTION_TARGETS['memory']} MB                  | {model_file_size_mb:.2f} MB                   | {'✔️ Met' if mem_ok else '✖️ Missed'}  |\")\n",
    "print(f\"| Latency         | < {PRODUCTION_TARGETS['latency']} ms                    | {latency_for_target:.3f} ms                  | {'✔️ Met' if lat_ok else '✖️ Missed'}  |\")\n",
    "print(f\"| Throughput      | > {PRODUCTION_TARGETS['throughput']:,} samples/sec       | {max_throughput:,.2f} samples/sec     | {'✔️ Met' if thr_ok else '✖️ Missed'}  |\")\n",
    "print(f\"| FLOP Reduction  | > {flops_target_reduction}%                     | {flops_achieved_reduction:.1f}%                     | {'✔️ Met' if flp_ok else '✖️ Missed'}  |\")\n",
    "print(f\"| Sensitivity     | > {PRODUCTION_TARGETS['sensitivity']}%                     | {clinical_results['sensitivity']:.2f}%                    | {'✔️ Met' if sen_ok else '✖️ Missed'}  |\")\n",
    "print(f\"\\nOverall Result: {'CONGRATS: All production targets met!' if all_ok else 'WARNING: Some targets were not met. Further optimization may be needed.'}\")\n",
    "print(f\"\\nNOTE: This analysis does not consider FLOPs which can are not improved through hardware acceleration; please check your results on this metric from notebook 2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Cross-platform deployment analysis\n",
    "\n",
    "We have successfully optimized our model to meet _UdaciMed's Universal Performance Standard_ on our standardized target device. \n",
    "\n",
    "With ONNX, we can easily deploy this optimized model across UdaciMed's diverse hardware fleet just by [changing the Execution Providers](https://onnxruntime.ai/docs/execution-providers/):\n",
    "\n",
    "| Deployment Target\t| Recommended Technology |\tPrimary Goal\t |\tKey Trade-Off | \n",
    "| :--- | :--- | :--- | :--- |\n",
    "| GPU Server (Cloud/On-Prem) |\t\tONNX Runtime + TensorRT\t\t |Max Throughput \t |\tHighest performance vs. more complex setup. | \n",
    "| CPU Workstation (Hospital) |\t\tONNX Runtime + OpenVINO\t\t |Low Latency  |\t\tExcellent CPU speed vs. being tied to Intel hardware. | \n",
    "| Mobile/Edge Device (Clinic) |\t\tONNX Runtime Mobile\t\t | Small Footprint  |\t\tMaximum portability vs. reduced model precision (quantization). | \n",
    "\n",
    "But **what if we need to squeeze out every last drop of performance from each deployment target?** To do this, let's consider moving beyond the portable ONNX format and use specialized, hardware-specific frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7.1: Optimization strategy for specialized GPU server deployment**\n",
    "\n",
    "We've established a strong performance baseline using the standard ONNX Runtime with its CUDA Execution Provider (EP). \n",
    "\n",
    "Now, let's explore more advanced options to see if we can unlock even greater performance or add production-grade features for our high-demand GPU deployments.\n",
    "\n",
    "#### TODO: Analyze GPU Deployment Options\n",
    "\n",
    "For a production environment, we need to decide not just if we use a GPU, but _how we use it_.\n",
    "\n",
    "_<\\<Complete the table below by filling in missing performance expectations\\>>_\n",
    "\n",
    "| Approach | How it Works | Key Performance Contributor | Complexity/Overhead | UdaciMed Suitability |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **ONNX Runtime with CUDA Execution Provider** | _(Our Baseline)_ Executes the ONNX graph directly on the GPU using CUDA libraries. | Good (fast, direct GPU access) | Low (simple library integration) | Excellent for direct application integration. |\n",
    "| **ONNX Runtime with TensorRT Execution Provider** |  |  |  |\n",
    "| **Triton Inference Server with TensorRT backend** |  |  |  |\n",
    "\n",
    "_<<Briefly answer the questions below based on UdaciMed's hospital deployment requirements>>_\n",
    "\n",
    "**1. What is the main business risk of choosing the TensorRT path over the CUDA EP baseline?**\n",
    "<br>_HINT: Think compatibility and portability._\n",
    "\n",
    "**2. Why might a small clinic with a single on-premise GPU workstation not want the complexity of Triton, even if it offers advanced features?**\n",
    "<br>_HINT: Think of management overhead_\n",
    "\n",
    "#### TODO: Make your strategic choice\n",
    "\n",
    "Based on your analysis, choose the best GPU server deployment approach for UdaciMed's long-term goal of a multi-tenant service.\n",
    "\n",
    "**My recommendation for UdaciMed's GPU server deployment:** \n",
    "\n",
    "_<<Choose one approach and justify your decision in 1-2 sentences>>_\n",
    "\n",
    "\n",
    "#### TODO: Fix this Triton Inference Server configuration \n",
    "\n",
    "Explain how to extend the following Triton configuration to introduce mixed-precision and dynamic batching.\n",
    "\n",
    "```config.pbtxt\n",
    "\n",
    "name: \"udacimed_pneumonia_prod\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 64\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32 \n",
    "    dims: [ 3, 64, 64 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 2 ]\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "<<Review the Triton documentation and explain how to add the requested hardware accelerations in 1-2 sentences.>>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7.2: Optimization strategy for specialized CPU deployment**\n",
    "\n",
    "Deploying on CPUs is critical for UdaciMed's success, as most hospitals and clinics rely on standard workstations without dedicated GPUs. Let's analyze CPU options for UdaciMed's hospital deployment!\n",
    "\n",
    "> **Numerical precision opportunities with GPU and CPU**: CPUs don't benefit from FP16 (most CPUs only emulate FP16). But CPUs supports another type of numerical optimization, remember?\n",
    "\n",
    "#### TODO: Analyze CPU deployment options\n",
    "\n",
    "While our ONNX model can run on any CPU, using specialized execution providers can unlock significant performance gains, especially on Intel hardware.\n",
    "\n",
    "_<\\<Complete the table below by filling in missing performance expectations\\>>_\n",
    "\n",
    "| Approach | How it Works | Conversion Path | Memory Footprint | Performance | UdaciMed Suitability |\n",
    "|----------|--------------|-----------------|------------------|-------------| ---------------------| \n",
    "| **PyTorch on CPU** | The original, un-optimized model running directly on the CPU.| Direct (no conversion) | High (includes Python interpreter overhead)| Baseline (slowest) | A good reference point, but not for production. |\n",
    "| **ONNX Runtime with Default CPU** |  |  |  |  |  |\n",
    "| **ONNX Runtime with OpenVINO** |  |  |  |  |  |\n",
    "| **OpenVINO** |  |  |  |  |  |\n",
    "| **OpenVINO Backend for Triton** |  |  |  |  |  |\n",
    "\n",
    "_<\\<Briefly answer the questions below based on UdaciMed's hospital deployment requirements>>_\n",
    "\n",
    "**1. What is the key advantage of converting the model to \"Native OpenVINO IR\" over simply using the ONNX + OpenVINO EP, and when would it be worth the extra effort?**\n",
    "<br>_HINT: Think of the advantages of specialized frameworks on their target devices._\n",
    "\n",
    "**2. Triton Server has the \"Highest\" memory overhead. When would it ever make sense to use it for a CPU-based deployment?**\n",
    "<br>_HINT: Think of centralization._\n",
    "\n",
    "**3. No matter which of the five options is chosen, what is the single most important metric to re-validate to ensure clinical safety?**\n",
    "<br>_HINT: Does model transformation across frameworks come with numerical changes?_\n",
    "\n",
    "#### TODO: Make your strategic choice\n",
    "\n",
    "Based on your analysis, choose the best CPU deployment approach for UdaciMed's typical hospital workstation client.\n",
    "\n",
    "**My recommendation for UdaciMed's hospital CPU deployment:** \n",
    "\n",
    "_<\\<Choose one approach and justify your decision in 1-2 sentences>>_\n",
    "\n",
    "#### TODO: Define an optimal CPU deployment configuration in OpenVINO\n",
    "\n",
    "Imagine you are testing out CPU deployment with OpenVINO for UdaciMed, and set up the OpenVINO configuration to balance performance, memory, and clinical safety.\n",
    "\n",
    "_<\\<Complete the OpenVINO configuration below>>_\n",
    "\n",
    "```yaml\n",
    "# openvino_hospital_config.yaml\n",
    "# UdaciMed Hospital Workstation Deployment Configuration\n",
    "\n",
    "model_optimization:\n",
    "  input_model: \"udacimed_pneumonia_optimized.onnx\"\n",
    "  target_device: \"CPU\"\n",
    "  \n",
    "  # Choose precision strategy\n",
    "  precision: # TODO - Options: \"FP32\" (safe), \"FP16\", or \"INT8\" (faster, smaller, but clinical risk)\n",
    "  \n",
    "  # Set optimization priority  \n",
    "  optimization_level: # TODO - Options: \"ACCURACY\" (safe) or \"PERFORMANCE\" (faster)\n",
    "  \n",
    "  # Configure quantization (if using INT8)\n",
    "  quantization:\n",
    "    enabled:  # TODO: true/false\n",
    "    calibration_dataset_size:  # TODO - Number of samples for INT8 calibration (if enabled)\n",
    "\n",
    "deployment_config:\n",
    "  # Configure CPU utilization for hospital workstations\n",
    "  cpu_threads: # TODO - Options: 1, 2, 4, 8 (consider multi-tenancy impact)\n",
    "  \n",
    "  # Set memory allocation for multi-tenant deployment\n",
    "  memory_pool_mb: # TODO - Memory budget per model instance\n",
    "  \n",
    "  # Choose batching strategy\n",
    "  max_batch_size: # TODO - 1 (single patient) or higher (if implementing manual batching)\n",
    "  \n",
    "  # Configure for hospital network environment\n",
    "  inference_timeout_ms: # TODO: Maximum inference time before timeout\n",
    "\n",
    "clinical_validation:\n",
    "  # Define validation requirements after CPU deployment\n",
    "  sensitivity_threshold: # TODO: Minimum acceptable sensitivity (should be >98%)\n",
    "  validation_dataset_size: # TODO: Number of samples for clinical re-validation\n",
    "  comparison_baseline: \"GPU_Triton_deployment\"  # Compare against your GPU results\n",
    "```\n",
    "\n",
    "_<\\<Justify each configuration choice with one sentence each>>_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7.3: Optimization strategy for mobile and edge deployment**\n",
    "\n",
    "UdaciMed's vision extends beyond hospital workstations to portable devices and mobile health applications. This enables pneumonia detection in rural clinics, emergency response, and preventive screening programs where traditional infrastructure is limited.\n",
    "\n",
    "> **Mobile and edge requirements**: These deployments require lightweight runtimes, offline capability, extended battery life, and often benefit from platform-specific optimizations. However, conversion complexity and clinical validation requirements vary significantly across approaches.\n",
    "\n",
    "#### TODO: Analyze mobile deployment options\n",
    "\n",
    "For mobile, the choice between a cross-platform solution and a native, OS-specific framework is the most critical decision, with significant long-term consequences for development and user experience.\n",
    "\n",
    "Here, the primary constraints are not raw speed, but model size, power consumption, and offline capability. We need a model that is small, efficient, and fully self-contained.\n",
    "\n",
    "_<\\<Complete the table below by filling in missing performance expectations\\>>_\n",
    "\n",
    "| Platform | How it Works | Key Strength | Main Trade-Off | UdaciMed Suitability |\n",
    "|----------|----------------|------------|---------------|-------------------|\n",
    "| **ONNX Runtime Mobile** | A cross-platform engine runs a single ONNX file on iOS & Android. | Portability & simplicity | Not the most optimized performance\t | Best for a fast, low-budget launch to reach all users. |\n",
    "| **ExecuTorch** |  |  |  |  |\n",
    "| **LiteRT** |  |  |  |  |\n",
    "| **Core ML (iOS)** |  |  |  |  |\n",
    "\n",
    "_<\\<Answer the questions below based on UdaciMed's mobile and edge deployment strategy>>_\n",
    "\n",
    "**1. What is the key trade-off between ONNX Runtime Mobile's \"simplicity\" and LiteRT's \"smallest size & fastest speed\"?**\n",
    "<br>_HINT: Think of simplicity vs performance._\n",
    "\n",
    "**2. Which frameworks are best suited for a fully offline-capable app for use in rural clinics with no internet, and why?**\n",
    "<br>_HINT: Think about runtime._\n",
    "\n",
    "**3. For a battery-powered portable device, which frameworks would likely offer the best power efficiency, and what is the trade-off?**\n",
    "<br>_HINT: Think about the benefits of specialized accelerations._\n",
    "\n",
    "#### TODO: Make your strategic choice\n",
    "\n",
    "Based on your analysis, choose the best mobile deployment approach for UdaciMed's initial launch.\n",
    "\n",
    "**My recommendation for UdaciMed's mobile and edge deployment strategy:**\n",
    "\n",
    "_<\\<Choose one approach and justify your decision in 1-2 sentences, considering clinical risk, development resources, and global health reach>>_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Congratulations!**\n",
    "\n",
    "You have successfully implemented a complete hardware-accelerated deployment pipeline! Let's recap the decisions you have made and results you have achieved while transforming an optimized model into a production-ready healthcare solution.\n",
    "\n",
    "### **TODO: Production deployment scorecard**\n",
    "\n",
    "**Final GPU deployment performance vs UdaciMed targets:**\n",
    "\n",
    "_<\\<Complete final scorecard based on your benchmarking results:>>_\n",
    "\n",
    "| Metric | Target | Achieved | Status |\n",
    "|--------|--------|----------|--------|\n",
    "| **Memory Usage** | <100MB | | |\n",
    "| **Throughput** | >2,000 samples/sec | | |\n",
    "| **Latency** | <3ms | | | |\n",
    "| **FLOP Reduction** | <0.4 GFLOPs per sample | | | |\n",
    "| **Clinical Safety** | >98% sensitivity | | | |\n",
    "\n",
    "_<\\<Give yourself a final production score given the number of targets met>>_\n",
    "\n",
    "**Overall production score: X/5 targets met!**\n",
    "\n",
    "### **TODO: Strategic deployment insights**\n",
    "\n",
    "_<\\<Reflect on the key decisions you made, and why>>_\n",
    "\n",
    "#### Mixed Precision Strategy\n",
    "**Your FP16/FP32 choice:** # _(FP32, FP16)_\n",
    "\n",
    "**Why you made this decision:**\n",
    "\n",
    "#### Backend Selection\n",
    "**Your ONNX execution provider choice:**  _(CPU EP, CUDA EP TensorRT EP, etc.)_\n",
    "\n",
    "**Why this backend aligned with UdaciMed's requirements:**\n",
    "\n",
    "#### Batching Configuration\n",
    "**Your dynamic batching setup:** # _(preferred batch sizes, queue delay, etc.)_\n",
    "\n",
    "**How this supports diverse clinical deployments:** \n",
    "\n",
    "### Optimization Philosophy\n",
    "**Meeting targets vs maximizing metrics:**\n",
    "\n",
    "_<\\<What did you learn about when to stop optimizing and why?>>_\n",
    "\n",
    "---\n",
    "\n",
    "**You have completed the full journey from architectural optimization to production-ready deployment, demonstrating the technical skills and strategic thinking essential for deploying AI in healthcare. Your UdaciMed pneumonia detection system is now ready to serve hospitals worldwide while maintaining the clinical safety standards that save lives.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
