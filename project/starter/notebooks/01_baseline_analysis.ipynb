{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciMed | Notebook 1: Baseline Analysis\n",
    "\n",
    "Welcome to UdaciMed! You are a Machine Learning Engineer tasked with optimizing our chest X-ray pneumonia detection model for production deployment.\n",
    "\n",
    "- **The challenge**: Before our new model can be approved for production, we must verify that it meets our strict performance SLAs. Deploying an unoptimized model could lead to high operational costs and poor performance across our diverse hardware fleetâ€”from shared T4 GPU cloud instances to portable clinic devices.\n",
    "\n",
    "- **Your mission**: Optimize the model to pass _UdaciMed's Universal Performance Standard_, a strict production service level agreement (SLA) that must be met using the universally compatible ONNX format on our standardized target device.\n",
    "\n",
    "### **Optimization goals**\n",
    "\n",
    "Your goal is to ensure the production model meets these production targets on our standardized development machine:\n",
    "\n",
    "- **< 0.4 GFLOPs per sample**: Floating Point Operations determine computational cost - reducing FLOPs is the most critical step toward broad-platform efficiency.\n",
    "- **< 100MB peak memory footprint**: Total memory consumption (parameters + activations + workspace) during inference - essential for running the model on memory-constrained edge devices and for enabling cost-effective multi-model environments in the cloud.\n",
    "- **< 3ms latency**: This ensures a real-time user experience. We will measure both *amortized latency* (average time per sample in a large batch) and *true latency* (time for a single-image inference), as both are important for different use cases.\n",
    "- **> 2000 samples/second throughput**: This specific target is for our high-performance hardware, like the reference T4 GPU. Meeting this goal proves the model is cost-effective and scalable for high-volume, server-side screening workflows.\n",
    "- **> 98% sensitivity***: This is a non-negotiable clinical safety requirement. We must ensure that a threshold percentage of all true pneumonia cases are correctly identified. All optimizations must be validated against this metric.\n",
    "\n",
    "#### **A note on our standardized target device**\n",
    "\n",
    "All performance targets in this project must be met on our official _\"standardized target device.\"_ This is an NVIDIA T4 GPU, a common and versatile datacenter GPU that represents a typical cloud deployment environment.\n",
    "\n",
    "By using a single, consistent hardware profile (NVIDIA T4 with 16GB VRAM, CUDA 12.4) for all our performance SLAs, we can:\n",
    "\n",
    "- *Ensure Reproducible Results*: Anyone on the team can validate performance and get consistent measurements.\n",
    "\n",
    "- *Create a Reliable Benchmark*: It provides a stable baseline to measure the impact of every optimization we make.\n",
    "\n",
    "If a model can meet our strict, universal performance standards on this reference hardware, we are confident it will perform well across our entire fleet of production devices.\n",
    "\n",
    "---\n",
    "\n",
    "Through this notebook, you will build the foundation for our optimization strategy by:\n",
    "\n",
    "1.  **Establishing baseline performance** with comprehensive profiling.\n",
    "2.  **Analyzing the primary bottlenecks**, distinguishing between compute, parameter memory, and activation memory.\n",
    "3.  **Identifying optimization opportunities** in both the model architecture and the deployment configuration.\n",
    "\n",
    "**Let's set up an optimization vision for UdaciMed's next-generation diagnostic platform!**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_\\*Understanding medical AI requirements_**:\n",
    "> \n",
    "> In medical AI, sensitivity (recall) is often more critical than overall accuracy. Missing a pneumonia case (false negative) can be life-threatening, while a false positive \"only\" leads to additional human review. This is why we prioritize sensitivity as our safety constraint."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up the environment\n",
    "The first step is to import all libraries and internal functionalities (from `utils`). \n",
    "\n",
    "Additionally, we set `pytorch` to use CUDA GPU if available (not only for faster execution, but also for benchmarking, as this will be our final deployment target!) and we include deterministic mode for reproducible benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import inspect\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import random\n",
    "from torchsummary import summary\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project utilities\n",
    "from utils.data_loader import (\n",
    "    load_pneumoniamnist, \n",
    "    get_dataset_info, \n",
    "    explore_dataset_splits,\n",
    "    visualize_sample_images,\n",
    "    get_sample_batch\n",
    ")\n",
    "from utils.model import (\n",
    "    create_baseline_model,\n",
    "    get_model_info,\n",
    "    count_parameters_by_type,\n",
    "    train_baseline_model,\n",
    "    plot_training_history\n",
    ")\n",
    "from utils.evaluation import (\n",
    "    evaluate_with_multiple_thresholds\n",
    ")\n",
    "from utils.profiling import (\n",
    "    PerformanceProfiler,\n",
    "    get_gpu_info,\n",
    "    check_environment,\n",
    "    measure_time\n",
    ")\n",
    "from utils.visualization import (\n",
    "    plot_dataset_distribution,\n",
    "    plot_performance_profile,\n",
    "    plot_operation_breakdown,\n",
    "    plot_batch_size_comparison,\n",
    ")\n",
    "\n",
    "# Check environment and GPU capabilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = get_gpu_info()\n",
    "    print(f\"GPU: {gpu_info.get('name', 'Unknown')}\")\n",
    "    print(f\"GPU Memory: {gpu_info.get('memory_total_gb', 0):.1f} GB\")\n",
    "    print(f\"Tensor Core Support: {gpu_info.get('tensor_core_support', False)}\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available - profiling will be limited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility across optimization experiments\n",
    "def set_deterministic_mode(seed=42):\n",
    "    \"\"\"\n",
    "    Enable deterministic mode for consistent benchmarking.\n",
    "    Critical for fair comparison between different techniques.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable for consistent timing\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_deterministic_mode(42)\n",
    "print(\"Deterministic mode enabled for reproducible benchmarking\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and analyze the dataset\n",
    "Now, we can get started with our baseline model set-up by loading the data and understanding its characteristics. \n",
    "\n",
    "For this project, we use the PneumoniaMNIST dataset from [MedMNIST](https://medmnist.com/). PneumoniaMNIST provides a standardized, validated dataset for pneumonia detection research. Its 64x64 resolution balances clinical detail with computational efficiency, making it ideal for optimization studies while maintaining diagnostic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset information\n",
    "dataset_info = get_dataset_info(use_binary=True)\n",
    "print(\"PneumoniaMNIST Dataset Information:\")\n",
    "for key, value in dataset_info.items():\n",
    "    print(f\"   {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration for baseline analysis\n",
    "CONFIG = {\n",
    "    'image_size': 64,  # Balanced for for memory usage and model accuracy\n",
    "    'num_classes': 2,  # Binary classification: normal vs pneumonia\n",
    "    'batch_size': 32,  # Balanced for memory usage and training stability\n",
    "    'subset_size': None,  # Use full dataset for production-representative results\n",
    "}\n",
    "\n",
    "# Load the dataset with optimized settings\n",
    "print(\"Loading PneumoniaMNIST dataset...\")\n",
    "\n",
    "with measure_time(\"Dataset loading\"):\n",
    "    train_loader = load_pneumoniamnist(\n",
    "        split=\"train\", download=True, \n",
    "        size=CONFIG['image_size'], batch_size=CONFIG['batch_size'], \n",
    "        subset_size=CONFIG['subset_size'] * 0.7 if CONFIG['subset_size'] is not None else None\n",
    "    )\n",
    "    \n",
    "    val_loader = load_pneumoniamnist(\n",
    "        split=\"val\", download=False, \n",
    "        size=CONFIG['image_size'], batch_size=CONFIG['batch_size'], \n",
    "        subset_size=CONFIG['subset_size'] * 0.15 if CONFIG['subset_size'] is not None else None\n",
    "    )\n",
    "    \n",
    "    test_loader = load_pneumoniamnist(\n",
    "        split=\"test\", download=False, \n",
    "        size=CONFIG['image_size'], batch_size=CONFIG['batch_size'], \n",
    "        subset_size=CONFIG['subset_size'] * 0.15 if CONFIG['subset_size'] is not None else None\n",
    "    )\n",
    "\n",
    "print(f\"Dataset loaded: {CONFIG['image_size']}x{CONFIG['image_size']} images, batch_size={CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset distribution for class imbalance considerations\n",
    "print(\"Analyzing dataset distribution...\")\n",
    "dataset_splits = explore_dataset_splits(train_loader, val_loader, test_loader)\n",
    "print(f\"\\nDataset Summary: {dataset_splits}\")\n",
    "\n",
    "# Visualize dataset distribution\n",
    "plot_dataset_distribution(dataset_splits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_The impact of class imbalance_**\n",
    "> \n",
    "> Medical datasets often have class imbalance. This affects optimization because:\n",
    "> \n",
    "> - Models may focus compute on majority class features\n",
    "> - Batch composition affects memory usage patterns\n",
    "> - Some optimization techniques (like pruning) may disproportionately affect minority class performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images to understand data characteristics\n",
    "print(\"Sample chest X-ray images:\")\n",
    "visualize_sample_images(train_loader, num_samples=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create and analyze the baseline model\n",
    "We will use [ResNet-18](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) as our baseline - a popular choice for medical imaging that balances accuracy and efficiency.\n",
    "\n",
    "The original model structure is architected for ImageNet (1000 classes), so we modify the model with a custom head to support our new classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the baseline ResNet-18 model\n",
    "print(\"Creating ResNet-18 baseline model...\")\n",
    "\n",
    "baseline_model = create_baseline_model(\n",
    "    num_classes=CONFIG['num_classes'], \n",
    "    input_size=CONFIG['image_size'], \n",
    "    pretrained=False  # Training from scratch for fair optimization comparison\n",
    ")\n",
    "baseline_model = baseline_model.to(device)\n",
    "\n",
    "print(f\"Baseline model created and deployed to {device}\")\n",
    "print(f\"   Architecture: {baseline_model.architecture_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model information\n",
    "model_info = get_model_info(baseline_model)\n",
    "\n",
    "print(f\"\\nModel Information:\")\n",
    "print(f\"   Architecture: {model_info['architecture']}\")\n",
    "print(f\"   Total Parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"   Model Size: {model_info['model_size_mb']:.1f} MB\")\n",
    "print(f\"   Input Size: {model_info['input_size']}x{model_info['input_size']}\")\n",
    "\n",
    "# Analyze layer composition \n",
    "layer_breakdown = model_info['layer_breakdown']\n",
    "print(f\"\\nLayer Composition:\")\n",
    "print(f\"   Convolution Layers: {layer_breakdown['conv_layers']['count']} ({layer_breakdown['conv_layers']['total_params']:,} params)\")\n",
    "print(f\"   Linear Layers: {layer_breakdown['linear_layers']['count']} ({layer_breakdown['linear_layers']['total_params']:,} params)\")\n",
    "print(f\"   Normalization Layers: {layer_breakdown['norm_layers']['count']}\")\n",
    "print(f\"   Activation Types: {', '.join(layer_breakdown['activation_layers']['types'])}\")\n",
    "\n",
    "# Get parameter distribution\n",
    "if 'parameter_distribution' in layer_breakdown:\n",
    "    param_dist = layer_breakdown['parameter_distribution']\n",
    "    print(f\"\\nParameter Distribution:\")\n",
    "    print(f\"   Convolution: {param_dist['conv_percentage']:.2f}%\")\n",
    "    print(f\"   Linear: {param_dist['linear_percentage']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model architecture\n",
    "summary(baseline_model, input_size=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Brainstorming time: Optimizations by layer type_**\n",
    "> \n",
    "> Try to remember from the course, which architectural optimizations most benefit each layer type? Looking at the layer composition, our model is convolution-heavy - this is your starting point!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train and evaluate baseline model\n",
    "Now, we define the baseline model and evaluate its performance on key accuracy metrics - all future optimizations should maintain a similar clinical performance standard.\n",
    "\n",
    "Establishing robust baseline metrics is crucial for medical AI. Any optimization must preserve clinical safety while improving computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'num_epochs': 10,           # Sufficient for convergence analysis\n",
    "    'learning_rate': 0.0003,    # Conservative rate for stable training\n",
    "    'lr_step_size': 3,          # Learning rate decay schedule\n",
    "    'weight_decay': 1e-4,       # Regularization for generalization\n",
    "    'patience': 3               # Early stopping for efficiency\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "baseline_model, training_history = train_baseline_model(\n",
    "    baseline_model, train_loader, val_loader, device, TRAINING_CONFIG\n",
    ")\n",
    "\n",
    "# Plot training curves with analysis\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Training insights: What does the training history tell us?**\n",
    "> \n",
    "> The initial low validation accuracy is due to extreme overfitting due to the small dataset size in comparison to the model's parameter size. The presence of early plateaus and fast convergence also highlight that the architecture has high representational power - we can likely apply aggressive optimization without accuracy degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance with multiple thresholds \n",
    "print(f\"Running eval benchmark on {dataset_splits['test']['total']} test samples ...\")\n",
    "eval_results = evaluate_with_multiple_thresholds(baseline_model, test_loader, device, [0.4, 0.7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Clinical threshold selection**\n",
    ">\n",
    "> Different thresholds optimize for different clinical scenarios. Lower thresholds (0.4) maximize sensitivity for screening, while higher thresholds (0.7) balance precision and recall for diagnostic confirmation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Profile baseline model for latency, throughput, and memory usage\n",
    "\n",
    "Comprehensive performance profiling forms the foundation of our optimization strategy. We will measure all key metrics that impact multi-tenant deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize performance profiler\n",
    "profiler = PerformanceProfiler(device=str(device))\n",
    "print(f\"Performance profiler initialized for {device}\")\n",
    "\n",
    "# Get sample batch for profiling\n",
    "sample_images, sample_labels = get_sample_batch(val_loader)\n",
    "sample_images = sample_images.to(device)\n",
    "sample_labels = sample_labels.to(device)\n",
    "\n",
    "print(f\"\\nSample batch for profiling:\")\n",
    "print(f\"   Batch shape: {sample_images.shape}\")\n",
    "print(f\"   Memory usage: {sample_images.numel() * sample_images.element_size() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile inference timing\n",
    "print(\"Profiling inference timing...\")\n",
    "\n",
    "timing_results = profiler.profile_inference_time(\n",
    "    model=baseline_model,\n",
    "    input_tensor=sample_images,\n",
    "    num_runs=100,       # Sufficient for statistical significance\n",
    "    warmup_runs=10      # GPU warmup for consistent measurements\n",
    ")\n",
    "\n",
    "print(f\"\\nTiming Results:\")\n",
    "print(f\"   Single Sample Latency: {timing_results['single_sample_ms']:.2f} ms\")\n",
    "print(f\"   (Single) Batch Throughput: {timing_results['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "print(f\"   Batch Latency: {timing_results['batch_total_ms']:.2f} ms\")\n",
    "print(f\"   Batch Throughput: {timing_results['batch_throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "print(f\"   Mean Inference Time: {timing_results['mean_ms']:.2f} ms\")\n",
    "print(f\"   95th Percentile: {timing_results['p95_ms']:.2f} ms\")\n",
    "print(f\"   Standard Deviation: {timing_results['std_ms']:.2f} ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Latency vs throughput trade-offs**\n",
    ">\n",
    "> Single sample latency measures real-time diagnostic speed, while batch throughput indicates multi-tenant efficiency. Both metrics are crucial for different deployment scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile FLOPs for computational efficiency analysis\n",
    "flops_results = profiler.profile_flops(\n",
    "    model=baseline_model, \n",
    "    input_tensor=sample_images\n",
    ")\n",
    "\n",
    "if 'error' in flops_results:\n",
    "    print(f\"FLOPs calculation failed: {flops_results['error']}\")\n",
    "else:\n",
    "    print(f\"\\nFLOPs Results:\")\n",
    "    print(f\"   Total: {flops_results['total_gflops']:.2f} GFLOPs\")\n",
    "    print(f\"   Per Sample: {flops_results['gflops_per_sample']:.2f} GFLOPs\")\n",
    "    if 'module_percentage' in flops_results and flops_results['module_percentage']:\n",
    "        print(f\"\\n   Top Operations (by FLOPs):\")\n",
    "        for module, percentage in list(flops_results['module_percentage'].items())[:5]:\n",
    "            gflops = flops_results['module_breakdown_gflops'][module]\n",
    "            print(f\"     {module}: {percentage:.1f}% ({gflops:.2f} GFLOPs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile GPU memory usage\n",
    "print(\"Profiling GPU memory usage...\")\n",
    "\n",
    "memory_results = profiler.profile_memory_usage(\n",
    "    model=baseline_model,\n",
    "    input_tensor=sample_images\n",
    ")\n",
    "\n",
    "if 'error' not in memory_results:\n",
    "    print(f\"\\nMemory Results:\")\n",
    "    print(f\"   Peak GPU Memory: {memory_results['peak_memory_mb']:.1f} MB\")\n",
    "    print(f\"   Memory Increase: {memory_results['memory_increase_mb']:.1f} MB\")\n",
    "    \n",
    "    # Component breakdown\n",
    "    if 'component_breakdown' in memory_results:\n",
    "        components = memory_results['component_breakdown']\n",
    "        print(f\"\\nMemory Component Breakdown:\")\n",
    "        for component, usage in components.items():\n",
    "            print(f\"   {component.replace('_', ' ').title()}: {usage:.1f} MB\")\n",
    "else:\n",
    "    print(f\"WARNING: Memory profiling error: {memory_results['error']}\")\n",
    "    memory_results = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Did you notice? A major optimization opportunity hiding in plain sight!_**\n",
    "> \n",
    "> Look carefully at the model summary above. Something doesn't add up with our input/output dimensions...\n",
    "> \n",
    "> Compare the input size we are feeding (64x64) with the first convolution layer's output size. The first Conv2d layer shows output `[-1, 64, 112, 112]` but our input is only 64x64. Where are those extra pixels coming from? Complete the TODO below to find out.\n",
    "> \n",
    "> *Optimization opportunity:* This might be your biggest single optimization win, in both speed and memory usage! Keep this insight in mind as you analyze the profiling results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the baseline model's forward method using `inspect.get_source()`\n",
    "print(\"Manually inspect the ResNetBaseline forward method:\")\n",
    "baseline_model_forward = # TODO: Add your code here\n",
    "print(baseline_model_forward)\n",
    "\n",
    "print(\"\\nDiscussion questions:\")\n",
    "print(\"1. What happens when height != self.target_size? What is self.target_size set to?\") \n",
    "print(\"2. How much computational and memory overhead does F.interpolate add? (Hint: Compare 64x64 vs 224x224 pixel counts)\")\n",
    "print(\"3. Is this interpolation necessary for pneumonia detection, or just a legacy from ImageNet pretraining?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed PyTorch profiler\n",
    "print(\"Running detailed PyTorch profiler...\")\n",
    "\n",
    "detailed_results = profiler.profile_with_pytorch_profiler(\n",
    "    model=baseline_model,\n",
    "    input_tensor=sample_images,\n",
    "    num_steps=10        # Sufficient for operation breakdown analysis\n",
    ")\n",
    "\n",
    "if 'error' not in detailed_results:\n",
    "    print(f\"\\nOperation Breakdown:\")\n",
    "    op_breakdown = detailed_results['operation_breakdown']\n",
    "    \n",
    "    # Show top operations for optimization targeting\n",
    "    sorted_ops = sorted(op_breakdown.items(), key=lambda x: x[1], reverse=True)\n",
    "    for op_type, percentage in sorted_ops:\n",
    "        if percentage > 1:  # Only show operations > 1%\n",
    "            print(f\"   {op_type.replace('_', ' ').title()}: {percentage:.1f}%\")\n",
    "else:\n",
    "    print(f\"WARNING: Detailed profiling error: {detailed_results['error']}\")\n",
    "    detailed_results = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize and save baseline model's performance\n",
    "\n",
    "Comprehensive visualization helps understand optimization opportunities and track progress across optimization experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance profile\n",
    "plot_performance_profile(timing_results)\n",
    "\n",
    "# Visualize operation breakdown\n",
    "if detailed_results and 'operation_breakdown' in detailed_results:\n",
    "    plot_operation_breakdown(detailed_results['operation_breakdown'])\n",
    "else:\n",
    "    print(\"WARNING: Operation breakdown visualization not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile baseline results for optimization notebooks\n",
    "baseline_results = {\n",
    "    'model_name': 'ResNet-18 Baseline',\n",
    "    'architecture': model_info['architecture'],\n",
    "    'total_parameters': model_info['total_parameters'],\n",
    "    'model_size_mb': model_info['model_size_mb'],\n",
    "    'config': CONFIG,\n",
    "    'eval_results': eval_results,\n",
    "    'timing': timing_results,\n",
    "    'flops': flops_results,\n",
    "    'memory': memory_results,\n",
    "    'operation_breakdown': detailed_results['operation_breakdown'],\n",
    "    'model_info': model_info,\n",
    "    'dataset_info': dataset_info,\n",
    "    'parameter_breakdown': count_parameters_by_type(baseline_model)\n",
    "}\n",
    "\n",
    "# Save baseline results\n",
    "with open('../results/baseline_results.pkl', 'wb') as f:\n",
    "    pickle.dump(baseline_results, f)\n",
    "\n",
    "print(\"Baseline results saved to 'baseline_results.pkl' in the `results/` folder\")\n",
    "print(\"   This will be used for comparison in optimization notebooks.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Brainstorming time!**\n",
    "> \n",
    "> Based on your profiling results above, analyze the following:\n",
    "> \n",
    "> 1. **Primary bottleneck**: What is the main performance bottleneck - compute time, memory usage, or data transfer?\n",
    "> \n",
    "> 2. **Operation analysis**: Which types of operations (convolution, linear, activation) consume the most time? What percentage?\n",
    "> \n",
    "> 3. **Memory patterns**: How much memory does the model use during inference? What contributes most to memory usage?\n",
    "> \n",
    "> 4. **Optimization priority**: Based on the profiling data, which optimization techniques would you prioritize:\n",
    ">    - Architecture modifications (channel reduction, efficient blocks)\n",
    ">    - Precision optimization (mixed precision, quantization)\n",
    ">    - Hardware acceleration (TensorRT for GPU, ...)\n",
    ">   \n",
    ">    _IMPORTANT:_ Did you discover the major inefficiency we hinted at earlier? How much improvement could removing the 64â†’224 interpolation provide?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Analyze optimization opportunities\n",
    "\n",
    "Now that we have established the baseline performance, it's time for you to conduct a deeper analysis that will guide the optimization strategy. This section contains **two focused analysis checkpoints**:\n",
    "\n",
    "1. **Architecture optimization analysis** - Identify specific opportunities in the ResNet-18 architecture\n",
    "2. **Deployment optimization analysis** - Understand hardware acceleration and deployment strategies\n",
    "\n",
    "Complete these analysis checkpoints to develop your optimization roadmap!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis checkpoint 1: Architecture optimization opportunities\n",
    "\n",
    "**Task:** Analyze the ResNet-18 architecture to identify the **top 2 optimization opportunities** from the techniques covered in the course. Available techniques to consider include:\n",
    "- Grouped convolutions\n",
    "- Depth-wise separable convolutions\n",
    "- Inverted residuals with linear bottlenecks\n",
    "- Low-rank factorization\n",
    "- Channel organization strategies _(NOTE: this is a hybrid optimization between architecture and hardware)_\n",
    "- Parameter sharing / weight tying\n",
    "\n",
    "Feel free to skip programmatic analysis of techniques which you deem to be non-applicable or less performant, but provide an explanation here or in the notebook's final markdown cell.\n",
    "\n",
    "**IMPORTANT:** Don't forget to also analyze the potential of interpolation removal from the model's forward method!\n",
    "\n",
    "#### Recommended strategic analysis approach\n",
    "\n",
    "Calculate the expected impact of applying each technique on parameter reduction programmatically to simplify follow-up analysis. Consider how parameter reduction / architectural improvements for each technique correlate with memory size (activation vs parameters), FLOPs, latency, throughput, and sensitivity to estimate optimization opportunity. HINTS are in each function's signature.\n",
    "\n",
    "To populate the analysis dictionary with estimated optimiation opportunity, you can either:\n",
    "1. Programmatically calculate optimization opportunity from parameter reduction and rule-of-thumb\n",
    "2. Directly add the expected value in the analysis entries, and add a one-line explanation of the value selected as an in-line comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement logic for each single technique analysis\n",
    "\n",
    "def analyze_grouped_conv_potential(model, sample_input_shape=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"]), groups=2):\n",
    "    \"\"\"\n",
    "    TODO: Analyze which convolution layers could benefit from grouped convolutions.\n",
    "\n",
    "    HINT: Look for standard convolutions with kernel_size > 1 and sufficient channels that are divisible by groups\n",
    "    \n",
    "    Memory impact: Note that parameter memory is only ~13% of total memory in ResNet-18\n",
    "    FLOP improvements: Update the `in_ch Ã— out_ch Ã— kernelÂ²` calculation for standard conv to account for `groups` division\n",
    "    Speedup: Proportional to FLOP reduction, but may have overhead from grouped operations on **parallel hardware**\n",
    "    Throughput: Can be limited by memory bandwidth and **hardware utilization** patterns for grouped operations.\n",
    "    \n",
    "    Also, note that, some backends (like CUDA/CUDNN) may require specific memory formats (e.g., channels_last) and mixed precision to unlock maximum throughput for grouped convolutions.\n",
    "    This is a known issue: https://discuss.pytorch.org/t/performing-convolutions-in-groups-but-not-grouped-convolution/59412/27\n",
    "    \"\"\"\n",
    "    # Initialize analysis structure (feel free to skip any entries you don't want to programmatically calculate)\n",
    "    analysis = {\n",
    "        'technique': 'Grouped Convolutions',\n",
    "        'groups': groups,\n",
    "        'candidate_layers': [],\n",
    "        'total_candidates': 0,\n",
    "        'avg_flop_reduction_percent': 0,\n",
    "        'avg_param_reduction_percent': 0,\n",
    "        'estimated_speedup': 1.0,\n",
    "        'estimated_memory_reduction_mb': 0,\n",
    "        'estimated_throughput_samples_sec': timing_results['throughput_samples_per_sec'],\n",
    "        'throughput_improvement_percent': 0,\n",
    "        'sensitivity_risk': ''\n",
    "    }\n",
    "\n",
    "    # TODO: Complete the analysis dictionary with relevant values\n",
    "    # HINT:\n",
    "    # 1. Find candidates: for name, module in model.named_modules() if isinstance(module, nn.Conv2d) and module.kernel_size[0] > 1 and module.groups == 1 and module.in_channels % groups == 0\n",
    "    # 2. Calculate per-layer: grouped_params = standard_params // groups, then param_reduction = (standard_params - grouped_params) / standard_params\n",
    "    # 3. Aggregate impact: total_flops_saved += per_layer_savings, then realistic_speedup = 1 / (1 - flop_reduction_ratio * conv_coverage * hardware_efficiency_factor)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def analyze_depthwise_separable_potential(model, sample_input_shape=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"])):\n",
    "    \"\"\"\n",
    "    TODO: Analyze which convolution layers could benefit from depthwise separable convolutions/\n",
    "\n",
    "    HINT: Look for standard convolutions with kernel_size > 1 and sufficient channels (e.g., >=16)\n",
    "    \n",
    "    Memory impact: Note that parameter memory is only ~13% of total memory in ResNet-18\n",
    "    FLOP improvements: Update the `in_ch Ã— out_ch Ã— kernelÂ²` calculation for standard conv to account for two layers now\n",
    "    Speedup: Proportional to FLOP reduction, but can be penalized (e.g., ~0.6x) due to memory-access overhead, as many small operations may not fully utilize hardware parallelism\n",
    "    Throughput: Directly scales with speedup estimates for batch processing scenarios\n",
    "    \"\"\"\n",
    "    # Initialize analysis structure (feel free to skip any entries you don't want to programmatically calculate)\n",
    "    analysis = {\n",
    "        'technique': 'Depthwise Separable Convolutions',\n",
    "        'candidate_layers': [],\n",
    "        'total_candidates': 0,\n",
    "        'avg_flop_reduction_percent': 0,\n",
    "        'avg_param_reduction_percent': 0,\n",
    "        'estimated_speedup': 1.0,\n",
    "        'estimated_memory_reduction_mb': 0,\n",
    "        'estimated_throughput_samples_sec': timing_results['throughput_samples_per_sec'],\n",
    "        'throughput_improvement_percent': 0,\n",
    "        'sensitivity_risk': ''\n",
    "    }\n",
    "\n",
    "    # TODO: Complete the analysis dictionary with relevant values\n",
    "    # HINT: \n",
    "    # 1. Find candidates: for name, module in model.named_modules() if isinstance(module, nn.Conv2d) and module.kernel_size[0] > 1 and module.in_channels >= 16\n",
    "    # 2. Calculate per-layer: separable_params = (in_ch * kÂ²) + (in_ch * out_ch), separable_flops = (feature_size * in_ch * kÂ²) + (feature_size * in_ch * out_ch)\n",
    "    # 3. Aggregate impact: avg_flop_reduction = total_flops_saved / total_candidate_flops, then apply hardware penalty: final_speedup = theoretical_speedup * 0.6    \n",
    "    return analysis\n",
    "\n",
    "def analyze_inverted_residuals_potential(model):\n",
    "    \"\"\"\n",
    "    TODO: Analyze potential for inverted residual blocks (expand -> depthwise -> project).\n",
    "    Hint: Look for 3x3 convolutions that expand channels (MobileNet-style patterns)\n",
    "    \n",
    "    Memory impact: Can reduce average memory but may increase peak during expansion phase\n",
    "    FLOP improvements: Layer-specific FLOP savings through efficient bottleneck patterns\n",
    "    Speedup: 1.1x to 1.2x based on candidate count and architecture structure (variable)\n",
    "    Throughput: Modest improvements proportional to architectural efficiency gains\n",
    "    \"\"\"\n",
    "    # Initialize analysis structure (feel free to skip any entries you don't want to programmatically calculate)\n",
    "    analysis = {\n",
    "        'technique': 'Inverted Residual Blocks',\n",
    "        'residual_candidates': [],\n",
    "        'total_candidates': 0,\n",
    "        'avg_flop_reduction_percent': 0,\n",
    "        'estimated_speedup': 1.0,\n",
    "        'estimated_memory_reduction_mb': 0,\n",
    "        'estimated_throughput_samples_sec': timing_results['throughput_samples_per_sec'],\n",
    "        'throughput_improvement_percent': 0,\n",
    "        'sensitivity_risk': ''\n",
    "    }\n",
    "    \n",
    "    # TODO: Complete the analysis dictionary with relevant values\n",
    "    # HINT:\n",
    "    # 1. Find candidates: for name, module in model.named_modules() if hasattr(module, 'children') and len([m for m in module.children() if isinstance(m, nn.Conv2d)]) >= 3\n",
    "    # 2. Calculate per-layer: Use rule-of-thumb flop_reduction_percent = 60 for inverted residual patterns (expand->depthwise->project)\n",
    "    # 3. Aggregate impact: coverage_factor = min(1.0, total_candidates / 8), then speedup = 1.0 + (avg_reduction / 100) * conv_impact * 2.5\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def analyze_lowrank_factorization_potential(model, batch_size=32):\n",
    "    \"\"\"\n",
    "    TODO: Analyze large linear layers that could benefit from low-rank factorization.\n",
    "    Hint: Focus on layers where input_features * output_features > 10000\n",
    "\n",
    "    Memory impact: Reduces parameter memory through low-rank matrix decomposition\n",
    "    FLOP improvements: Matrix multiply FLOPs reduced proportionally to rank reduction --> batch_size Ã— (in_features Ã— rank + rank Ã— out_features)\n",
    "    Speedup: 1.0xâ€“1.4x depending on layers present, but penalized due to factorization overhead\n",
    "    Throughput: Limited improvement for ResNet-18 due to small linear layers, but scales with any FLOP reductions achieved\n",
    "    \"\"\"\n",
    "    # Initialize analysis structure (feel free to skip any entries you don't want to programmatically calculate)\n",
    "    analysis = {\n",
    "        'technique': 'Low-Rank Factorization',\n",
    "        'factorization_candidates': [],\n",
    "        'total_candidates': 0,\n",
    "        'avg_param_reduction_percent': 0,\n",
    "        'avg_flop_reduction_percent': 0,\n",
    "        'estimated_speedup': 1.0,\n",
    "        'estimated_memory_reduction_mb': 0,\n",
    "        'estimated_throughput_samples_sec': timing_results['throughput_samples_per_sec'],\n",
    "        'throughput_improvement_percent': 0,\n",
    "        'sensitivity_risk': ''\n",
    "    }\n",
    "    \n",
    "    # TODO: Complete the analysis dictionary with relevant values\n",
    "    # HINT:\n",
    "    # 1. Find candidates: for name, module in model.named_modules() if isinstance(module, nn.Linear) and (module.in_features * module.out_features) > 10000\n",
    "    # 2. Calculate per-layer: optimal_rank = int(min_dim * rank_ratio), factorized_params = (in_features * rank) + (rank * out_features)\n",
    "    # 3. Aggregate impact: Limited by matrix_multiply_impact from operation_breakdown, speedup = 1.0 + (param_reduction / 100) * matrix_multiply_impact * 0.7\n",
    "        \n",
    "    return analysis\n",
    "\n",
    "def analyze_channel_organization_potential(model):\n",
    "    \"\"\"\n",
    "    TODO: Analyze channel organization optimizations for hardware efficiency.\n",
    "    Hint: Look for in-place operation opportunities (architceture optimization) and memory layout optimizations (hardware optimization)\n",
    "    \n",
    "    Memory impact: Improved memory access patterns or reduced memory via more efficient channel organization (variable)\n",
    "    FLOP improvements: No FLOP reduction - same operations, just more efficient execution\n",
    "    Speedup: 1.05x-1.10x depending on hardware and implementation (variable)\n",
    "    Throughput: Small but consistent improvements through better hardware utilization patterns\n",
    "    \"\"\"\n",
    "    # Initialize analysis structure (feel free to skip any entries you don't want to programmatically calculate)\n",
    "    analysis = {\n",
    "        'technique': 'Channel Organization (channels_last + in-place ReLU)',\n",
    "        'inplace_opportunities': {},\n",
    "        'estimated_speedup': 1.0,\n",
    "        'estimated_memory_reduction_mb': 0,\n",
    "        'avg_flop_reduction_percent': 0,\n",
    "        'estimated_throughput_samples_sec': timing_results['throughput_samples_per_sec'],\n",
    "        'throughput_improvement_percent': 0,\n",
    "        'sensitivity_risk': ''\n",
    "    }\n",
    "    \n",
    "    # TODO: Complete the analysis dictionary with relevant values\n",
    "    # HINT:\n",
    "    # 1. Find candidates: relu_count = sum(1 for m in model.modules() if isinstance(m, nn.ReLU) and not m.inplace)\n",
    "    # 2. Calculate per-layer: No parameter reduction - this is memory layout optimization (channels_last + in-place operations)\n",
    "    # 3. Aggregate impact: Base speedup = 1.2 for channels_last, then speedup *= (1.0 + relu_count * 0.008) for in-place opportunities\n",
    "\n",
    "    return analysis\n",
    "\n",
    "def analyze_parameter_sharing_potential(model):\n",
    "    \"\"\"\n",
    "    TODO: Analyze potential for parameter sharing across similar layers.\n",
    "    Hint: Use convolution-to-total parameter ratio to estimate redundancy potential.\n",
    "          Higher conv ratio indicates greater opportunity for sharing similar operations.\n",
    "\n",
    "    Memory impact: Reduces parameter memory footprint directly through weight sharing\n",
    "    FLOP improvements: Sharing can reduce redundant computation and improve cache locality, but may also introduce overhead\n",
    "    Speedup: Apply 0.5x multiplier to sharing ratio due to implementation complexity\n",
    "    Throughput: Conservative improvements due to potential overhead from shared parameter access patterns\n",
    "    \"\"\"\n",
    "    # Initialize analysis structure (feel free to skip any entries you don't want to programmatically calculate)\n",
    "    analysis = {\n",
    "        'technique': 'Parameter Sharing',\n",
    "        'similar_layer_groups': [],\n",
    "        'sharing_potential_percent': 0,\n",
    "        'avg_flop_reduction_percent': 0,\n",
    "        'estimated_speedup': 1.0,\n",
    "        'estimated_memory_reduction_mb': 0,\n",
    "        'estimated_throughput_samples_sec': timing_results['throughput_samples_per_sec'],\n",
    "        'throughput_improvement_percent': 0,\n",
    "        'sensitivity_risk': ''\n",
    "    }\n",
    "    \n",
    "    # TODO: Complete the analysis dictionary with relevant values\n",
    "    # HINT:\n",
    "    # 1. Find candidates: Group conv layers by similar shapes: if abs(layer1_channels - layer2_channels) <= threshold and same kernel_size\n",
    "    # 2. Calculate per-layer: For each sharing group, saved_params = sum(params for duplicate layers in group[1:])\n",
    "    # 3. Aggregate impact: sharing_potential = min(0.25, total_shareable_params / total_params), then speedup = 1.0 + sharing_potential * 0.4\n",
    "\n",
    "    return analysis\n",
    "\n",
    "def analyze_interpolation_removal_potential(model, sample_input_shape=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"])):\n",
    "    \"\"\"\n",
    "    TODO: Analyze the potential for removing interpolation overhead by processing images at native resolution.\n",
    "    HINT: This is the biggest single optimization opportunity! Focus on calculating the computational overhead from 64x64â†’224x224 interpolation\n",
    "    \n",
    "    Memory impact: A % of activtion memory scales with input size reduction (12x theoretical)\n",
    "    FLOP improvements: Direct 12x reduction in convolution operations\n",
    "    Speedup: LIMITED by Amdahl's law - a % of the convolution_ops scales with input size, but there are also fixed overheads (FC layers, data loading) that don't scale with input size\n",
    "    Throughput: Better for batch processing where fixed costs are amortized (make sure to multiply estimate by at least the current batch processing)\n",
    "    \"\"\"\n",
    "    # Initialize analysis structure (feel free to skip any entries you don't want to programmatically calculate)\n",
    "    analysis = {\n",
    "        'technique': 'Interpolation Removal (Native Resolution)',\n",
    "        'interpolation_size': None,  # TODO: Find this integer value in the model's forward() method \n",
    "        'original_image_size': CONFIG[\"image_size\"],\n",
    "        'avg_flop_reduction_percent': 0,\n",
    "        'estimated_speedup': 1.0,\n",
    "        'estimated_memory_reduction_mb': 0,\n",
    "        'estimated_throughput_samples_sec': timing_results['throughput_samples_per_sec'],\n",
    "        'throughput_improvement_percent': 0,\n",
    "        'sensitivity_risk': ''\n",
    "    }\n",
    "\n",
    "    # TODO: Complete the analysis dictionary with relevant values\n",
    "    # HINT:\n",
    "    # 1. Find overhead: Look for interpolation_size = 224 in model.forward(), calculate interpolation_factor = (224 / 64)Â² = 12.25x\n",
    "    # 2. Calculate theoretical: theoretical_flop_reduction = 1.0 - (1.0 / interpolation_factor) = ~91.8%\n",
    "    # 3. Apply Amdahl's Law: Only scalable_portion = conv_coverage * 0.6 benefits, speedup = 1 / (fixed_portion + scalable_portion / interpolation_factor)\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete architecture optimization analysis with all 5 implemented analysis functions above\n",
    "def run_architecture_optimization_analysis(model):\n",
    "    \"\"\"\n",
    "    Main function to run all architecture optimization analyses.\n",
    "    \"\"\"\n",
    "    print(\"Running architecture optimization analysis...\")\n",
    "    \n",
    "    # Call each analysis function\n",
    "    depthwise_analysis = analyze_depthwise_separable_potential(model)\n",
    "    grouped_analysis = analyze_grouped_conv_potential(model)\n",
    "    inverted_analysis = analyze_inverted_residuals_potential(model)\n",
    "    lowrank_analysis = analyze_lowrank_factorization_potential(model)\n",
    "    channel_analysis = analyze_channel_organization_potential(model)\n",
    "    sharing_analysis = analyze_parameter_sharing_potential(model)\n",
    "    interpolation_analysis = analyze_interpolation_removal_potential(model)\n",
    "    \n",
    "    # Combine all analyses into comprehensive result\n",
    "    # TODO: Remove any techniques which haven't been analyzed\n",
    "    optimization_techniques = {\n",
    "        'grouped_convolutions': {\n",
    "            'candidate_layers': grouped_analysis['total_candidates'],\n",
    "            'groups': grouped_analysis['groups'],\n",
    "            'avg_param_reduction_percent': grouped_analysis['avg_param_reduction_percent'],\n",
    "            'estimated_speedup': grouped_analysis['estimated_speedup'],\n",
    "            'avg_flop_reduction_percent': grouped_analysis['avg_flop_reduction_percent'],\n",
    "            'estimated_memory_reduction_mb': grouped_analysis['estimated_memory_reduction_mb'],\n",
    "            'throughput_improvement_percent': grouped_analysis['throughput_improvement_percent'],\n",
    "            'sensitivity_risk': grouped_analysis['sensitivity_risk'],\n",
    "            'details': grouped_analysis['candidate_layers'][:3]  # Show top 3\n",
    "        },\n",
    "        'depthwise_separable': {\n",
    "            'candidate_layers': depthwise_analysis['total_candidates'],\n",
    "            'avg_param_reduction_percent': depthwise_analysis['avg_param_reduction_percent'],\n",
    "            'estimated_speedup': depthwise_analysis['estimated_speedup'],\n",
    "            'avg_flop_reduction_percent': depthwise_analysis['avg_flop_reduction_percent'],\n",
    "            'estimated_memory_reduction_mb': depthwise_analysis['estimated_memory_reduction_mb'],\n",
    "            'throughput_improvement_percent': depthwise_analysis['throughput_improvement_percent'],\n",
    "            'sensitivity_risk': depthwise_analysis['sensitivity_risk'],\n",
    "            'details': depthwise_analysis['candidate_layers'][:3]  # Show top 3\n",
    "        },\n",
    "        'inverted_residuals': {\n",
    "            'expansion_candidates': inverted_analysis['total_candidates'],\n",
    "            'estimated_speedup': inverted_analysis['estimated_speedup'],\n",
    "            'avg_flop_reduction_percent': inverted_analysis['avg_flop_reduction_percent'],\n",
    "            'estimated_memory_reduction_mb': inverted_analysis['estimated_memory_reduction_mb'],\n",
    "            'throughput_improvement_percent': inverted_analysis['throughput_improvement_percent'],\n",
    "            'sensitivity_risk': inverted_analysis['sensitivity_risk']\n",
    "        },\n",
    "        'low_rank_factorization': {\n",
    "            'candidate_layers': lowrank_analysis['total_candidates'],\n",
    "            'avg_param_reduction_percent': lowrank_analysis['avg_param_reduction_percent'],\n",
    "            'estimated_speedup': lowrank_analysis['estimated_speedup'],\n",
    "            'avg_flop_reduction_percent': lowrank_analysis['avg_flop_reduction_percent'],\n",
    "            'estimated_memory_reduction_mb': lowrank_analysis['estimated_memory_reduction_mb'],\n",
    "            'throughput_improvement_percent': lowrank_analysis['throughput_improvement_percent'],\n",
    "            'sensitivity_risk': lowrank_analysis['sensitivity_risk']\n",
    "        },\n",
    "        'channel_organization': {\n",
    "            'inplace_opportunities': channel_analysis['inplace_opportunities'],\n",
    "            'estimated_speedup': channel_analysis['estimated_speedup'],\n",
    "            'avg_flop_reduction_percent': channel_analysis['avg_flop_reduction_percent'],\n",
    "            'estimated_memory_reduction_mb': channel_analysis['estimated_memory_reduction_mb'],\n",
    "            'throughput_improvement_percent': channel_analysis['throughput_improvement_percent'],\n",
    "            'sensitivity_risk': channel_analysis['sensitivity_risk']\n",
    "        },\n",
    "        'parameter_sharing': {\n",
    "            'sharing_potential_percent': sharing_analysis['sharing_potential_percent'],\n",
    "            'estimated_speedup': sharing_analysis['estimated_speedup'],\n",
    "            'avg_flop_reduction_percent': sharing_analysis['avg_flop_reduction_percent'],\n",
    "            'estimated_memory_reduction_mb': sharing_analysis['estimated_memory_reduction_mb'],\n",
    "            'throughput_improvement_percent': sharing_analysis['throughput_improvement_percent'],\n",
    "            'sensitivity_risk': sharing_analysis['sensitivity_risk']\n",
    "        },\n",
    "        'interpolation_removal': {\n",
    "            'current_input_size': interpolation_analysis[\"interpolation_size\"],\n",
    "            'native_input_size': interpolation_analysis[\"original_image_size\"],\n",
    "            'estimated_speedup': interpolation_analysis['estimated_speedup'],\n",
    "            'avg_flop_reduction_percent': interpolation_analysis['avg_flop_reduction_percent'],\n",
    "            'estimated_memory_reduction_mb': interpolation_analysis['estimated_memory_reduction_mb'],\n",
    "            'throughput_improvement_percent': interpolation_analysis['throughput_improvement_percent'],\n",
    "            'sensitivity_risk': interpolation_analysis['sensitivity_risk']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return optimization_techniques\n",
    "\n",
    "# Execute architecture analysis\n",
    "arch_analysis = run_architecture_optimization_analysis(baseline_model)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nARCHITECTURE OPTIMIZATION RESULTS:\")\n",
    "for technique, details in arch_analysis.items():\n",
    "    print(f\"\\n   {technique.replace('_', ' ').title()}:\")\n",
    "    for key, value in details.items():\n",
    "        if key != 'details':\n",
    "            print(f\"     {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Before you move on...brainstorming time!_**\n",
    "> \n",
    "> Based on your architecture analysis results, collect yours insights on architectural opportunities\n",
    "> \n",
    "> 1. **Primary bottleneck**: What operation type consumes the most compute time in your model? Why does this pattern make sense for ResNet-18 architecture? How does this inform your optimization strategy?\n",
    "> \n",
    "> 2. **Resource trade-offs:**: Which optimizations reduce parameters vs which improve compute efficiency? How might accuracy be affected by each technique?\n",
    "> \n",
    "> 3. **Optimization priority**: In which order would you recommend implementing the 5 architectural techniques? Consider both impact and implementation difficulty.\n",
    "> \n",
    "> 4. **Feasibility assessment:**: Will the combined optimizations achieve the 3ms target? If not, what additional techniques might be needed?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis checkpoint 2: Deployment optimization opportunities\n",
    "\n",
    "**Task:** Analyze deployment characteristics for our reference hardware, with a focus on batch processing trade-offs and mixed precision acceleration.\n",
    "\n",
    "_Feel free to also add considerations from other hardware optimization techniques, such as leveraging specialized hardware acceleration units (e.g., Tensor Cores, NPUs) and other memory optimization patterns._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement logic for each single technique analysis \n",
    "\n",
    "def analyze_mixed_precision_potential(detailed_results):\n",
    "    \"\"\"    \n",
    "    TODO: Analyze mixed precision (FP16) acceleration potential.\n",
    "    HINTS: These are the key analysis points to focus on:\n",
    "    - Mixed precision suitability calculation based on operation types (hardware accelerators for FP16 typically target matrix multiplies and convolutions)\n",
    "    - Research-backed speedup estimates (1.8-2.5x for high coverage)\n",
    "    - Memory reduction modeling (FP32->FP16 = ~50% reduction)\n",
    "    - Throughput impact calculation (scales with speedup for batch processing scenarios)\n",
    "    \"\"\"\n",
    "    # Initialize analysis structure (feel free to skip any entries you don't want to programmatically calculate)\n",
    "    analysis = {\n",
    "        'technique': 'Mixed Precision (FP16)',\n",
    "        'mixed_precision_eligible_ops': 0,\n",
    "        'mixed_precision_coverage_percent': 0,\n",
    "        'estimated_speedup': 1.0,\n",
    "        'estimated_memory_reduction_mb': 0,\n",
    "        'avg_flop_reduction_percent': 0,\n",
    "        'estimated_throughput_samples_sec': timing_results['batch_throughput_samples_per_sec'],\n",
    "        'throughput_improvement_percent': 0,\n",
    "        'sensitivity_risk': ''\n",
    "    }\n",
    "\n",
    "    # TODO: Complete the analysis dictionary with relevant values\n",
    "    # HINT: 3-step process\n",
    "    # 1. Find mixed precision eligible ops: eligible_ops = matrix_ops + conv_ops from detailed_results['operation_breakdown']\n",
    "    # 2. Calculate coverage: coverage_percent = (eligible_ops / total_ops) * 100, then research-backed speedup = 1.8 + (coverage - 50) * 0.014 if coverage > 50%\n",
    "    # 3. Aggregate impact: memory_reduction = peak_memory_mb * 0.5 (FP32->FP16), throughput = baseline_throughput * estimated_speedup\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def analyze_batch_processing_scenarios(model, mixed_precision_speedup, sample_input_shape=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"])):\n",
    "    \"\"\"\n",
    "    TODO: Analyze different deployment scenarios with realistic batch processing logic.\n",
    "    \n",
    "    HINT: Focus on finding optimal batch sizes for different use cases (real-time vs throughput) using the profiler.profile_multiple_batch_sizes() function.\n",
    "    Real-time optimizes for lowest latency per sample, throughput optimizes for maximum samples/sec\n",
    "    \"\"\"\n",
    "    # Initialize analysis structure (feel free to skip any entries you don't want to programmatically calculate)\n",
    "    scenarios = {\n",
    "        'real_time_diagnosis': {\n",
    "            'optimal_batch_size': None,\n",
    "            'current_latency_ms': None,\n",
    "            'mixed_precision_latency_ms': None,\n",
    "            'use_case': 'Emergency diagnosis, single patient processing'\n",
    "        },\n",
    "        'batch_processing': {\n",
    "            'optimal_batch_size': None,\n",
    "            'current_throughput_samples_sec': None,\n",
    "            'mixed_precision_throughput_samples_sec': None,\n",
    "            'use_case': 'Screening workflows, research processing'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Test batch sizes appropriate for T4 memory constraints\n",
    "    batch_sizes = []  # TODO: Add your values here\n",
    "\n",
    "    print(\"   Profiling multiple batch sizes...\")\n",
    "    batch_results = profiler.profile_multiple_batch_sizes(\n",
    "        model, sample_images.shape, batch_sizes\n",
    "    )\n",
    "    \n",
    "    # TODO: Complete the analysis dictionary with relevant values\n",
    "    # HINT: You can extract all information from batch_results \n",
    "    # 1. Find optimal configs: real_time_optimal = min(batches, key=lambda k: latency_per_sample), throughput_optimal = max(batches, key=lambda k: samples_per_sec)\n",
    "    # 2. Create scenarios: Remember to apply mixed_precision_speedup to both latency (divide) and throughput (multiply) for deployment projections\n",
    "\n",
    "    return batch_results, scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete deployment optimization analysis with all 2 implemented analysis functions above\n",
    "def run_deployment_optimization_analysis(baseline_model, timing_results, memory_results, detailed_results):\n",
    "    \"\"\"\n",
    "    Main function to run all deployment optimization analyses.\n",
    "    \"\"\"\n",
    "    print(\"Running Deployment Optimization Analysis...\")\n",
    "\n",
    "    # Analyze mixed precision potential\n",
    "    mixed_precision = analyze_mixed_precision_potential(detailed_results)\n",
    "    mixed_precision_speedup = mixed_precision['estimated_speedup']\n",
    "    \n",
    "    # Analyze batch processing scenarios\n",
    "    batch_results, batch_scenarios = analyze_batch_processing_scenarios(baseline_model, mixed_precision_speedup)\n",
    "\n",
    "    # Visualize batch size analysis for deployment understanding\n",
    "    plot_batch_size_comparison(batch_results)\n",
    "\n",
    "    # Calculate deployment readiness\n",
    "    current_latency = timing_results['single_sample_ms']\n",
    "    current_throughput = timing_results['throughput_samples_per_sec']\n",
    "    \n",
    "    performance_metrics = {\n",
    "        'latency_ms': current_latency,\n",
    "        'throughput_samples_sec': current_throughput\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'mixed_precision': mixed_precision,\n",
    "        'batch_scenarios': batch_scenarios\n",
    "    }\n",
    "\n",
    "# Execute deployment analysis\n",
    "deployment_analysis = run_deployment_optimization_analysis(baseline_model, timing_results, memory_results, detailed_results)\n",
    "\n",
    "print(f\"\\nDEPLOYMENT OPTIMIZATION RESULTS:\")\n",
    "mp_details = deployment_analysis['mixed_precision']\n",
    "print(f\"\\n   Mixed Precision (FP16):\")\n",
    "print(f\"     Tensor Core Eligible: {mp_details['mixed_precision_coverage_percent']:.1f}%\")\n",
    "print(f\"     Estimated FLOP improvements: {mp_details['avg_flop_reduction_percent']:.1f}MB\")\n",
    "print(f\"     Estimated Speedup: {mp_details['estimated_speedup']:.1f}x\")\n",
    "print(f\"     Estimated Throughput improvements %: {mp_details['throughput_improvement_percent']:.1f}%\")\n",
    "print(f\"     Estimated Memory Savings: {mp_details['estimated_memory_reduction_mb']:.1f}MB\")\n",
    "print(f\"     Estimated Sensitivity Risk: {mp_details['sensitivity_risk']}\")\n",
    "\n",
    "if 'error' not in deployment_analysis['batch_scenarios']:\n",
    "    scenarios = deployment_analysis['batch_scenarios']\n",
    "    print(f\"\\n   Deployment Scenarios:\")\n",
    "    for scenario, details in scenarios.items():\n",
    "        print(f\"     {scenario.replace('_', ' ').title()}:\")\n",
    "        print(f\"       Optimal Batch Size: {details['optimal_batch_size']}\")\n",
    "        print(f\"       Use Case: {details['use_case']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Before you move on...brainstorming time!_**\n",
    "> \n",
    "> Based on your deployment analysis results above, collect your thoughts about hardware deployment opportunities - this will help you in completing your optimization plan at the end of the notebook:\n",
    ">  \n",
    "> 1. **Shared-Resource Constraints**: What is the main limiting factor when deploying the model alongside other applicationsâ€”memory or compute?\n",
    "> \n",
    "> 2. **Batch processing trade-offs**: How does performance change with batch size? What's the optimal configuration for different deployment scenarios?\n",
    "> \n",
    "> 3. **Mixed precision impact**: What percentage of operations can benefit from FP16? How much speedup can you realistically expect? What are the implementation risks?\n",
    "> \n",
    "> 4. **Production Readiness Assessment**: Which KPI targets can be met with hardware acceleration alone?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Congratulations!** \n",
    "\n",
    "You have completed the model baseline analysis! This foundational work will guide all subsequent optimization efforts.\n",
    "\n",
    "### **Summary: Key findings**\n",
    "Document your analysis results using this framework:\n",
    "\n",
    "1.  **Overall performance profile**: \n",
    "\n",
    "_<<TODO: Collect key points about the performance baseline>>_\n",
    "\n",
    "_<<TODO: Summarize whether the optimization targets can be met>>_\n",
    "\n",
    "2. **Bottlenecks**: \n",
    "\n",
    "_<<TODO: Identify the main performance limitations by memory/compute/latency - and, don't forget the \"Did you notice?\" point on input interpolation!>>_\n",
    "\n",
    "3. **Architecture optimization**:\n",
    "\n",
    "_<<TODO: Fill in your findings from Analysis Checkpoint 1 including:_<br>\n",
    "_- Top 2 architectural techniques with highest impact potential_<br>\n",
    "_- Implementation difficulty vs expected benefit analysis_<br>\n",
    "_- Estimated parameter reduction and optimization goals projections_<br>\n",
    "_- Other techniques you may consider beyond those listed>>_\n",
    "\n",
    "4. **Hardware deployment optimization**: \n",
    "\n",
    "_<<TODO: Fill in your findings from Analysis Checkpoint 2 including:_<br>\n",
    "_- Mixed precision acceleration potential and implementation plan_<br>\n",
    "_- Optimal batch configurations for different use cases_>>\n",
    "\n",
    "### **Recommended optimization roadmap**\n",
    "\n",
    "Based on the analysis, prioritize the optimization techniques and highlight the estimated combined impact on optimization goals for each phase:\n",
    "\n",
    "**Phase 1 (Quick Wins):**\n",
    "\n",
    "_<<TODO: List >=1 highest priority architectural recommendations>>_\n",
    "\n",
    "_<<TODO: List >=1 highest priority hardware deployment recommendations>>_\n",
    "\n",
    "_<<TODO: Summarize estimated impact of this phase>>_\n",
    "\n",
    "**Phase 2 (Extra Impact):**\n",
    "\n",
    "_<<TODO: List >=1 Additional architectural and hardware deployment recommendations>>_\n",
    "\n",
    "_<<TODO: Summarize estimated impact of this phase>>_\n",
    "\n",
    "---\n",
    "\n",
    "**You are now ready to move to Notebook 2: Architecture Optimization!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
