{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Investigate model performance bottlenecks with PyTorch Profiler\n",
    "\n",
    "You have explored hardware platforms, performance metrics, and hardware-first thinking in theory. It is now time to move from understanding concepts to hands-on diagnosis, and systematically investigate real model performance bottlenecks using professional profiling tools.\n",
    "\n",
    "> **Overview:** A fintech company's customer churn prediction model (a multi-layer perceptron processing transaction features) is causing API timeouts in production. Use PyTorch Profiler's multiple analysis modes to diagnose performance issues and identify optimization opportunities on your GPU.\n",
    "> \n",
    "> **Scenario:** You work for a fintech startup that predicts customer churn to trigger retention campaigns. Your ML team deployed a neural network that processes customer transaction features to predict churn probability. In development on your local machine, individual predictions ran smoothly, but when deployed to production with realistic API loads and batch processing, your DevOps team reports inconsistent performance that's causing service level agreement violations.\n",
    "> \n",
    "> Your DevOps team reports the production servers use NVIDIA T4 GPUs (same as your development environment), but something about the production workload is causing severe performance degradation. You need to systematically profile the model to identify what's limiting throughput and causing the performance variability.\n",
    "> \n",
    "> **Goal**: Learn to use PyTorch Profiler's timeline analysis, memory profiling, and GPU utilization monitoring to systematically diagnose performance bottlenecks and identify hardware-specific optimization opportunities in production AI deployments.\n",
    "> \n",
    "> **Tools**: PyTorch, PyTorch Profiler, pynvml, pandas\n",
    "> \n",
    "> **Estimated Time**: 15 minutes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Let's start by setting up our profiling environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out the cell block again and restart the notebook\n",
    "# # ! pip install pynvml kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.profiler\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pynvml\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"assets/exercise2\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NVIDIA management library\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "# Check GPU availability and get T4 info\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Using GPU: {gpu_name}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Load the dataset\n",
    "\n",
    "For this exercise, we use the [Telco Customer Churn dataset](https://www.kaggle.com/datasets/blastchar/telco-customer-churn), hosted in Kaggle.\n",
    "The raw data contains 7043 rows (customers) and 21 columns (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "df_name = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "df_dirpath = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "df_path = os.path.join(df_dirpath, df_name)\n",
    "print(\"Path to dataset files:\", df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "# Get dataset info\n",
    "print(f\"Dataset colums: {list(df.columns)}\")\n",
    "print(df.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "# Data preprocessing for the real dataset\n",
    "print(\"\\nPreprocessing the dataset...\")\n",
    "\n",
    "# Handle TotalCharges column (contains some string values that need conversion)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', \n",
    "                      'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                      'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', \n",
    "                      'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "# Use LabelEncoder for binary categorical variables and get_dummies for multi-class\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    if df[col].nunique() == 2 or (df[col].nunique() == 3 and 'No phone service' in df[col].values):\n",
    "        # Binary encoding for simple categorical variables\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "    else:\n",
    "        # One-hot encoding for multi-class variables\n",
    "        dummies = pd.get_dummies(df[col], prefix=col)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Encode target variable\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Drop customerID as it's just an identifier\n",
    "df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = [col for col in df.columns if col != 'Churn']\n",
    "X = df[feature_columns].values\n",
    "y = df['Churn'].values\n",
    "\n",
    "print(f\"\\nFinal dataset info:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Churn rate: {y.mean():.1%}\")\n",
    "print(f\"Number of features after encoding: {len(feature_columns)}\")\n",
    "\n",
    "# Display first few feature names for reference\n",
    "print(f\"\\nFeature Columns: {feature_columns}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dataset insights**: The Telco dataset contains 7,043 customers with 21 features covering demographics, services, and billing information. \n",
    "> \n",
    "> After preprocessing categorical variables (gender, contract types, internet services, etc.), we end up with 38 features given one-hot encoding. \n",
    "> \n",
    "> The 26.5% churn rate provides a realistic class imbalance that mirrors production scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "label_col = 'Churn'\n",
    "feature_columns = [col for col in df.columns if col != label_col]\n",
    "X = df[feature_columns].values\n",
    "y = df[label_col].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "print(f\"Training data shape: {X_train_tensor.shape}\")\n",
    "print(f\"Test data shape: {X_test_tensor.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the \"problematic\" model architecture\n",
    "Let's create a neural network with intentional inefficiencies that mirror common real-world performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProblematicChurnModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A deliberately inefficient model architecture that creates realistic bottlenecks\n",
    "    on T4 GPUs. This simulates common mistakes in production deployments.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes=[512, 1024, 512, 256, 128]):\n",
    "        super(ProblematicChurnModel, self).__init__()\n",
    "        \n",
    "        # Inefficient architecture choices that will show up in profiling:\n",
    "        \n",
    "        # 1. Oversized hidden layers for the problem complexity\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            # 2. Using individual Linear layers instead of Sequential blocks\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            # 3. Inefficient dropout placement (will cause extra memory operations)\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # 4. Output layer\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # 5. Inefficient custom forward pass that will show up in profiling\n",
    "        self.custom_activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 6. Inefficient forward pass with unnecessary operations\n",
    "        # This creates CPU-GPU synchronization points\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Extra tensor operations that fragment GPU utilization\n",
    "        x = x + 0.0001  # Unnecessary operation that creates new tensor\n",
    "        x = x * 1.0001  # Another unnecessary operation\n",
    "        \n",
    "        # Pass through main network\n",
    "        x = self.network(x)\n",
    "        \n",
    "        # 7. Inefficient final processing\n",
    "        # Force CPU-GPU sync (performance killer!)\n",
    "        if batch_size > 1:\n",
    "            # This creates a synchronization point\n",
    "            mean_activation = torch.mean(x).item()  # .item() forces sync!\n",
    "            \n",
    "        return x\n",
    "\n",
    "# TODO: Create the model on GPU\n",
    "# Hint: Review https://discuss.pytorch.org/t/is-there-any-difference-between-x-to-cuda-vs-x-cuda-which-one-should-i-use/20137\n",
    "# for information on how to move a model to cuda\n",
    "model =  # Add your code here\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Architecture analysis**: Notice the deliberately problematic choices:\n",
    "> \n",
    "> - Oversized layers (1024 units for simple tabular data)\n",
    "> - Inefficient forward pass with unnecessary tensor operations\n",
    "> - CPU-GPU synchronization points that will show up clearly in profiling\n",
    "> - Dropout in inference mode (though PyTorch optimizes this out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Establish baseline model performance\n",
    "\n",
    "Let's train now our baseline model to measure baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the problematic model\n",
    "print(\"Training the problematic model...\")\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training loop (keep it short for the exercise)\n",
    "model.train()\n",
    "for epoch in range(5):  # Quick training, just enough to get reasonable weights\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/5, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note on model accuracy**: Since this exercise focuses on performance profiling, we skip evaluating model accuracy. Our goal is to diagnose why that model runs slowly during inference, regardless of how well it predicts churn.\n",
    "> \n",
    "> In production, you'd have a well-trained model that meets your accuracy requirements. Feel free to extend this exercise by adding validation accuracy tracking, confusion matrix analysis, or ROC curve evaluation if you want to explore the trade-offs between model performance optimization and predictive accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create DataLoaders with different batch sizes to simulate production scenarios\n",
    "batch_sizes = [32, 64, 128, 512, 1024]  # We'll test different batch sizes\n",
    "dataloaders = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    dataloaders[batch_size] = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "print(f\"Created DataLoaders for batch sizes: {batch_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure baseline performance without profiling\n",
    "def measure_baseline_performance(model, dataloader, num_batches=10):\n",
    "    \"\"\"Measure baseline inference performance\"\"\"\n",
    "    \n",
    "        \n",
    "    model.eval()\n",
    "    times = []\n",
    "\n",
    "    # TODO: Measure inference times on first few batches\n",
    "    # Hint: Use torch.cuda.synchronize() before and after inference, then measure time with time.perf_counter()\n",
    "    # Don't forget to break the loop after num_batches has been achieved\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    return times\n",
    "\n",
    "## Start measurement wifferent batch sizes\n",
    "baseline_results = {}\n",
    "print(\"Measuring baseline performance...\")\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    times = measure_baseline_performance(model, dataloaders[batch_size])\n",
    "    avg_time = np.mean(times) * 1000  # Convert to milliseconds\n",
    "    baseline_results[batch_size] = {\n",
    "        'avg_time_ms': avg_time,\n",
    "        'std_time_ms': np.std(times) * 1000,\n",
    "        'times': times\n",
    "    }\n",
    "    print(f\"Batch size {batch_size}: {avg_time:.2f} ± {baseline_results[batch_size]['std_time_ms']:.2f} ms\")\n",
    "\n",
    "baseline_results = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Batch size optimization insight**: Notice that it is not the larger batch size to achieve better performance This is because:\n",
    "> \n",
    "> - **With too small batches**: GPU cores underutilized, overhead from frequent kernel launches\n",
    "> - **With optimal batch**: Good GPU utilization without memory pressure \n",
    "> - **With too large batches**: Memory bandwidth saturation and increased variance\n",
    "> \n",
    "> This \"sweet spot\" pattern is common in production - optimal batch size balances GPU utilization with memory efficiency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: GPU system monitoring with pynvml\n",
    "Before diving into detailed profiling, let's establish our hardware baseline using NVIDIA's system monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to monitor GPU utilization and memory usage\n",
    "\n",
    "def get_gpu_stats():\n",
    "    \"\"\"Get current GPU utilization and memory usage\"\"\"\n",
    "    # TODO: Get utilization rates (GPU and memory) using pynvml\n",
    "    # Hint: Find a complete list of methods at https://docs.nvidia.com/deploy/nvml-api/nvml-api-reference.html\n",
    "    # You can use the `handle` previously created\n",
    "    util =   # Add your code here\n",
    "    memory =   # Add your code here\n",
    "    temp =   # Add your code here\n",
    "    power_usage =   # Add your code here\n",
    "    power_limit =   # Add your code here\n",
    "    graphics_clock =   # Add your code here\n",
    "    memory_clock =   # Add your code here\n",
    "    perf_state =   # Add your code here\n",
    "    throttle_reasons =   # Add your code here\n",
    "    \n",
    "    return {\n",
    "        # Basic utilization\n",
    "        'gpu_util': util.gpu,\n",
    "        'memory_util': util.memory,\n",
    "        'memory_used_mb': memory.used / 1024**2,\n",
    "        'memory_total_mb': memory.total / 1024**2,\n",
    "        'memory_free_mb': memory.free / 1024**2,\n",
    "        \n",
    "        # Thermal and power\n",
    "        'temperature_c': temp,\n",
    "        'power_usage_w': power_usage,\n",
    "        'power_limit_w': power_limit,\n",
    "        'power_efficiency': util.gpu / max(power_usage, 1),  # GPU util per watt\n",
    "        \n",
    "        # Performance monitoring\n",
    "        'graphics_clock_mhz': graphics_clock,\n",
    "        'memory_clock_mhz': memory_clock,\n",
    "        'performance_state': perf_state,\n",
    "        'is_throttling': throttle_reasons != 0,\n",
    "        'throttle_reasons': throttle_reasons\n",
    "    }\n",
    "\n",
    "\n",
    "def monitor_inference_workload(model, dataloader, duration_seconds=10):\n",
    "    \"\"\"Monitor GPU stats during inference workload\"\"\"\n",
    "    model.eval()\n",
    "    stats_history = []\n",
    "\n",
    "    # Add debugging to ensure we're actually using GPU\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Input data will be on: {device}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    batch_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while time.time() - start_time < duration_seconds:\n",
    "            for inputs, _ in dataloader:\n",
    "                # Record stats before inference\n",
    "                stats_before = get_gpu_stats()\n",
    "                \n",
    "                # TODO: Run inference\n",
    "                # Hint: Remember to synchronize, and add a small delay to ensure GPU work is captured\n",
    "\n",
    "                # Add your code here\n",
    "                \n",
    "                # Record stats after inference\n",
    "                stats_after = get_gpu_stats()\n",
    "                \n",
    "                stats_history.append({\n",
    "                    'batch': batch_count,\n",
    "                    'timestamp': time.time() - start_time,     \n",
    "                    'batch_size': inputs.shape[0],\n",
    "\n",
    "                    # Before inference\n",
    "                    'gpu_util_before': stats_before['gpu_util'],\n",
    "                    'memory_used_before_mb': stats_before['memory_used_mb'],\n",
    "                    'temp_before_c': stats_before['temperature_c'],\n",
    "                    'power_before_w': stats_before['power_usage_w'],\n",
    "                    \n",
    "                    # After inference  \n",
    "                    'gpu_util_after': stats_after['gpu_util'],\n",
    "                    'memory_used_after_mb': stats_after['memory_used_mb'],\n",
    "                    'temp_after_c': stats_after['temperature_c'],\n",
    "                    'power_after_w': stats_after['power_usage_w'],\n",
    "                    \n",
    "                    # Derived metrics\n",
    "                    'gpu_util_delta': stats_after['gpu_util'] - stats_before['gpu_util'],\n",
    "                    'memory_delta_mb': stats_after['memory_used_mb'] - stats_before['memory_used_mb'],\n",
    "                    'temp_delta_c': stats_after['temperature_c'] - stats_before['temperature_c'],\n",
    "                    'power_delta_w': stats_after['power_usage_w'] - stats_before['power_usage_w'],\n",
    "                    \n",
    "                    # Hardware state\n",
    "                    'is_throttling': stats_after['is_throttling'],\n",
    "                    'performance_state': stats_after['performance_state']\n",
    "                })\n",
    "                \n",
    "                batch_count += 1\n",
    "                \n",
    "                if time.time() - start_time >= duration_seconds:\n",
    "                    break\n",
    "    \n",
    "    return stats_history\n",
    "\n",
    "# Run monitoring for batch size 64 and analyze the results\n",
    "print(\"Monitoring GPU utilization during inference...\")\n",
    "stats = monitor_inference_workload(model, dataloaders[512], duration_seconds=8)\n",
    "\n",
    "# Analyze the monitoring results\n",
    "gpu_utils_after = [s['gpu_util_after'] for s in stats]  \n",
    "temps_after = [s['temp_after_c'] for s in stats]\n",
    "power_usage_after = [s['power_after_w'] for s in stats]\n",
    "memory_usage_after = [s['memory_used_after_mb'] for s in stats]\n",
    "throttling_events = [s for s in stats if s['is_throttling']]\n",
    "\n",
    "# Calculate power efficiency\n",
    "power_efficiency = [gpu_util / max(power, 1) for gpu_util, power in zip(gpu_utils_after, power_usage_after)]\n",
    "\n",
    "print(f\"\\n=== COMPREHENSIVE GPU ANALYSIS ===\")\n",
    "print(f\"Average GPU utilization: {np.mean(gpu_utils_after):.1f}%\")\n",
    "print(f\"Peak GPU utilization: {np.max(gpu_utils_after):.1f}%\")\n",
    "print(f\"GPU utilization std dev: {np.std(gpu_utils_after):.1f}%\")\n",
    "print(f\"Temperature range: {np.min(temps_after):.1f}°C - {np.max(temps_after):.1f}°C\")\n",
    "print(f\"Power usage range: {np.min(power_usage_after):.1f}W - {np.max(power_usage_after):.1f}W\")\n",
    "print(f\"Average power efficiency: {np.mean(power_efficiency):.2f} util%/W\")\n",
    "print(f\"Average memory usage: {np.mean(memory_usage_after):.1f} MB\")\n",
    "print(f\"Performance state (P0=max): P{stats[0]['performance_state']}\")\n",
    "\n",
    "if throttling_events:\n",
    "    print(f\"WARNING:  THROTTLING DETECTED: {len(throttling_events)} events out of {len(stats)} batches\")\n",
    "    print(\"   This could explain performance degradation!\")\n",
    "else:\n",
    "    print(\"SUCCESS: No throttling detected during monitoring period\")\n",
    "\n",
    "# Analyze the before/after patterns  \n",
    "gpu_util_deltas = [s['gpu_util_delta'] for s in stats]\n",
    "memory_deltas = [s['memory_delta_mb'] for s in stats]\n",
    "temp_deltas = [s['temp_delta_c'] for s in stats]\n",
    "power_deltas = [s['power_delta_w'] for s in stats]\n",
    "\n",
    "print(f\"\\n=== INFERENCE IMPACT ANALYSIS ===\")\n",
    "print(f\"GPU utilization change per batch: {np.mean(gpu_util_deltas):.1f}% ± {np.std(gpu_util_deltas):.1f}%\")\n",
    "print(f\"Memory allocation per batch: {np.mean(memory_deltas):.2f} MB ± {np.std(memory_deltas):.2f} MB\")\n",
    "print(f\"Temperature increase per batch: {np.mean(temp_deltas):.2f}°C ± {np.std(temp_deltas):.2f}°C\") \n",
    "print(f\"Power consumption change: {np.mean(power_deltas):.1f}W ± {np.std(power_deltas):.1f}W\")\n",
    "\n",
    "print(f\"\\nProcessed {len(stats)} batches with comprehensive hardware monitoring\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO: Analyze GPU monitoring insights and identify potential issues\n",
    "> \n",
    "> Hint: Focus on GPU utilization patterns and power efficiency metrics. What does extremely low GPU utilization tell you about how the model is using the T4's parallel processing capabilities?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deep profiling with PyTorch Profiler\n",
    "Now let's use PyTorch Profiler to understand what's happening at the kernel level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_model_performance(model, dataloader, batch_size, num_batches=5):\n",
    "    \"\"\"Profile model with PyTorch Profiler\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # TODO: Configure profiler settings as a reusable configuration dictionary\n",
    "    # Hint: Create profiler_config dict with activities, record_shapes, profile_memory, etc.\n",
    "    # Documentation: https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile\n",
    "    profiler_config = {}  # Add your code here\n",
    "    \n",
    "    with torch.profiler.profile(**profiler_config) as prof:\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, _) in enumerate(dataloader):\n",
    "                if i >= num_batches:\n",
    "                    break\n",
    "                \n",
    "                # TODO: Run model inference with profiling\n",
    "                # Hint: How does the profiler know that the next iteration has started?\n",
    "                # Documentation: https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile\n",
    "\n",
    "                # Add your code here\n",
    "    \n",
    "    return prof\n",
    "\n",
    "# Profile the model with batch size 64\n",
    "print(\"Starting detailed profiling...\")\n",
    "profiler_result = profile_model_performance(model, dataloaders[512], batch_size=64)\n",
    "\n",
    "print(\"Profiling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze profiling results\n",
    "\n",
    "print(\"=== TOP 10 MOST TIME-CONSUMING OPERATIONS ===\")\n",
    "# TODO: Display operations that take the most total time on the GPU\n",
    "# Hint: Consider grouping operations based on how they're called, and look for a method to summarize and rank them by total GPU time.\n",
    "# The profiler has an API for generating a table of summarized events: https://pytorch.org/docs/stable/profiler.html\n",
    "top_ops = # Add your code here\n",
    "print(top_ops)\n",
    "\n",
    "print(\"\\n=== MEMORY-INTENSIVE OPERATIONS ===\")\n",
    "# TODO: Highlight operations responsible for the most GPU memory usage\n",
    "# Hint: Focus on identifying which operations allocate the most memory directly (not cumulative usage).\n",
    "# There's a metric related to \"self\" memory usage that can help isolate expensive memory allocators: https://pytorch.org/docs/stable/profiler.html\n",
    "memory_ops =  # Add your code here\n",
    "print(memory_ops)\n",
    "\n",
    "print(\"\\n=== CPU vs CUDA TIME ANALYSIS ===\")\n",
    "# TODO: Identify operations with large discrepancies between CPU and GPU execution time\n",
    "# Hint: Look for signs of CPU-GPU synchronization delays; these usually appear when CPU time dominates.\n",
    "# Try sorting by total CPU time and observe the ratio of CPU to CUDA time for each operation: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "cpu_analysis =  # Add your code here\n",
    "print(cpu_analysis)\n",
    "\n",
    "print(\"\\n=== OPERATION CALL FREQUENCY ANALYSIS ===\")\n",
    "# TODO: Detect frequently called operations with minimal per-call overhead\n",
    "# Hint: Sorting by how often operations are called can uncover inefficiencies in batching or kernel launches.\n",
    "# High frequency + low time per call = potential optimization opportunity: https://pytorch.org/docs/stable/profiler.html\n",
    "frequency_analysis =  # Add your code here\n",
    "print(frequency_analysis)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO: Extract key performance insights from PyTorch profiler output\n",
    "> \n",
    "> Hint: Focus on identifying CPU-GPU synchronization issues (high CPU time + low CUDA time) and kernel launch overhead (many small operations).\n",
    "> \n",
    "> Find inspiration at https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#analyzing-performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.5: Profile layer-by-layer model performance\n",
    "\n",
    "While Step 6 revealed data loading as our primary bottleneck, we should also understand how our individual model layers perform on GPU. Even though the model represents only ~3% of execution time, layer-level analysis helps identify which architectural components are GPU-efficient versus CPU-bound, providing our optimization priorities once we fix the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by model layer\n",
    "def analyze_model_layers(profiler_result):\n",
    "    \"\"\"Break down performance by individual model layers\"\"\"\n",
    "    \n",
    "    # TODO: Extract only layer-specific operations from profiler results\n",
    "    # Hint: Filter profiler_result's events 'for events containing layer names like 'linear', 'relu', ...\n",
    "    layer_events =  # Add your code here\n",
    "    \n",
    "    print(\"=== LAYER-BY-LAYER PERFORMANCE BREAKDOWN ===\")\n",
    "    layer_analysis = profiler_result.key_averages().table(\n",
    "        sort_by=\"cuda_time_total\", row_limit=15,\n",
    "        header=\"Layer Performance Analysis\"\n",
    "    )\n",
    "    print(layer_analysis)\n",
    "    \n",
    "    layer_groups = {}\n",
    "    # TODO: Group layer events by operation type and calculate aggregate statistics\n",
    "    # Hint: Create a dictionary where keys are layer types ('Linear Layers', 'ReLU Activations', etc.) \n",
    "    # and values contain summed `cuda_time``, `cpu_time``, `memory`` usage, and call `counts``\n",
    "\n",
    "    # Add your code here\n",
    "\n",
    "    return layer_groups\n",
    "\n",
    "layer_breakdown = analyze_model_layers(profiler_result)\n",
    "\n",
    "# Visualize layer contributions\n",
    "print(\"\\n=== MODEL LAYER EFFICIENCY ANALYSIS ===\")\n",
    "for layer_type, stats in layer_breakdown.items():\n",
    "    cuda_ms = stats['cuda_time'] / 1000\n",
    "    cpu_ms = stats['cpu_time'] / 1000\n",
    "    memory_mb = stats['memory'] / (1024**2)\n",
    "    efficiency_ratio = cuda_ms / max(cpu_ms, 0.001)\n",
    "    \n",
    "    print(f\"{layer_type}:\")\n",
    "    print(f\"  CUDA time: {cuda_ms:.2f}ms | CPU time: {cpu_ms:.2f}ms\")\n",
    "    print(f\"  Memory usage: {memory_mb:.1f}MB | Calls: {stats['count']}\")\n",
    "    print(f\"  Efficiency ratio (CUDA/CPU): {efficiency_ratio:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO: Analyze layer-level performance insights and their impact on optimization strategy\n",
    "> \n",
    "> Hint: Focus on GPU efficiency ratios for each layer type and the relationship between number of operations and kernel launch overhead. How does this change your understanding of the primary performance bottleneck identified in Step 5?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Step: Visualize timeline analysis\n",
    "Let's create visualizations to understand the temporal behavior of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate timeline trace for detailed temporal analysis\n",
    "def generate_timeline_analysis(profiler_result, filename=\"model_trace\"):\n",
    "    \"\"\"Generate timeline trace and basic analysis\"\"\"\n",
    "    \n",
    "    trace_file = os.path.join(output_dir, f\"{filename}.json\")\n",
    "    # TODO: Export trace (can be viewed in chrome://tracing)\n",
    "    # Hint: Look for exporting functions at https://docs.pytorch.org/docs/stable/profiler.html\n",
    "    # And note that the export can be called only once for session\n",
    "    \n",
    "    # Add your code here\n",
    "    \n",
    "    # TODO: Get detailed event information\n",
    "    # Hint: # Tip: Start exploring with .key_averages(), then add grouping arguments.\n",
    "    events =   # Add your code here\n",
    "    \n",
    "    # TODO: Analyze GPU vs CPU time distribution\n",
    "    # Hint: Each profiler event object contains both CPU and CUDA timing attributes.\n",
    "    # Explore an event to find out all attributes\n",
    "    total_cuda_time =  # Add your code here\n",
    "    total_cpu_time =  # Add your code here\n",
    "    \n",
    "    timeline_stats = {\n",
    "        'trace_file': trace_file,\n",
    "        'total_cuda_time_ms': total_cuda_time / 1000,\n",
    "        'total_cpu_time_ms': total_cpu_time / 1000,\n",
    "        'cuda_cpu_ratio': total_cuda_time / max(total_cpu_time, 1),\n",
    "        'num_events': len(events)\n",
    "    }\n",
    "    \n",
    "    return timeline_stats, events\n",
    "\n",
    "timeline_stats, detailed_events = generate_timeline_analysis(profiler_result)\n",
    "\n",
    "print(\"=== TIMELINE ANALYSIS RESULTS ===\")\n",
    "print(f\"Chrome trace saved to: {timeline_stats['trace_file']}\")\n",
    "print(f\"Total CUDA time: {timeline_stats['total_cuda_time_ms']:.2f} ms\")\n",
    "print(f\"Total CPU time: {timeline_stats['total_cpu_time_ms']:.2f} ms\")\n",
    "print(f\"CUDA/CPU ratio: {timeline_stats['cuda_cpu_ratio']:.2f}\")\n",
    "print(f\"Number of profiled events: {timeline_stats['num_events']}\")\n",
    "\n",
    "# Create visualization of the performance bottlenecks\n",
    "def visualize_bottlenecks(events, top_n=8):\n",
    "    \"\"\"Create visualization of performance bottlenecks\"\"\"\n",
    "    \n",
    "    # TODO: Extract top_n operations by CUDA time, sort them, and extract relevant keys (names, cuda times in ms, and cpu times in ms)\n",
    "    # Hint: The .cuda_time attribute (in microseconds) can help rank performance bottlenecks.\n",
    "    # For the `names``, you may want to trim long operation names for better visualization.\n",
    "    sorted_events =  # Add your code here    \n",
    "    names =  # Add your code here\n",
    "    cuda_times =  # Add your code here\n",
    "    cpu_times =  # Add your code here\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # CUDA time breakdown\n",
    "    bars1 = ax1.barh(names, cuda_times, color='#2ecc71', alpha=0.7)\n",
    "    ax1.set_xlabel('CUDA Time (ms)')\n",
    "    ax1.set_title('Top Operations by CUDA Time')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, time_val in zip(bars1, cuda_times):\n",
    "        ax1.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{time_val:.2f}ms', va='center', fontsize=9)\n",
    "    \n",
    "    # CPU vs CUDA comparison\n",
    "    x_pos = np.arange(len(names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars2 = ax2.bar(x_pos - width/2, cpu_times, width, label='CPU Time', \n",
    "                    color='#3498db', alpha=0.7)\n",
    "    bars3 = ax2.bar(x_pos + width/2, cuda_times, width, label='CUDA Time', \n",
    "                    color='#e74c3c', alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Operations')\n",
    "    ax2.set_ylabel('Time (ms)')\n",
    "    ax2.set_title('CPU vs CUDA Time Comparison')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels([name[:20] + \"...\" if len(name) > 20 else name \n",
    "                        for name in names], rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'performance_bottlenecks.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return sorted_events\n",
    "\n",
    "top_bottlenecks = visualize_bottlenecks(detailed_events)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Timeline visualization**: You can open the generated Chrome trace file in Chrome browser by navigating to chrome://tracing and loading the JSON file for detailed timeline analysis. This gives a detailed, interactive timeline of CPU and GPU activities, helping you pinpoint performance bottlenecks like synchronization delays, long-running ops, or kernel launch gaps.\n",
    ">\n",
    "> And, it's also a great artifact to share with your team when reviewing model performance!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've learned how to diagnose and optimize AI model performance using a systematic, multi-layered profiling approach. Specifically, you have explored how to:\n",
    "\n",
    "- **Use PyTorch Profiler to identify performance bottlenecks** at both system and operation levels\n",
    "- **Monitor GPU utilization with pynvml** to understand hardware resource usage and detect under-utilization patterns\n",
    "- **Analyze timeline data** to find CPU-GPU synchronization issues and kernel launch overhead\n",
    "- **Conduct layer-by-layer performance analysis** to identify architectural inefficiencies within model execution\n",
    "- **Prioritize compound optimization opportunities** by distinguishing between macro-level (system) and micro-level (model) bottlenecks\n",
    "- **Apply systematic performance measurement** to validate which optimizations deliver the greatest impact\n",
    "\n",
    "\n",
    "These profiling skills directly translate to production scenarios where models perform differently than expected. **The key insight**: production performance problems are often compound issues requiring multiple optimization strategies, with clear prioritization based on profiling evidence rather than assumptions.\n",
    "\n",
    "The next time your model is slower than expected in production, you'll know exactly how to systematically diagnose both system-level and model-level inefficiencies, then prioritize fixes based on actual performance impact!\n",
    "\n",
    "**Bonus exploration ideas**:\n",
    "\n",
    "- Mixed precision profiling: Enable automatic mixed precision (AMP) and profile the performance/memory improvements\n",
    "- Memory timeline exploration: Use [`export_memory_timeline()`](https://docs.pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.export_memory_timeline) to spot fragmentation or memory leaks.\n",
    "- Batch size optimization: Find the optimal batch size for your specific model architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
