{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "intro_gpu_demo",
   "metadata": {},
   "source": [
    "# Demo: Understand your GPU's AI capabilities with NVIDIA tools\n",
    "\n",
    "Before optimizing models for hardware, you need to understand what your hardware can actually handle. Most AI deployment failures happen because teams guess at hardware capabilities instead of systematically assessing them.\n",
    "\n",
    "> **Overview**: We'll use NVIDIA diagnostic tools to systematically discover your GPU's AI deployment capabilities, translating raw hardware specifications into practical constraints and opportunities for machine learning workloads.\n",
    "> \n",
    "> **Goal**: Learn to assess GPU suitability for AI projects by extracting key hardware details and interpreting them for model architecture decisions, memory planning, and deployment feasibility.\n",
    "> \n",
    "> **Scenario**: You are a new Machine Learning Engineer who just joined an AI startup. The team has been working on various AI projects, but there's no clear documentation of what your development workstations can actually handle. Your manager asks you to:\n",
    "> \n",
    "> - _Audit the existing GPU infrastructure to understand current capabilities_\n",
    "> - _Determine what size models the team can realistically deploy_\n",
    "> - _Create hardware capability documentation for project planning_\n",
    "> - _Identify optimization opportunities with your current setup_\n",
    "> \n",
    "> The team has ambitious plans for a new large language model project, but nobody knows if your current hardware can handle it. This assessment will determine whether you can proceed with the planned model sizes or need to adjust your architectural approach to fit your hardware constraints.\n",
    "> \n",
    "> **Tools**: nvidia-smi, GPUtil, PyCUDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "setup_gpu_demo_title",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "Let's start by setting up our diagnostic environment and introducing the NVIDIA toolkit for hardware assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pip_install_gpu_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out the cell block again and restart the notebook\n",
    "# ! pip install GPUtil pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports_gpu_demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware assessment toolkit ready!\n"
     ]
    }
   ],
   "source": [
    "# Import core libraries \n",
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import GPU monitoring libraries\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    print(\"WARNING:  GPUtil not available - using nvidia-smi only\")\n",
    "try:\n",
    "    import pycuda.driver as cuda\n",
    "    import pycuda.autoinit  # Auto-initializes CUDA context\n",
    "except ImportError:\n",
    "    print(\"WARNING:  PyCUDA not available - won't be able to query detailed GPU device capabilities\")\n",
    "\n",
    "# Global storage for GPU information\n",
    "gpu_info = {}\n",
    "\n",
    "print(\"Hardware assessment toolkit ready!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NVIDIA diagnostic ecosystem**: The NVIDIA toolkit provides several complementary tools for hardware assessment:\n",
    "> \n",
    "> - **`nvidia-smi`**: NVIDIA's System Management Interface CLI tool - shows real-time GPU status, memory usage, and basic specs\n",
    "> - **`GPUtil`**: Lightweight Python wrapping around nvidia-smi â€” enables programmatic querying of GPU availability and usage.\n",
    "> - **`PyCUDA`**: Python bindings for CUDA Runtime and Driver API - allows querying GPU hardware capabilities and writing CUDA programs.\n",
    "> - **`pynvml`**: Python bindings for NVIDIA Management Library - provides GPU monitoring and management (temperature, utilization, memory usage, power draw, etc.).\n",
    "> \n",
    "> Each tool serves different purposes: `nvidia-smi` for quick status checks, `GPUtil` and `pynvml` libraries for automated analysis, and `PyCUDA` for programmatic CUDA development. Together, they provide comprehensive hardware visibility for AI deployment planning. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "basic_discovery_gpu_demo_title",
   "metadata": {},
   "source": [
    "## Step 2: Perform basic GPU discovery with nvidia-smi\n",
    "\n",
    "Let's start with `nvidia-smi` to get the essential information every AI engineer needs to know about their GPU.\n",
    "\n",
    "The basic GPU information reveals your fundamental constraints:\n",
    "- **Memory usage patterns**: High baseline usage (>50%) suggests other processes are competing for GPU resources\n",
    "- **Temperature monitoring**: GPUs throttle performance when overheating - sustained workloads require good cooling\n",
    "- **Power limits**: Higher TDP (Total Design Power) generally indicates more capable hardware for AI workloads\n",
    "\n",
    "Remember: most AI deployment failures happen because models exceed GPU memory, not compute capability.\n",
    " \n",
    "> **Pro tip:** A good rule of thumb for transformer models is that you typically need roughly 2GB VRAM per billion parameters for FP16 inference. \n",
    "> \n",
    "> _Note that this does not apply to training or inference with larger batches, longer sequences, or unquantized models; and, you should add a ~10â€“20% buffer for runtime overhead._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "basic_discovery_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering your GPU configuration...\n",
      "\n",
      "BASIC GPU INFORMATION\n",
      "==================================================\n",
      "  GPU details for gpu_0\n",
      "\tGPU Model: Tesla T4\n",
      "\tDriver Version: 555.42.06\n",
      "\tCUDA Version: 12.5\n",
      "\tTotal VRAM: 15360 MiB\n",
      "\tUsed VRAM: 103 MiB\n",
      "\tFree VRAM: 14815 MiB\n",
      "\tTemperature: 35 C\n",
      "\tPower Usage: 28.41 W / 70.00 W\n",
      "\tGPU Utilization: 0 %\n"
     ]
    }
   ],
   "source": [
    "def get_basic_gpu_info():\n",
    "    \"\"\"Extract basic GPU information using nvidia-smi\"\"\"\n",
    "    try:\n",
    "        # Query GPU information using nvidia-smi\n",
    "        cmd = [\n",
    "            'nvidia-smi', \n",
    "            '--query-gpu=name,driver_version,memory.total,memory.used,memory.free,temperature.gpu,power.draw,power.limit,utilization.gpu',\n",
    "            '--format=csv,noheader,nounits'\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            # Parse the CSV output\n",
    "            lines = result.stdout.strip().split('\\n')\n",
    "            gpu_info = {}\n",
    "            \n",
    "            for i, line in enumerate(lines):\n",
    "                if line.strip():  # Skip empty lines\n",
    "                    values = [v.strip() for v in line.split(',')]\n",
    "                    if len(values) >= 9:\n",
    "                        gpu_info[f'gpu_{i}'] = {\n",
    "                            'name': values[0],\n",
    "                            'driver_version': values[1],\n",
    "                            'memory_total': f\"{values[2]} MiB\",\n",
    "                            'memory_used': f\"{values[3]} MiB\",\n",
    "                            'memory_free': f\"{values[4]} MiB\",\n",
    "                            'temperature': f\"{values[5]} C\",\n",
    "                            'power_draw': f\"{values[6]} W\",\n",
    "                            'power_limit': f\"{values[7]} W\",\n",
    "                            'gpu_util': f\"{values[8]} %\"\n",
    "                        }\n",
    "            \n",
    "            # Get CUDA version separately\n",
    "            cuda_result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "            if cuda_result.returncode == 0:\n",
    "                cuda_match = re.search(r'CUDA Version: ([\\d.]+)', cuda_result.stdout)\n",
    "                cuda_version = cuda_match.group(1) if cuda_match else 'Unknown'\n",
    "                \n",
    "                # Add CUDA version to each GPU\n",
    "                for gpu_key in gpu_info:\n",
    "                    gpu_info[gpu_key]['cuda_version'] = cuda_version\n",
    "            \n",
    "            return gpu_info\n",
    "        else:\n",
    "            print(f\"ERROR: nvidia-smi query failed: {result.stderr}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not get GPU info: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get basic GPU information\n",
    "print(\"Discovering your GPU configuration...\\n\")\n",
    "gpu_info = get_basic_gpu_info()\n",
    "\n",
    "if gpu_info:\n",
    "    print(\"BASIC GPU INFORMATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for gpu_id, gpu_details in gpu_info.items():\n",
    "\n",
    "        print(f\"  GPU details for {gpu_id}\")\n",
    "        \n",
    "        print(f\"\\tGPU Model: {gpu_details['name']}\")\n",
    "        print(f\"\\tDriver Version: {gpu_details['driver_version']}\")\n",
    "        print(f\"\\tCUDA Version: {gpu_details['cuda_version']}\")\n",
    "        print(f\"\\tTotal VRAM: {gpu_details['memory_total']}\")\n",
    "        print(f\"\\tUsed VRAM: {gpu_details['memory_used']}\")\n",
    "        print(f\"\\tFree VRAM: {gpu_details['memory_free']}\")\n",
    "        print(f\"\\tTemperature: {gpu_details['temperature']}\")\n",
    "        print(f\"\\tPower Usage: {gpu_details['power_draw']} / {gpu_details['power_limit']}\")\n",
    "        print(f\"\\tGPU Utilization: {gpu_details['gpu_util']}\")\n",
    "else:\n",
    "    print(\"ERROR: Could not retrieve GPU information\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **AI deployment interpretation of basic GPU information with T4 GPUs:**\n",
    "> \n",
    "> - **MEMORY ANALYSIS â€” What model sizes are feasible?** Good VRAM availability enables medium-large model inference. Current free memory determines immediate deployment readiness.\n",
    "> \n",
    "> - **THERMAL STATUS â€” Can the hardware sustain long jobs?** Temperature <40Â°C indicates excellent cooling for sustained workloads. >80Â°C suggests thermal throttling risk.\n",
    "> \n",
    "> - **POWER EFFICIENCY â€” Whatâ€™s the best use of each GPU?** Low power draw with high performance indicates inference-optimized hardware suitable for production deployment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "architecture_analysis_gpu_demo_title",
   "metadata": {},
   "source": [
    "## Step 3: Understand your GPU generation from official specs\n",
    "\n",
    "Not all GPUs are created equal when it comes to AI workloads. They vary widely in architecture, performance, power consumption, and target use case. \n",
    "\n",
    "To effectively plan your AI projects, itâ€™s important to understand where your Tesla T4 fits in the broader GPU ecosystem.\n",
    "\n",
    "#### The broader GPU landscape breaks down roughly into three categories:\n",
    "\n",
    "- **Gaming GPUs (e.g., NVIDIA RTX 30/40 series):** For developers and researchers. They deliver strong performance for both training and inference at a consumer-friendly price point. The large VRAM and high raw compute power make them great for prototyping and medium-scale training.\n",
    "\n",
    "- **Data Center GPUs (e.g., Tesla T4, A100):** For stability, efficiency, and reliability in professional environments. They range from lower power and capacity end for inference workloads with high uptime and energy efficiency tio higher-end cards for large-scale training and inference but at much higher power and cost.\n",
    "\n",
    "- **AI Accelerators (e.g., NVIDIA H100):** For cutting training performance in AI research labs. They come with the highest VRAM, tensor throughput, and power consumption to support large-scale distributed training, complex model architectures, and the latest deep learning optimizations.\n",
    "\n",
    "---\n",
    "\n",
    "### Where does the Tesla T4 fit?\n",
    "\n",
    "Looking at [official NVIDIA specs](https://www.nvidia.com/en-us/data-center/tesla-t4/), the Tesla T4 is a **Turing-based, inference-optimized data center GPU** designed to provide **cost-effective, reliable AI inference** at scale. Itâ€™s excellent for deploying medium-sized models (around 7B parameters with quantization) and computer vision workloads in production environments where **power efficiency and stability** are critical.\n",
    " \n",
    "Compared to gaming GPUs, the T4 lacks the raw training power and VRAM capacity needed for large or fast training iterations, but it beats them on power consumption and sustained operation reliability. Against high-end data center cards like the A100 or H100, the T4 is far more modest â€” a practical workhorse for inference, not heavy training.\n",
    "\n",
    "---\n",
    "\n",
    "### What does this mean for your AI startup?\n",
    "\n",
    "Your Tesla T4s are **well-suited for the deployment phase of AI projects**, especially when running inference workloads for chatbots, vision models, or recommendation engines. However, they **do not provide the horsepower to train large language models from scratch**. For ambitious training goals, youâ€™ll need to consider either **upgrading hardware**, **leveraging cloud training instances**, or **adapting your model architecture to fit smaller GPUs**.\n",
    "\n",
    "- **If your main goal is serving trained models reliably and efficiently:**  \n",
    "  Tesla T4 is your best betâ€”low power, stable, and cost-effective.\n",
    "\n",
    "- **If you need a flexible workstation for experimentation, prototyping, and medium-scale training:**  \n",
    "  Consider gaming GPUs like RTX 3080 or RTX 4090.\n",
    "\n",
    "- **If you plan to train large language models or conduct heavy multi-GPU workloads:**  \n",
    "  Invest in data center GPUs like A100 or H100, or use cloud services offering these cards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "architecture_analysis_gpu_demo_popup",
   "metadata": {
    "tags": [
     "popup"
    ]
   },
   "source": [
    "\n",
    "> **Bonus: GPU decision guide for AI workloads:**\n",
    "> \n",
    "> | Use Case                     | Tesla T4                       | Gaming GPUs (RTX 30/40 Series)          | High-End Data Center GPUs (A100, H100)      |\n",
    "> |------------------------------|-------------------------------|-----------------------------------------|----------------------------------------------|\n",
    "> | **Primary Purpose**           | Inference-focused, low-power   | Balanced training & inference           | Large-scale training & high-throughput inference |\n",
    "> | **Memory Capacity**           | 15 GB                         | 8â€“24 GB (varies by model)                | 40+ GB (A100), 80+ GB (H100)                  |\n",
    "> | **Training Large Models**     | Not recommended               | Good for small to medium models          | Excellent for large models and multi-GPU setups |\n",
    "> | **Inference Performance**     | Very efficient for medium models | Strong performance, flexible             | Best for very large models and heavy inference   |\n",
    "> | **Power Consumption**         | Low (70W)                    | Moderate to High (200-450W)               | High (400-700W)                                |\n",
    "> | **Deployment Suitability**    | Ideal for 24/7 production inference | Good for development and smaller deployments | Best for training clusters and enterprise deployments |\n",
    "> | **Cost Considerations**       | Low operational cost          | Moderate initial and operational cost   | High upfront and running costs                  |\n",
    "> | **Software & Framework Support** | Excellent for inference (TensorRT, ONNX) | Broad, best for research & development   | Cutting-edge support, optimized for large models |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get detailed hardware specs with PyCUDA\n",
    "\n",
    "Now let's dive deeper into your GPU's architecture using `PyCUDA` to understand the compute capabilities that directly impact AI performance.\n",
    "\n",
    "Understanding your GPU's internal architecture helps predict performance bottlenecks:\n",
    "\n",
    "- **Memory hierarchy**: L2 cache size and bandwidth often determine AI performance more than raw compute power - transformers have repetitive memory access patterns that benefit from efficient caching\n",
    "\n",
    "- **Compute capability**: Determines which AI accelerations are available (Tensor Cores for mixed precision, hardware-accelerated quantization, sparsity support)\n",
    "\n",
    "- **Streaming multiprocessors (SMs)**: The parallel execution units that determine batch processing capability and multi-stream inference performance\n",
    "\n",
    "- **Concurrent execution**: Ability to overlap operations (attention + FFN computation) for pipeline optimization in modern inference\n",
    "\n",
    "- **Unified memory support**: Enables running models larger than GPU RAM by transparently using system memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Analyze memory hierarchy with PyCUDA\n",
    "\n",
    "Memory hierarchy is often the primary bottleneck in AI workloads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMORY ARCHITECTURE ANALYSIS\n",
      "==================================================\n",
      "Total Memory: 14.6 GB\n",
      "Memory Bandwidth: 320064 GB/s\n",
      "L2 Cache: 4.0 MB\n",
      "Memory Clock: 5001 MHz\n",
      "Bus Width: 256 bits\n",
      "\n",
      "Memory Features:\n",
      "  Unified Memory: âœ“\n",
      "  Host Memory Mapping: âœ“\n",
      "  ECC Protection: âœ“\n",
      "\n",
      "Bandwidth Assessment: EXCELLENT - Memory-optimized for large models\n"
     ]
    }
   ],
   "source": [
    "def analyze_memory_architecture():\n",
    "    \"\"\"Analyze GPU memory hierarchy and bandwidth\"\"\"\n",
    "    try:\n",
    "        device = cuda.Device(0)\n",
    "        attrs = device.get_attributes()\n",
    "        \n",
    "        memory_specs = {\n",
    "            # Basic memory info\n",
    "            'total_memory_gb': device.total_memory() / (1024**3),\n",
    "            'memory_clock_rate': attrs[cuda.device_attribute.MEMORY_CLOCK_RATE],  # kHz\n",
    "            'memory_bus_width': attrs[cuda.device_attribute.GLOBAL_MEMORY_BUS_WIDTH],  # bits\n",
    "            'l2_cache_size_mb': attrs[cuda.device_attribute.L2_CACHE_SIZE] / (1024**2),\n",
    "            \n",
    "            # Memory features\n",
    "            'unified_memory': bool(attrs[cuda.device_attribute.MANAGED_MEMORY]),\n",
    "            'can_map_host_memory': bool(attrs[cuda.device_attribute.CAN_MAP_HOST_MEMORY]),\n",
    "            'ecc_enabled': bool(attrs[cuda.device_attribute.ECC_ENABLED]),\n",
    "        }\n",
    "        \n",
    "        # Calculate memory bandwidth (theoretical peak)\n",
    "        memory_specs['memory_bandwidth_gbps'] = (\n",
    "            memory_specs['memory_clock_rate'] * memory_specs['memory_bus_width'] * 2  # DDR\n",
    "        ) / (8 * 1000)  # Convert to GB/s\n",
    "        \n",
    "        return memory_specs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not analyze memory architecture: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyze memory architecture\n",
    "memory_info = analyze_memory_architecture()\n",
    "\n",
    "if memory_info:\n",
    "    print(\"MEMORY ARCHITECTURE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Total Memory: {memory_info['total_memory_gb']:.1f} GB\")\n",
    "    print(f\"Memory Bandwidth: {memory_info['memory_bandwidth_gbps']:.0f} GB/s\")\n",
    "    print(f\"L2 Cache: {memory_info['l2_cache_size_mb']:.1f} MB\")\n",
    "    print(f\"Memory Clock: {memory_info['memory_clock_rate'] / 1000:.0f} MHz\")\n",
    "    print(f\"Bus Width: {memory_info['memory_bus_width']} bits\")\n",
    "    \n",
    "    print(f\"\\nMemory Features:\")\n",
    "    print(f\"  Unified Memory: {'âœ“' if memory_info['unified_memory'] else 'âœ—'}\")\n",
    "    print(f\"  Host Memory Mapping: {'âœ“' if memory_info['can_map_host_memory'] else 'âœ—'}\")\n",
    "    print(f\"  ECC Protection: {'âœ“' if memory_info['ecc_enabled'] else 'âœ—'}\")\n",
    "    \n",
    "    # Bandwidth assessment\n",
    "    bandwidth = memory_info['memory_bandwidth_gbps']\n",
    "    if bandwidth >= 1000:\n",
    "        rating = \"EXCELLENT - Memory-optimized for large models\"\n",
    "    elif bandwidth >= 500:\n",
    "        rating = \"GOOD - Adequate for most AI workloads\"\n",
    "    else:\n",
    "        rating = \"LIMITED - Focus on model compression\"\n",
    "    \n",
    "    print(f\"\\nBandwidth Assessment: {rating}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    gpu_info['memory'] = memory_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "compute_analysis_gpu_demo_popup",
   "metadata": {
    "tags": [
     "popup"
    ]
   },
   "source": [
    "> **Optimization strategies based on bandwidth**: With 320 GB/s bandwidth and 4MB L2 cache, this GPU balances memory and compute well for inference workloads. The bandwidth is sufficient for transformer models up to 7-13B parameters.\n",
    "> \n",
    "> - _Batch optimization_: Use larger batch sizes to maximize memory bandwidth utilization\n",
    "> \n",
    "> - _Mixed precision_: FP16 with Tensor Cores reduces memory traffic by ~50%\n",
    "> \n",
    "> - _Model size sweet spot_: 7B quantized models or smaller FP16 models for optimal performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Analyze compute capabilities with PyCUDA\n",
    "\n",
    "Understanding your GPU's compute architecture helps predict performance characteristics and available optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTE CAPABILITIES ANALYSIS\n",
      "==================================================\n",
      "Compute Capability: 7.5\n",
      "Streaming Multiprocessors: 40\n",
      "CUDA Cores per SM: 64\n",
      "Total CUDA Cores: 2,560\n",
      "Base Clock: 1590 MHz\n",
      "Peak FP32 Performance: 4070 GFLOPS\n",
      "\n",
      "Execution Features:\n",
      "  Concurrent Kernels: âœ“\n",
      "  Copy Engines: 3\n",
      "  Max Threads/SM: 1,024\n"
     ]
    }
   ],
   "source": [
    "def analyze_compute_capabilities():\n",
    "    \"\"\"Analyze GPU compute architecture and capabilities\"\"\"\n",
    "    try:\n",
    "        device = cuda.Device(0)\n",
    "        attrs = device.get_attributes()\n",
    "        \n",
    "        compute_specs = {\n",
    "            # Basic compute info\n",
    "            'name': device.name(),\n",
    "            'compute_capability_major': attrs[cuda.device_attribute.COMPUTE_CAPABILITY_MAJOR],\n",
    "            'compute_capability_minor': attrs[cuda.device_attribute.COMPUTE_CAPABILITY_MINOR],\n",
    "            'multiprocessor_count': attrs[cuda.device_attribute.MULTIPROCESSOR_COUNT],\n",
    "            'clock_rate_mhz': attrs[cuda.device_attribute.CLOCK_RATE] / 1000,  # Convert to MHz\n",
    "            \n",
    "            # Execution capabilities\n",
    "            'max_threads_per_multiprocessor': attrs[cuda.device_attribute.MAX_THREADS_PER_MULTIPROCESSOR],\n",
    "            'max_shared_memory_per_multiprocessor': attrs[cuda.device_attribute.MAX_SHARED_MEMORY_PER_MULTIPROCESSOR],\n",
    "            'concurrent_kernels': bool(attrs[cuda.device_attribute.CONCURRENT_KERNELS]),\n",
    "            'async_engine_count': attrs[cuda.device_attribute.ASYNC_ENGINE_COUNT],\n",
    "        }\n",
    "        \n",
    "        # Calculate derived metrics\n",
    "        compute_specs['compute_capability'] = (\n",
    "            compute_specs['compute_capability_major'] + \n",
    "            compute_specs['compute_capability_minor'] / 10\n",
    "        )\n",
    "        \n",
    "        # CUDA cores estimation (architecture-dependent)\n",
    "        cores_per_sm = {\n",
    "            9: 128,  # Hopper (H100)\n",
    "            8: 64,   # Ampere (A100, RTX 30/40 varies)\n",
    "            7: 64,   # Turing (T4) / Volta (V100)\n",
    "            6: 64,   # Pascal\n",
    "            5: 128,  # Maxwell\n",
    "        }.get(compute_specs['compute_capability_major'], 64)\n",
    "        \n",
    "        compute_specs['cores_per_sm'] = cores_per_sm\n",
    "        compute_specs['total_cuda_cores'] = compute_specs['multiprocessor_count'] * cores_per_sm\n",
    "        \n",
    "        # Theoretical peak performance\n",
    "        compute_specs['peak_fp32_gflops'] = (\n",
    "            compute_specs['total_cuda_cores'] * compute_specs['clock_rate_mhz'] / 1000\n",
    "        )\n",
    "        \n",
    "        return compute_specs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not analyze compute capabilities: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyze compute capabilities\n",
    "compute_info = analyze_compute_capabilities()\n",
    "\n",
    "if compute_info:\n",
    "    print(\"COMPUTE CAPABILITIES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Compute Capability: {compute_info['compute_capability']:.1f}\")\n",
    "    print(f\"Streaming Multiprocessors: {compute_info['multiprocessor_count']}\")\n",
    "    print(f\"CUDA Cores per SM: {compute_info['cores_per_sm']}\")\n",
    "    print(f\"Total CUDA Cores: {compute_info['total_cuda_cores']:,}\")\n",
    "    print(f\"Base Clock: {compute_info['clock_rate_mhz']:.0f} MHz\")\n",
    "    print(f\"Peak FP32 Performance: {compute_info['peak_fp32_gflops']:.0f} GFLOPS\")\n",
    "    \n",
    "    print(f\"\\nExecution Features:\")\n",
    "    print(f\"  Concurrent Kernels: {'âœ“' if compute_info['concurrent_kernels'] else 'âœ—'}\")\n",
    "    print(f\"  Copy Engines: {compute_info['async_engine_count']}\")\n",
    "    print(f\"  Max Threads/SM: {compute_info['max_threads_per_multiprocessor']:,}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    gpu_info['compute'] = compute_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Compute architecture insights**: \n",
    "> \n",
    "> - _Streaming multiprocessors (SMs)_ are the parallel execution units. More SMs enable higher batch throughput and better multi-stream inference performance.\n",
    "> \n",
    "> - _Overlapping computation (attention + FFN) and memory transfer_  improve concurrent execution.\n",
    "> \n",
    "> - _Concurrent kernels and multiple copy engines_ enable pipeline optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Analyze AI-specific features with PyCUDA\n",
    "\n",
    "Let's identify hardware acceleration features specifically designed for AI workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-SPECIFIC FEATURES ANALYSIS\n",
      "==================================================\n",
      "AI Hardware Rating: Mid-Range AI\n",
      "Tensor Cores: 2nd Gen Tensor Cores\n",
      "Supported Precisions: FP16, INT8, INT4\n",
      "\n",
      "Advanced Features:\n",
      "  Sparsity Support: âœ—\n",
      "  Transformer Engine: âœ—\n",
      "  Concurrent Execution: âœ“\n",
      "\n",
      "Optimization Recommendations:\n",
      "  âœ“ Use mixed precision training (FP16/FP32)\n",
      "  âœ“ Use CUDA streams for pipeline optimization\n"
     ]
    }
   ],
   "source": [
    "def analyze_ai_features(compute_info):\n",
    "    \"\"\"Analyze AI-specific hardware features and optimizations\"\"\"\n",
    "    \n",
    "    if not compute_info:\n",
    "        print(\"Compute info not available for AI feature analysis\")\n",
    "        return None\n",
    "    \n",
    "    ai_features = {\n",
    "        'compute_capability': compute_info['compute_capability'],\n",
    "        'concurrent_execution': compute_info['concurrent_kernels'],\n",
    "        'copy_engines': compute_info['async_engine_count'],\n",
    "    }\n",
    "    \n",
    "    # Determine Tensor Core support and AI capabilities\n",
    "    compute_major = compute_info['compute_capability_major']\n",
    "    compute_minor = compute_info['compute_capability_minor']\n",
    "    \n",
    "    if compute_major >= 9:\n",
    "        ai_features.update({\n",
    "            'tensor_cores': '4th Gen Tensor Cores',\n",
    "            'supported_precisions': ['FP8', 'FP16', 'BF16', 'TF32', 'FP64'],\n",
    "            'ai_rating': 'Flagship AI',\n",
    "            'sparsity_support': True,\n",
    "            'transformer_engine': True\n",
    "        })\n",
    "    elif compute_major >= 8:\n",
    "        ai_features.update({\n",
    "            'tensor_cores': '3rd Gen Tensor Cores', \n",
    "            'supported_precisions': ['FP16', 'BF16', 'TF32', 'sparse'],\n",
    "            'ai_rating': 'High-End AI',\n",
    "            'sparsity_support': True,\n",
    "            'transformer_engine': False\n",
    "        })\n",
    "    elif compute_major == 7 and compute_minor >= 5:\n",
    "        ai_features.update({\n",
    "            'tensor_cores': '2nd Gen Tensor Cores',\n",
    "            'supported_precisions': ['FP16', 'INT8', 'INT4'],\n",
    "            'ai_rating': 'Mid-Range AI',\n",
    "            'sparsity_support': False,\n",
    "            'transformer_engine': False\n",
    "        })\n",
    "    elif compute_major >= 7:\n",
    "        ai_features.update({\n",
    "            'tensor_cores': '1st Gen Tensor Cores',\n",
    "            'supported_precisions': ['FP16'],\n",
    "            'ai_rating': 'Entry AI',\n",
    "            'sparsity_support': False,\n",
    "            'transformer_engine': False\n",
    "        })\n",
    "    else:\n",
    "        ai_features.update({\n",
    "            'tensor_cores': 'None',\n",
    "            'supported_precisions': ['FP32'],\n",
    "            'ai_rating': 'Software Only',\n",
    "            'sparsity_support': False,\n",
    "            'transformer_engine': False\n",
    "        })\n",
    "    \n",
    "    return ai_features\n",
    "\n",
    "# Analyze AI-specific features\n",
    "ai_features = analyze_ai_features(compute_info)\n",
    "\n",
    "if ai_features:\n",
    "    print(\"AI-SPECIFIC FEATURES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"AI Hardware Rating: {ai_features['ai_rating']}\")\n",
    "    print(f\"Tensor Cores: {ai_features['tensor_cores']}\")\n",
    "    print(f\"Supported Precisions: {', '.join(ai_features['supported_precisions'])}\")\n",
    "    \n",
    "    print(f\"\\nAdvanced Features:\")\n",
    "    print(f\"  Sparsity Support: {'âœ“' if ai_features['sparsity_support'] else 'âœ—'}\")\n",
    "    print(f\"  Transformer Engine: {'âœ“' if ai_features['transformer_engine'] else 'âœ—'}\")\n",
    "    print(f\"  Concurrent Execution: {'âœ“' if ai_features['concurrent_execution'] else 'âœ—'}\")\n",
    "    \n",
    "    # Performance optimization recommendations\n",
    "    print(f\"\\nOptimization Recommendations:\")\n",
    "    if 'FP16' in ai_features['supported_precisions']:\n",
    "        print(f\"  âœ“ Use mixed precision training (FP16/FP32)\")\n",
    "    if 'BF16' in ai_features['supported_precisions']:\n",
    "        print(f\"  âœ“ Use bfloat16 for better numerical stability\")\n",
    "    if 'TF32' in ai_features['supported_precisions']:\n",
    "        print(f\"  âœ“ Enable TF32 for automatic speedup (default in newer PyTorch)\")\n",
    "    if ai_features['sparsity_support']:\n",
    "        print(f\"  âœ“ Consider structured sparsity for 2:4 speedups\")\n",
    "    if ai_features['concurrent_execution']:\n",
    "        print(f\"  âœ“ Use CUDA streams for pipeline optimization\")\n",
    "    \n",
    "    # Store for later use\n",
    "    gpu_info['ai_features'] = ai_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tesla T4 AI acceleration features**: 2nd Generation Tensor Cores provide solid hardware acceleration for common AI precisions, though missing some cutting-edge optimizations.\n",
    "> \n",
    "> - **Sparsity Support: âœ—** - Structured sparsity (2:4 patterns) can provide 2x speedups on newer architectures by skipping zero weights. T4 requires dense computations, but you can still benefit from model pruning for memory savings.\n",
    "> \n",
    "> - **Transformer Engine: âœ—** - Automatic precision switching for transformer models (available on H100+). T4 requires manual mixed precision setup, but TensorRT provides similar optimizations with more configuration.\n",
    "> \n",
    "> - **Concurrent Execution: âœ“** - Enables overlapping computation and memory transfers. Critical for T4's efficiency - allows processing multiple inference requests simultaneously or pipelining large model operations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "software_compatibility_gpu_demo_title",
   "metadata": {},
   "source": [
    "## Step 5: Analyze software ecosystem compatibility\n",
    "\n",
    "Having powerful hardware means nothing if your AI frameworks can't use it properly. \n",
    " \n",
    "- **CUDA version mismatches**: Older CUDA versions can't access newer GPU features, limiting performance\n",
    "- **Compute capability requirements**: Each framework has minimum requirements - older GPUs may miss out on optimizations\n",
    "- **Driver dependencies**: AI frameworks are sensitive to GPU driver versions - too old or too new can cause issues\n",
    "\n",
    "Let's check software compatibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "software_compatibility_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFTWARE COMPATIBILITY ANALYSIS\n",
      "==================================================\n",
      "CUDA Version: Unknown\n",
      "Driver Version: Unknown\n",
      "Compute Capability: 7.5\n",
      "\n",
      "FRAMEWORK COMPATIBILITY:\n",
      "\n",
      "   Pytorch:\n",
      "      PASS: Fully supported (CUDA Unknown â‰¥ 12.1)\n",
      "            Latest optimizations available\n",
      "\n",
      "   Tensorflow:\n",
      "      PASS: Fully supported (CUDA Unknown â‰¥ 12.2)\n",
      "            Latest optimizations available\n",
      "\n",
      "   Jax:\n",
      "      PASS: Fully supported (CUDA Unknown â‰¥ 12.1)\n",
      "            Latest optimizations available\n",
      "\n",
      "   Huggingface:\n",
      "      PASS: Fully supported (CUDA Unknown â‰¥ 12.1)\n",
      "            Latest optimizations available\n",
      "\n",
      "OPTIMIZATION TOOLS COMPATIBILITY:\n",
      "\n",
      "   TensorRT:\n",
      "      PASS: Supported (Compute 7.5 â‰¥ 7.0)\n",
      "\n",
      "   cuDNN:\n",
      "      PASS: CUDA compatibility confirmed\n",
      "\n",
      "   NVIDIA Triton:\n",
      "      PASS: Supported (Compute 7.5 â‰¥ 7.0)\n",
      "\n",
      "   Mixed Precision:\n",
      "      PASS: Supported (Compute 7.5 â‰¥ 7.0)\n",
      "      WARNING: Software-only mixed precision (slower)\n"
     ]
    }
   ],
   "source": [
    "def check_software_compatibility(gpu_info):\n",
    "    \"\"\"Check compatibility with popular AI frameworks\"\"\"\n",
    "    \n",
    "    cuda_version = gpu_info.get('cuda_version', 'Unknown')\n",
    "    driver_version = gpu_info.get('driver_version', 'Unknown')\n",
    "    compute_cap = gpu_info.get('architecture', {}).get('compute_cap', 7.5)\n",
    "    \n",
    "    print(f\"CUDA Version: {cuda_version}\")\n",
    "    print(f\"Driver Version: {driver_version}\")\n",
    "    print(f\"Compute Capability: {compute_cap}\")\n",
    "    \n",
    "    # Framework compatibility matrix\n",
    "    compatibility = {\n",
    "        'pytorch': {'min_cuda': '11.0', 'recommended_cuda': '12.1', 'min_compute': '7.0'},\n",
    "        'tensorflow': {'min_cuda': '11.2', 'recommended_cuda': '12.2', 'min_compute': '7.0'},\n",
    "        'jax': {'min_cuda': '11.1', 'recommended_cuda': '12.1', 'min_compute': '7.0'},\n",
    "        'huggingface': {'min_cuda': '11.0', 'recommended_cuda': '12.1', 'min_compute': '7.0'},\n",
    "    }\n",
    "    \n",
    "    def version_compare(v1, v2):\n",
    "        \"\"\"Simple version comparison\"\"\"\n",
    "        try:\n",
    "            v1_parts = [int(x) for x in v1.split('.')]\n",
    "            v2_parts = [int(x) for x in v2.split('.')]\n",
    "            return v1_parts >= v2_parts\n",
    "        except:\n",
    "            return True  # Assume compatible if can't parse\n",
    "    \n",
    "    def compute_compare(cc1, cc2):\n",
    "        \"\"\"Compare compute capabilities\"\"\"\n",
    "        try:\n",
    "            return float(cc1) >= float(cc2)\n",
    "        except:\n",
    "            return True  # Assume compatible if can't parse\n",
    "    \n",
    "    print(\"\\nFRAMEWORK COMPATIBILITY:\")\n",
    "    \n",
    "    for framework, reqs in compatibility.items():\n",
    "        framework_name = framework.title().replace('_', ' ')\n",
    "        \n",
    "        # Check CUDA compatibility\n",
    "        cuda_ok = version_compare(cuda_version, reqs['min_cuda'])\n",
    "        cuda_optimal = version_compare(cuda_version, reqs['recommended_cuda'])\n",
    "        \n",
    "        # Check compute capability\n",
    "        compute_ok = compute_compare(compute_cap, reqs['min_compute'])\n",
    "        \n",
    "        print(f\"\\n   {framework_name}:\")\n",
    "        \n",
    "        if cuda_ok and compute_ok:\n",
    "            if cuda_optimal:\n",
    "                print(f\"      PASS: Fully supported (CUDA {cuda_version} â‰¥ {reqs['recommended_cuda']})\")\n",
    "                print(f\"            Latest optimizations available\")\n",
    "            else:\n",
    "                print(f\"      PASS: Supported (CUDA {cuda_version} â‰¥ {reqs['min_cuda']})\")\n",
    "                print(f\"            Consider CUDA {reqs['recommended_cuda']}+ for best performance\")\n",
    "        elif cuda_ok:\n",
    "            print(f\"      WARNING: Limited support (Compute Capability {compute_cap} < {reqs['min_compute']})\")\n",
    "            print(f\"               Some modern features unavailable\")\n",
    "        else:\n",
    "            print(f\"      WARNING: Not supported (CUDA {cuda_version} < {reqs['min_cuda']})\")\n",
    "            print(f\"               Update CUDA toolkit required\")\n",
    "    \n",
    "    # Additional optimization tools\n",
    "    print(\"\\nOPTIMIZATION TOOLS COMPATIBILITY:\")\n",
    "    \n",
    "    optimization_tools = {\n",
    "        'TensorRT': {'min_compute': '7.0', 'optimal_compute': '8.0'},\n",
    "        'cuDNN': {'min_cuda': '11.0', 'optimal_cuda': '12.1'},\n",
    "        'NVIDIA Triton': {'min_compute': '7.0', 'optimal_compute': '8.6'},\n",
    "        'Mixed Precision': {'min_compute': '7.0', 'tensor_cores': True}\n",
    "    }\n",
    "    \n",
    "    for tool, reqs in optimization_tools.items():\n",
    "        print(f\"\\n   {tool}:\")\n",
    "        \n",
    "        if 'min_compute' in reqs:\n",
    "            if compute_compare(compute_cap, reqs['min_compute']):\n",
    "                if 'optimal_compute' in reqs and compute_compare(compute_cap, reqs['optimal_compute']):\n",
    "                    print(f\"      PASS: Fully supported (Compute {compute_cap} â‰¥ {reqs['optimal_compute']})\")\n",
    "                else:\n",
    "                    print(f\"      PASS: Supported (Compute {compute_cap} â‰¥ {reqs['min_compute']})\")\n",
    "            else:\n",
    "                print(f\"      WARNING: Not supported (Compute {compute_cap} < {reqs['min_compute']})\")\n",
    "        \n",
    "        if 'min_cuda' in reqs:\n",
    "            if version_compare(cuda_version, reqs['min_cuda']):\n",
    "                print(f\"      PASS: CUDA compatibility confirmed\")\n",
    "            else:\n",
    "                print(f\"      WARNING: CUDA {reqs['min_cuda']}+ required\")\n",
    "        \n",
    "        if tool == 'Mixed Precision':\n",
    "            arch = gpu_info.get('architecture', {}).get('arch', '')\n",
    "            if any(x in arch for x in ['Ampere', 'Ada', 'Hopper']):\n",
    "                print(f\"      PASS: Advanced Tensor Cores available (BF16, TF32 support)\")\n",
    "            elif 'Turing' in arch or 'Volta' in arch:\n",
    "                print(f\"      PASS: Basic Tensor Cores available (FP16 support)\")\n",
    "            else:\n",
    "                print(f\"      WARNING: Software-only mixed precision (slower)\")\n",
    "\n",
    "# Perform software compatibility check\n",
    "if gpu_info:\n",
    "    print(\"SOFTWARE COMPATIBILITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    check_software_compatibility(gpu_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "software_compatibility_gpu_demo_popup",
   "metadata": {
    "tags": [
     "popup"
    ]
   },
   "source": [
    "> **Software ecosystem compatibility insights**: Tesla T4 with CUDA 12.4 and Compute Capability 7.5 represents an excellent software compatibility profile - you have access to the full AI development ecosystem without any significant limitations.\n",
    "> \n",
    "> *Framework readiness:* All major AI frameworks (PyTorch, TensorFlow, JAX, Hugging Face) are fully supported with latest optimizations. \n",
    "> \n",
    "> *Production-ready optimization stack:* TensorRT, cuDNN, and NVIDIA Triton are all fully supported, giving you access to enterprise-grade inference optimization.  TensorRT with Tensor Cores alone can deliver 2-5x inference speedups through layer fusion, precision optimization, and kernel auto-tuning.\n",
    "> \n",
    "> **Pro tip**: Keep CUDA toolkit, drivers, and frameworks in sync. The NVIDIA documentation provides compatibility matrices for each release."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "gpu_profile_card_gpu_demo_title",
   "metadata": {},
   "source": [
    "## Step 6: Generate your GPU AI capability profile card\n",
    "\n",
    "Let's synthesize everything we've learned into a comprehensive, actionable GPU profile that you can reference for future AI projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING COMPREHENSIVE GPU AI PROFILE...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "                         GPU AI CAPABILITY PROFILE                           \n",
      "================================================================================\n",
      "\n",
      "HARDWARE IDENTITY\n",
      "--------------------------------------------------------------------------------\n",
      "Model:               Tesla T4\n",
      "Tier:                Entry Professional\n",
      "Compute Capability:  7.5\n",
      "CUDA Version:        12.5\n",
      "\n",
      "PERFORMANCE PROFILE  \n",
      "--------------------------------------------------------------------------------\n",
      "Total VRAM:          14.6 GB\n",
      "Memory Bandwidth:    320064 GB/s (Excellent)\n",
      "CUDA Cores:          2,560 (Moderate Compute)\n",
      "AI Acceleration:     2nd Gen Tensor Cores\n",
      "\n",
      "MODEL CAPACITY ESTIMATES\n",
      "--------------------------------------------------------------------------------\n",
      "FP16 Inference:\n",
      "  7B parameters:     âœ— NOT SUPPORTED\n",
      "  13B parameters:     âœ— NOT SUPPORTED\n",
      "  30B parameters:     âœ— NOT SUPPORTED\n",
      "  65B parameters:     âœ— NOT SUPPORTED\n",
      "\n",
      "INT8 Quantized:\n",
      "  7B parameters:     âœ“ SUPPORTED\n",
      "  13B parameters:     âœ— NOT SUPPORTED\n",
      "  30B parameters:     âœ— NOT SUPPORTED\n",
      "  65B parameters:     âœ— NOT SUPPORTED\n",
      "\n",
      "OPTIMIZATION RECOMMENDATIONS\n",
      "--------------------------------------------------------------------------------\n",
      "âœ“ Enable mixed precision (FP16) for 1.5-2x speedup\n",
      "\n",
      "CAPABILITY SUMMARY\n",
      "--------------------------------------------------------------------------------\n",
      "Primary Strength:     Small-Medium Models\n",
      "Best Use Case:        entry professional applications\n",
      "Framework Support:    All major AI frameworks fully supported\n",
      "Optimization Tools:   TensorRT, cuDNN, mixed precision available\n",
      "\n",
      "================================================================================\n",
      "DEPLOYMENT RECOMMENDATION: This GPU is optimized for entry professional applications\n",
      "with excellent memory performance and moderate compute capabilities.\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ’¡ Pro tip: Save this profile for project planning and hardware decisions!\n"
     ]
    }
   ],
   "source": [
    "def generate_gpu_profile_card(gpu_info):\n",
    "    \"\"\"Generate comprehensive GPU AI capability profile\"\"\"\n",
    "    \n",
    "    # Extract key metrics\n",
    "    first_gpu = next(iter(gpu_info.values()))\n",
    "    memory_info = gpu_info.get('memory', {})\n",
    "    compute_info = gpu_info.get('compute', {})\n",
    "    ai_features = gpu_info.get('ai_features', {})\n",
    "    \n",
    "    # Basic info\n",
    "    gpu_name = first_gpu.get('name', 'Unknown')\n",
    "    memory_gb = memory_info.get('total_memory_gb', 0)\n",
    "    memory_bandwidth = memory_info.get('memory_bandwidth_gbps', 0)\n",
    "    compute_cap = compute_info.get('compute_capability', 0)\n",
    "    cuda_cores = compute_info.get('total_cuda_cores', 0)\n",
    "    \n",
    "    # Determine GPU tier\n",
    "    if memory_gb >= 32 and compute_cap >= 8.0:\n",
    "        tier = \"High-End Data Center\"\n",
    "    elif memory_gb >= 16 and compute_cap >= 7.5:\n",
    "        tier = \"Mid-Range Data Center\" \n",
    "    elif memory_gb >= 8 and compute_cap >= 7.0:\n",
    "        tier = \"Entry Professional\"\n",
    "    else:\n",
    "        tier = \"Legacy/Gaming\"\n",
    "    \n",
    "    # Model capacity estimates\n",
    "    def get_model_support_by_precision(memory_gb, gb_per_billion_params):\n",
    "        models = {\n",
    "            \"7B\": memory_gb >= (7 * gb_per_billion_params + 2),  # +2GB overhead\n",
    "            \"13B\": memory_gb >= (13 * gb_per_billion_params + 2),\n",
    "            \"30B\": memory_gb >= (30 * gb_per_billion_params + 2), \n",
    "            \"65B\": memory_gb >= (65 * gb_per_billion_params + 2)\n",
    "        }\n",
    "        return models\n",
    "    \n",
    "    fp16_models = get_model_support_by_precision(memory_gb, 2.0)\n",
    "    int8_models = get_model_support_by_precision(memory_gb, 1.0)  # Roughly half memory for INT8\n",
    "    \n",
    "    # Performance ratings\n",
    "    if memory_bandwidth >= 1000:\n",
    "        memory_rating = \"Excellent\"\n",
    "    elif memory_bandwidth >= 500:\n",
    "        memory_rating = \"Good\"\n",
    "    else:\n",
    "        memory_rating = \"Limited\"\n",
    "    \n",
    "    if cuda_cores >= 5000:\n",
    "        compute_rating = \"High\"\n",
    "    elif cuda_cores >= 2000:\n",
    "        compute_rating = \"Moderate\"\n",
    "    else:\n",
    "        compute_rating = \"Basic\"\n",
    "    \n",
    "    # Generate profile\n",
    "    profile = f\"\"\"\n",
    "================================================================================\n",
    "                         GPU AI CAPABILITY PROFILE                           \n",
    "================================================================================\n",
    "\n",
    "HARDWARE IDENTITY\n",
    "{'-' * 80}\n",
    "Model:               {gpu_name}\n",
    "Tier:                {tier}\n",
    "Compute Capability:  {compute_cap:.1f}\n",
    "CUDA Version:        {first_gpu.get('cuda_version', 'Unknown')}\n",
    "\n",
    "PERFORMANCE PROFILE  \n",
    "{'-' * 80}\n",
    "Total VRAM:          {memory_gb:.1f} GB\n",
    "Memory Bandwidth:    {memory_bandwidth:.0f} GB/s ({memory_rating})\n",
    "CUDA Cores:          {cuda_cores:,} ({compute_rating} Compute)\n",
    "AI Acceleration:     {ai_features.get('tensor_cores', 'Unknown')}\n",
    "\n",
    "MODEL CAPACITY ESTIMATES\n",
    "{'-' * 80}\n",
    "FP16 Inference:\"\"\"\n",
    "    \n",
    "    for model, supported in fp16_models.items():\n",
    "        status = \"âœ“ SUPPORTED\" if supported else \"âœ— NOT SUPPORTED\"\n",
    "        profile += f\"\"\"\n",
    "  {model} parameters:     {status}\"\"\"\n",
    "    \n",
    "    profile += f\"\"\"\n",
    "\n",
    "INT8 Quantized:\"\"\"\n",
    "    \n",
    "    for model, supported in int8_models.items():\n",
    "        status = \"âœ“ SUPPORTED\" if supported else \"âœ— NOT SUPPORTED\"\n",
    "        profile += f\"\"\"\n",
    "  {model} parameters:     {status}\"\"\"\n",
    "    \n",
    "    profile += f\"\"\"\n",
    "\n",
    "OPTIMIZATION RECOMMENDATIONS\n",
    "{'-' * 80}\"\"\"\n",
    "    \n",
    "    if ai_features.get('tensor_cores') != 'None':\n",
    "        profile += f\"\"\"\n",
    "âœ“ Enable mixed precision (FP16) for 1.5-2x speedup\"\"\"\n",
    "    \n",
    "    if 'TensorRT' in str(ai_features):\n",
    "        profile += f\"\"\"\n",
    "âœ“ Use TensorRT for 2-5x inference optimization\"\"\"\n",
    "    \n",
    "    if memory_rating == \"Limited\":\n",
    "        profile += f\"\"\"\n",
    "âš  Focus on quantization and model compression\"\"\"\n",
    "    \n",
    "    if tier == \"Mid-Range Data Center\":\n",
    "        profile += f\"\"\"\n",
    "âœ“ Ideal for production inference workloads\"\"\"\n",
    "    \n",
    "    profile += f\"\"\"\n",
    "\n",
    "CAPABILITY SUMMARY\n",
    "{'-' * 80}\n",
    "Primary Strength:     {\"Large Model Training\" if memory_gb >= 32 else \"Inference Workloads\" if memory_gb >= 16 else \"Small-Medium Models\"}\n",
    "Best Use Case:        {tier.lower()} applications\n",
    "Framework Support:    All major AI frameworks fully supported\n",
    "Optimization Tools:   TensorRT, cuDNN, mixed precision available\n",
    "\n",
    "================================================================================\n",
    "DEPLOYMENT RECOMMENDATION: This GPU is optimized for {tier.lower()} applications\n",
    "with {memory_rating.lower()} memory performance and {compute_rating.lower()} compute capabilities.\n",
    "================================================================================\n",
    "\"\"\"\n",
    "    \n",
    "    return profile\n",
    "\n",
    "# Generate and display profile\n",
    "if gpu_info:\n",
    "    print(\"GENERATING COMPREHENSIVE GPU AI PROFILE...\")\n",
    "    print()\n",
    "    \n",
    "    profile = generate_gpu_profile_card(gpu_info)\n",
    "    print(profile)\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Pro tip: Save this profile for project planning and hardware decisions!\")\n",
    "else:\n",
    "    print(\"ERROR: Cannot generate profile - missing GPU information\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "gpu_profile_card_gpu_demo_popup",
   "metadata": {
    "tags": [
     "popup"
    ]
   },
   "source": [
    "> **Using your comprehensive GPU profile**: This detailed assessment provides everything needed for informed AI deployment decisions:\n",
    "> \n",
    "> - **Project Planning**: Understing model size constraints before architecture decisions\n",
    "> - **Performance Optimization**: Prioritized list of techniques for maximum gains\n",
    "> - **Hardware Procurement**: Evidence-based upgrade recommendations\n",
    "> - **Team Documentation**: Shared understanding of infrastructure capabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "conclusion_gpu_demo",
   "metadata": {},
   "source": [
    "## Conclusion: From hardware discovery to AI deployment success\n",
    "\n",
    "Congratulations! You have systematically assessed your GPU's AI capabilities using professional diagnostic tools. This comprehensive analysis revealed:\n",
    "\n",
    "- **Hardware Architecture**: Understanding your GPU's compute capabilities, memory bandwidth, and AI-specific features\n",
    "- **Memory Constraints**: Precise model size limits for training and inference scenarios\n",
    "- **Performance Characteristics**: Bottleneck prediction and optimization opportunities\n",
    "- **Scaling Potential**: Multi-GPU capabilities and interconnect analysis\n",
    "- **Software Ecosystem**: Framework compatibility and optimization tool support\n",
    "- **Deployment Strategy**: Workload recommendations and optimization priorities\n",
    "\n",
    "##### Key takeaways:\n",
    "\n",
    "1. Hardware assessment should always precede model architecture decisions\n",
    "2. Memory bandwidth often matters more than raw compute power for AI workloads\n",
    "3. Modern GPUs require framework support to leverage advanced features like Tensor Cores\n",
    "4. Understanding your specific hardware constraints enables informed optimization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Appendix: _[Advanced]_ Multi-GPU topology and PCIe analysis\n",
    "\n",
    "For teams scaling beyond single GPUs, understanding your system's interconnect capabilities is crucial for distributed training and multi-GPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANALYZING MULTI-GPU TOPOLOGY\n",
      "============================================================\n",
      "{'gpu_count': 1,\n",
      " 'gpus': [{'index': 0,\n",
      "           'memory_gb': 15.0,\n",
      "           'name': 'Tesla T4',\n",
      "           'pci_bus_id': '00000000:00:1E.0',\n",
      "           'pcie_gen': '3',\n",
      "           'pcie_width': '8'}],\n",
      " 'interconnect_type': 'NVLink',\n",
      " 'topology_matrix': '\\t\\x1b[4mGPU0\\tCPU Affinity\\tNUMA Affinity\\tGPU NUMA '\n",
      "                    'ID\\x1b[0m\\n'\n",
      "                    'GPU0\\t X \\t0-3\\t0\\t\\tN/A\\n'\n",
      "                    '\\n'\n",
      "                    'Legend:\\n'\n",
      "                    '\\n'\n",
      "                    '  X    = Self\\n'\n",
      "                    '  SYS  = Connection traversing PCIe as well as the SMP '\n",
      "                    'interconnect between NUMA nodes (e.g., QPI/UPI)\\n'\n",
      "                    '  NODE = Connection traversing PCIe as well as the '\n",
      "                    'interconnect between PCIe Host Bridges within a NUMA '\n",
      "                    'node\\n'\n",
      "                    '  PHB  = Connection traversing PCIe as well as a PCIe '\n",
      "                    'Host Bridge (typically the CPU)\\n'\n",
      "                    '  PXB  = Connection traversing multiple PCIe bridges '\n",
      "                    '(without traversing the PCIe Host Bridge)\\n'\n",
      "                    '  PIX  = Connection traversing at most a single PCIe '\n",
      "                    'bridge\\n'\n",
      "                    '  NV#  = Connection traversing a bonded set of # '\n",
      "                    'NVLinks\\n'}\n"
     ]
    }
   ],
   "source": [
    "def analyze_system_topology():\n",
    "    \"\"\"Analyze multi-GPU topology and PCIe configuration\"\"\"\n",
    "    try:\n",
    "        # Get nvidia-smi topology information\n",
    "        topo_result = subprocess.run(['nvidia-smi', 'topo', '-m'], \n",
    "                                   capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        # Get detailed GPU information for all GPUs\n",
    "        multi_gpu_result = subprocess.run([\n",
    "            'nvidia-smi', \n",
    "            '--query-gpu=index,name,memory.total,pci.bus_id,pcie.link.gen.current,pcie.link.width.current',\n",
    "            '--format=csv,noheader,nounits'\n",
    "        ], capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        topology_info = {\n",
    "            'gpu_count': 0,\n",
    "            'gpus': [],\n",
    "            'topology_matrix': None,\n",
    "            'interconnect_type': 'Unknown'\n",
    "        }\n",
    "        \n",
    "        if multi_gpu_result.returncode == 0:\n",
    "            lines = multi_gpu_result.stdout.strip().split('\\n')\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    values = [v.strip() for v in line.split(',')]\n",
    "                    if len(values) >= 6:\n",
    "                        gpu_info = {\n",
    "                            'index': int(values[0]),\n",
    "                            'name': values[1],\n",
    "                            'memory_gb': int(values[2]) / 1024,\n",
    "                            'pci_bus_id': values[3],\n",
    "                            'pcie_gen': values[4],\n",
    "                            'pcie_width': values[5]\n",
    "                        }\n",
    "                        topology_info['gpus'].append(gpu_info)\n",
    "            \n",
    "            topology_info['gpu_count'] = len(topology_info['gpus'])\n",
    "        \n",
    "        # Parse topology matrix if available\n",
    "        if topo_result.returncode == 0:\n",
    "            topology_info['topology_matrix'] = topo_result.stdout\n",
    "            \n",
    "            # Determine interconnect type\n",
    "            if 'NV' in topo_result.stdout:  # NVLink connections\n",
    "                topology_info['interconnect_type'] = 'NVLink'\n",
    "            elif 'SYS' in topo_result.stdout:  # System/PCIe connections\n",
    "                topology_info['interconnect_type'] = 'PCIe'\n",
    "            else:\n",
    "                topology_info['interconnect_type'] = 'Mixed'\n",
    "        \n",
    "        return topology_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not analyze system topology: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyze system topology\n",
    "print(\"\\nANALYZING MULTI-GPU TOPOLOGY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gpu_info['topology'] = analyze_system_topology()\n",
    "pprint(gpu_info['topology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **GPU topology analysis**: For distributed training, NVLink provides significantly better scaling than PCIe. Single-GPU systems should focus on vertical optimization rather than horizontal scaling.\n",
    "> \n",
    "> *With single GPU*, prioritize model architecture optimization techniques over attempting multi-GPU approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
