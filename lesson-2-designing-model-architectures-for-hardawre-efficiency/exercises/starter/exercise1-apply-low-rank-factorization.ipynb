{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Apply Low-Rank Factorization for model parameter reduction\n",
    "\n",
    "You have learned about efficient architectural patterns in theory. Now it's time to implement one of the most fundamental techniques for reducing model memory footprint: low-rank factorization. This technique is essential when deploying models to memory-constrained environments like edge devices, mobile platforms, or cost-optimized cloud instances.\n",
    "\n",
    "> **Overview**: Implement and evaluate low-rank matrix factorization as a memory optimization technique, understand the trade-offs between compression ratio and model performance, and make data-driven decisions about which layers to factorize.\n",
    "> \n",
    "> **Scenario**: You work for an IoT company developing smart security cameras. Your image classification model works perfectly on development servers, but the target edge device has only 512MB RAM total. The hardware team can't increase memory due to cost constraints, so you need to optimize the model architecture to take less than 10MB.\n",
    "> \n",
    "> Your goal is to reduce model memory footprint to <=8MB with at most a 3% accuracy drop tolerance, through strategic low-rank factorization of the most memory-intensive layers.\n",
    "> \n",
    "> **Tools**: PyTorch, torchvision, matplotlib\n",
    "> <br>_Prior experience recommended!_\n",
    "> \n",
    "> **Estimated Time**: 15 minutes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Let's start by setting up our development environment and establishing our hardware context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "output_dir = \"assets/exercise1\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Development environment: {device}\")\n",
    "print(f\"Target deployment: Edge device with 512MB RAM constraint\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Development vs deployment environment**: We are developing on a T4 with 16GB memory but targeting edge devices with 512MB total RAM. \n",
    "> \n",
    "> We'll need to keep our target edge device constraints in mind throughout the exercise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and analyze our \"problematic\" model\n",
    "\n",
    "Now we'll load a custom model with large linear layers that actually benefit from low-rank factorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom model that better demonstrates low-rank factorization\n",
    "# We'll use a feature extractor + large MLP classifier (more realistic for edge deployment)\n",
    "class EdgeClassificationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model with large linear layers that benefit significantly from low-rank factorization.\n",
    "    This simulates an edge device model where the classifier dominates memory usage.\n",
    "    Uses flat layer names for easier access and factorization.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(EdgeClassificationModel, self).__init__()\n",
    "        \n",
    "        # Simple feature extractor (lightweight)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Large MLP classifier with flat layer names (memory-intensive - this is what we'll optimize)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 1024)     # 2,097,152 parameters (largest)\n",
    "        self.fc2 = nn.Linear(1024, 512)            # 524,288 parameters (second largest)  \n",
    "        self.fc3 = nn.Linear(512, 256)             # 131,072 parameters\n",
    "        self.fc4 = nn.Linear(256, num_classes)     # 2,560 parameters\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create our custom model\n",
    "original_model = EdgeClassificationModel(num_classes=10).to(device)\n",
    "\n",
    "print(\"Original model architecture:\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in original_model.parameters()):,}\")\n",
    "\n",
    "# Calculate memory usage\n",
    "def calculate_model_memory(model):\n",
    "    \"\"\"Calculate model memory usage in MB\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    total_size = param_size + buffer_size\n",
    "    return total_size / (1024**2)  # Convert to MB\n",
    "\n",
    "original_memory = calculate_model_memory(original_model)\n",
    "print(f\"Original model memory: {original_memory:.1f} MB\")\n",
    "print(f\"Edge device memory budget: 512 MB\")\n",
    "print(f\"Model memory utilization: {(original_memory/512)*100:.1f}%\")\n",
    "\n",
    "# TODO: Calculate the practical memory budget for model parameters on an edge device\n",
    "# HINT: Edge devices need memory for OS (~200MB), app framework (~100MB), buffers (~100MB), other processes (~50MB)\n",
    "# What percentage of 512MB remains available for model parameters?\n",
    "# Documentation: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#enable-async-data-loading\n",
    "practical_budget =  # Add your int/float value here\n",
    "\n",
    "if original_memory > practical_budget:\n",
    "    print(f\"\\nPROBLEM: Model ({original_memory:.1f}MB) exceeds practical edge budget ({practical_budget:.1f}MB)!\")\n",
    "    print(\"   SOLUTION: Apply low-rank factorization to reduce memory usage\")\n",
    "else:\n",
    "    print(\"\\nModel fits within practical edge device memory budget\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why is optimization relevant if the model fits in memory?** Production edge devices typically allocate:\n",
    "> \n",
    "> - Operating system overhead (~200MB)\n",
    "> - Application framework (~100MB)\n",
    "> - Input buffers and activations (~100MB)\n",
    "> - System processes (~50-100MB)\n",
    "> \n",
    "> This leaves only ~12MB for model parameters in a 512MB device. Our model's 10.5MB base is within limit, but factoring in activation memory and optimization headroom, compression becomes essential for reliable edge deployment without memory pressure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understand low-rank factorization theory\n",
    "\n",
    "Before diving into implementation, let's visualize what low-rank factorization actually does to neural network layers. Understanding the mathematical foundation will help us make better decisions about when and how to apply this technique.\n",
    "\n",
    "> **Important: Rank ratios vs Absolute ranks**: We'll use **rank ratios** (0.25, 0.5, 0.75) rather than absolute ranks. This means each layer gets a rank proportional to its size: larger layers get larger ranks, smaller layers get smaller ranks. This approach scales naturally across different layer dimensions and avoids the awkwardness of using the same absolute rank on layers of vastly different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up example visualization parameters: matrix input features (matrix_rows), matrix output features (matrix_cols), and rank ratio in (0, 1) (rank_ratio)\n",
    "# Hint: Try to replicate the dimensions of our model's fully connected (fc) layers, and cross-experiment with different ratios like 0.25, 0.5, ..., to observe how compression changes\n",
    "matrix_rows =  # Add your int value here\n",
    "matrix_cols =  # Add your int value here\n",
    "rank_ratio =  # Add your float value here\n",
    "\n",
    "# Visualize low-rank factorization concept\n",
    "print(\"Visualizing low-rank factorization concept...\")\n",
    "print(\"This shows how we can approximate a large weight matrix with two smaller matrices\")\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "# Original matrix\n",
    "original_matrix = np.random.randn(matrix_rows, matrix_cols)\n",
    "im1 = ax1.imshow(original_matrix[:50, :50], cmap='RdBu', vmin=-2, vmax=2)\n",
    "ax1.set_title(f'Original Layer\\n{matrix_rows}×{matrix_cols} = {matrix_rows*matrix_cols:,} parameters')\n",
    "ax1.set_xlabel('Input Features')\n",
    "ax1.set_ylabel('Output Features')\n",
    "\n",
    "# Calculate factorized matrix\n",
    "rank = int(rank_ratio * min(matrix_rows, matrix_cols))  # Convert ratio to actual rank\n",
    "U = np.random.randn(matrix_rows, rank)\n",
    "V = np.random.randn(rank, matrix_cols)\n",
    "factorized_matrix = U @ V\n",
    "\n",
    "im2 = ax2.imshow(factorized_matrix[:50, :50], cmap='RdBu', vmin=-2, vmax=2)\n",
    "ax2.set_title(f'Factorized Layer\\n{matrix_rows}×{rank} + {rank}×{matrix_cols} = {matrix_rows*rank + rank*matrix_cols:,} parameters')\n",
    "ax2.set_xlabel('Input Features')\n",
    "ax2.set_ylabel('Output Features')\n",
    "\n",
    "# TODO: Calculate the parameter reduction ratio for your chosen rank\n",
    "# HINT: The factorized count should be less than the original parameters as defined in `original_matriz`\n",
    "# Compare original parameter count with factorized count (including bias)\n",
    "original_params =  # Add your code here\n",
    "factorized_params =  # Add your code here\n",
    "reduction_ratio = factorized_params / original_params\n",
    "\n",
    "sizes = [original_params, factorized_params]\n",
    "labels = ['Original', 'Factorized']\n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "\n",
    "bars = ax3.bar(labels, sizes, color=colors, alpha=0.7)\n",
    "ax3.set_title(f'Parameter Reduction\\nRank ratio {rank_ratio} = {reduction_ratio:.1%} of original size')\n",
    "ax3.set_ylabel('Parameter Count')\n",
    "ax3.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, size in zip(bars, sizes):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5000, \n",
    "             f'{size:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'lowrank_concept.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Layer dimensions: {matrix_rows}×{matrix_cols}\")\n",
    "print(f\"Parameter reduction: {original_params:,} → {factorized_params:,}\")\n",
    "print(f\"Rank ratio: {rank_ratio} → Actual rank: {rank}\")\n",
    "print(f\"Compression ratio: {reduction_ratio:.1%} of original size\")\n",
    "print(f\"Memory savings: {(1-reduction_ratio)*100:.1f}% reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mathematical insight**: Low-rank factorization works because many neural network weight matrices contain redundant information—they have lower intrinsic dimensionality than their full size suggests. By decomposing W = U × V, we capture the most important patterns while eliminating redundancy. The rank parameter controls this trade-off: lower rank = more compression but potentially more information loss."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement low-rank factorization class\n",
    "\n",
    "Now comes the core implementation. We'll create a `LowRankLinear` class that can replace any standard PyTorch linear layer while using significantly fewer parameters. \n",
    "\n",
    "The key challenge is preserving as much of the original layer's learned information as possible during the factorization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowRankLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Low-rank approximation of a linear layer using matrix factorization.\n",
    "    Replaces weight matrix W (in_features × out_features) with \n",
    "    U (in_features × rank) and V (rank × out_features) such that W ≈ U @ V\n",
    "    \"\"\"\n",
    "    def __init__(self, original_layer, rank_ratio=0.5):\n",
    "        super(LowRankLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = original_layer.in_features\n",
    "        self.out_features = original_layer.out_features\n",
    "        \n",
    "        # TODO: Determine the rank dimension for factorization\n",
    "        # HINT: rank should be a fraction of the smallest dimension to ensure factorization makes sense\n",
    "        # What's the relationship between rank_ratio and min(in_features, out_features)?\n",
    "        # Make sure rank is at least 1 and doesn't exceed either input dimension\n",
    "        # Reference: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.rank =  # Add your code here\n",
    "        \n",
    "        # TODO: Design the factorized layer architecture\n",
    "        # HINT: You need two linear transformations: first reduces dimensionality, second expands it back\n",
    "        # Think about where bias should be placed - first layer, second layer, or both?\n",
    "        self.U =  # Add your code here\n",
    "        self.V =  # Add your code here\n",
    "        \n",
    "        # Initialize weights using SVD approximation of original weights\n",
    "        self._initialize_from_original(original_layer)\n",
    "        \n",
    "    def _initialize_from_original(self, original_layer):\n",
    "        \"\"\"Initialize factorized layers using SVD of original weights\"\"\"\n",
    "        with torch.no_grad():            \n",
    "            # TODO: Apply Singular Value Decomposition to extract the most important components\n",
    "            # HINT: PyTorch provides built-in functionalities that you can find at https://docs.pytorch.org/docs/stable/torch.html\n",
    "            # What transformation does the original layer's weight matrix need to undergo, according to theory and docs?\n",
    "            weight_matrix =  # Add your code here\n",
    "            U_full, S, V_full =  # Add your code here\n",
    "            \n",
    "            # TODO: Truncate SVD components to the desired rank\n",
    "            # HINT: Take only the first 'self.rank' components from each matrix\n",
    "            # How do the singular values (S) relate to the importance of each component?\n",
    "            U_truncated =  # Add your code here\n",
    "            S_truncated =  # Add your code here\n",
    "            V_truncated =  # Add your code here\n",
    "            \n",
    "            # TODO: Initialize the factorized layer weights for self.U and self.V using the truncated SVD\n",
    "            # HINT: The goal is to reconstruct W ≈ U @ diag(S) @ V^T\n",
    "            # How should you distribute the singular values between the two layers?\n",
    "            # One approach: put all singular values in the second layer\n",
    "\n",
    "            # Add your code here \n",
    "            \n",
    "            # TODO: Handle bias initialization\n",
    "            # HINT: Where was the bias in the original layer? Where should it go in the factorized version?\n",
    "\n",
    "            # Add your code here\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass through the factorized layers\n",
    "        # HINT: Data flows through U first, then through V\n",
    "        # This should be equivalent to a single matrix multiplication in the original layer\n",
    "        return  # Add your code here\n",
    "    \n",
    "    def parameter_count(self):\n",
    "        \"\"\"Calculate total parameters in factorized layer\"\"\"        \n",
    "        # TODO: Count all parameters in both U and V layers\n",
    "        # HINT: Include weights and biases from both layers\n",
    "        # How does this compare to the original layer's parameter count?\n",
    "        return  # Add your code here\n",
    "\n",
    "\n",
    "# Test the low-rank implementation\n",
    "print(\"Testing low-rank factorization implementation...\")\n",
    "\n",
    "# Create a test linear layer\n",
    "test_layer = nn.Linear(512, 256)\n",
    "\n",
    "# TODO: Calculate the original layer's parameter count\n",
    "# HINT: Don't forget to include bias parameters if they exist\n",
    "# Documentation: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "original_params =  # Add your code here\n",
    "\n",
    "# TODO: Test different rank ratios and analyze the compression achieved\n",
    "# HINT: Try ratios like 0.25, 0.5, 0.75 and observe the parameter reduction\n",
    "# What rank ratio gives you approximately 50% parameter reduction?\n",
    "rank_ratios = []  # Add your float values in [0, 1] here\n",
    "for ratio in rank_ratios:\n",
    "    lr_layer = LowRankLinear(test_layer, rank_ratio=ratio)\n",
    "    lr_params = lr_layer.parameter_count()\n",
    "    compression = lr_params / original_params\n",
    "    \n",
    "    print(f\"Rank ratio {ratio}: {original_params:,} → {lr_params:,} params ({compression:.1%} of original)\")\n",
    "\n",
    "print(\"Low-rank implementation complete!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Factorization implementation tip**: The SVD initialization is crucial for preserving model performance. By using the dominant singular values and vectors from the original weight matrix, we ensure our factorized layer starts with the most important information intact. Random initialization would require extensive retraining and likely result in significant performance degradation.\n",
    "> \n",
    "> **_Are you noticing a parameter increase at high ratios?_** That's expected! Remember: low-rank factorization only reduces parameters when rank < (m×n)/(m+n). Pick rank ratios that actually save parameters!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Identify optimization targets through layer analysis\n",
    "\n",
    "Before applying factorization everywhere, we need to strategically identify which layers offer the greatest memory reduction opportunities. Not all layers are equal—some dominate memory usage while others have minimal impact on our deployment constraints.\n",
    "\n",
    "*In our custom model, which layer is likely to be the largest?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_memory_usage(model):\n",
    "    \"\"\"Analyze memory usage of each layer to prioritize factorization\"\"\"\n",
    "    layer_info = {}\n",
    "    \n",
    "    # Iterate through all model linear layers and collect memory statistics\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # TODO: Calculate parameter count for this layer\n",
    "            # HINT: Don't forget weight matrix and bias vector\n",
    "            # Weight matrix: in_features × out_features\n",
    "            # Bias vector: out_features (if it exists)\n",
    "            params =  # Add your code here\n",
    "            if module.bias is not None:\n",
    "                params += module.out_features\n",
    "            \n",
    "            # TODO: Convert parameter count to memory usage\n",
    "            # HINT: Each float32 parameter uses 4 bytes\n",
    "            # Convert to KB for easier reading\n",
    "            memory_kb =  # Add your code here\n",
    "            \n",
    "            layer_info[name] = {\n",
    "                'type': 'Linear',\n",
    "                'in_features': module.in_features,\n",
    "                'out_features': module.out_features,\n",
    "                'parameters': params,\n",
    "                'memory_kb': memory_kb\n",
    "            }\n",
    "    \n",
    "    return layer_info\n",
    "\n",
    "# Analyze original model layers\n",
    "print(\"Analyzing layer memory usage to prioritize factorization targets...\")\n",
    "layer_analysis = analyze_layer_memory_usage(original_model)\n",
    "\n",
    "print(\"=== LAYER MEMORY ANALYSIS ===\")\n",
    "\n",
    "# TODO: Sort layers by memory usage to identify the biggest targets\n",
    "# HINT: Use sorted() with a key function to sort by memory_kb\n",
    "# Which layer consumes the most memory?\n",
    "sorted_layers =  # Add your code here\n",
    "\n",
    "total_linear_memory = sum(info['memory_kb'] for _, info in sorted_layers)\n",
    "print(f\"Total linear layer memory: {total_linear_memory:.1f} KB\\n\")\n",
    "\n",
    "for name, info in sorted_layers:\n",
    "    percentage = (info['memory_kb'] / total_linear_memory) * 100\n",
    "    print(f\"{name}: {info['parameters']:,} params ({info['memory_kb']:.1f} KB, {percentage:.1f}% of linear layers)\")\n",
    "\n",
    "# Identify the primary optimization target\n",
    "if sorted_layers:\n",
    "    largest_layer_name = sorted_layers[0][0]\n",
    "    largest_layer_info = sorted_layers[0][1]\n",
    "    print(f\"\\nPrimary target for factorization: {largest_layer_name}\")\n",
    "    print(f\"   {largest_layer_info['in_features']} → {largest_layer_info['out_features']} features\")\n",
    "    print(f\"   {largest_layer_info['memory_kb']:.1f} KB ({(largest_layer_info['memory_kb']/total_linear_memory)*100:.1f}% of linear layer memory)\")\n",
    "else:\n",
    "    print(\"No linear layers found for factorization\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO: Write a one-line markdown explanation of why your layer memory analysis is expected (or not), and what layers you want to factorize**\n",
    "> \n",
    "> ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create factorized model\n",
    "\n",
    "Now that we know which layers to work on, let's create the factorized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_factorized_model(original_model, target_layers, rank_ratio=0.5):\n",
    "    \"\"\"Create a new model with specified layers replaced by low-rank versions\"\"\"\n",
    "    \n",
    "    print(f\"Creating factorized model with rank ratio {rank_ratio}...\")\n",
    "    \n",
    "    # Create a copy of the original model architecture\n",
    "    factorized_model = EdgeClassificationModel(num_classes=10)\n",
    "    \n",
    "    # TODO: Transfer all weights from the original model\n",
    "    # HINT: Review how to load the state dict from https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "    # Don't forget to move the model to the correct device afterward\n",
    "\n",
    "     # Add your code here\n",
    "    \n",
    "    # Replace specified layers with low-rank versions\n",
    "    # HINT: Find inspiration at https://discuss.pytorch.org/t/replace-layers-in-model-by-another-and-with-extra-parameters/160601\n",
    "    # Use our own LowRankLinear class\n",
    "    for layer_name in target_layers:\n",
    "        print(f\"  Factorizing {layer_name}...\")\n",
    "        \n",
    "        # TODO: Get the original layer and create a low-rank replacement\n",
    "        original_layer =  # Add your code here\n",
    "        low_rank_layer =  # Add your code here\n",
    "        \n",
    "        # TODO: Replace the layer in the model\n",
    "        # Add your code here\n",
    "        \n",
    "        # TODO: Calculate and report the parameter reduction achieved\n",
    "        # HINT: Calculate the ratio of parameter counts before and after factorization\n",
    "        original_params =  # Add your code here\n",
    "        factorized_params =  # Add your code here\n",
    "        reduction =  # Add your code here\n",
    "        \n",
    "        print(f\"    {original_layer.in_features}×{original_layer.out_features} → rank {low_rank_layer.rank}\")\n",
    "        print(f\"    Parameters: {original_params:,} → {factorized_params:,} ({reduction:.1%} reduction)\")\n",
    "    \n",
    "    return factorized_model\n",
    "\n",
    "# TODO: Experiment with different compression levels\n",
    "# HINT: Try rank ratios that give different levels of compression\n",
    "rank_ratios = []  # Add your float values in [0,1] here\n",
    "factorized_models = {}\n",
    "\n",
    "# TODO: Choose which layer(s) to factorize\n",
    "# HINT: Based on your analysis above, which layer(s) are the best target?\n",
    "target_layers = []  # Add your string value(s) here\n",
    "\n",
    "for ratio in rank_ratios:\n",
    "    print(f\"\\n--- Creating model with rank ratio {ratio} ---\")\n",
    "    factorized_models[ratio] = create_factorized_model(original_model, target_layers, rank_ratio=ratio)\n",
    "\n",
    "print(\"\\nAll factorized models created successfully!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Benchmark and compare model performance\n",
    "\n",
    "Now comes the critical evaluation phase. We need to measure how our factorization affects the key metrics that matter for edge deployment: memory usage, inference speed, and model accuracy. This data will drive our final deployment decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, model_name, input_shape=(1, 3, 224, 224), num_iterations=50):\n",
    "    \"\"\"Comprehensive benchmark of model performance\"\"\"\n",
    "    print(f\"Benchmarking {model_name}...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_mb = calculate_model_memory(model)\n",
    "    \n",
    "    # Create sample input\n",
    "    sample_input = torch.randn(input_shape).to(device)\n",
    "    \n",
    "    times = []\n",
    "    # TODO: Implement accurate inference timing\n",
    "    # HINT: Find general best practices for the TODOs below at https://medium.com/@MarkAiCode/mastering-pytorch-inference-time-measurement-22da0eaebab7\n",
    "    # Also, three tips:\n",
    "    # 1. Don't forget to wrap your code in torch.no_grad\n",
    "    # 2. Using time.perf_counter() is recommended\n",
    "    # 3. Convert time to ms at the end\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    # Calculate timing statistics\n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    # SIMULATE accuracy based on compression level (more aggressive compression typically leads to more accuracy loss at around 3-8% according to literature)\n",
    "    # We select 0.05 as an average threshold here\n",
    "    # In practice, you'd evaluate on a validation dataset as the accuracy loss from factorization is highlight task dependant!\n",
    "    if \"factorized\" in model_name.lower():\n",
    "        # Extract rank ratio from model name and estimate accuracy loss\n",
    "        rank_ratio = float(model_name.split(\"_\")[-1]) if \"_\" in model_name else 0.5\n",
    "        accuracy_loss = (1 - rank_ratio) * 0.05  # Average 5% loss\n",
    "        accuracy = 0.92 - accuracy_loss  # Baseline minus estimated loss\n",
    "    else:\n",
    "        accuracy = 0.92  # Original model baseline\n",
    "    \n",
    "    return {\n",
    "        'name': model_name,\n",
    "        'memory_mb': memory_mb,\n",
    "        'inference_time_ms': avg_time,\n",
    "        'inference_std_ms': std_time,\n",
    "        'accuracy': accuracy,\n",
    "        'parameter_count': sum(p.numel() for p in model.parameters())\n",
    "    }\n",
    "\n",
    "# Benchmark all models\n",
    "print(\"=== COMPREHENSIVE MODEL BENCHMARKING ===\")\n",
    "benchmark_results = []\n",
    "\n",
    "# Original model\n",
    "original_benchmark = benchmark_model(original_model, \"Original\")\n",
    "benchmark_results.append(original_benchmark)\n",
    "\n",
    "# Benchmark all factorized models\n",
    "for ratio in rank_ratios:\n",
    "    factorized_benchmark = benchmark_model(\n",
    "        factorized_models[ratio].to(device), \n",
    "        f\"Factorized_{ratio}\",\n",
    "        num_iterations=50\n",
    "    )\n",
    "    benchmark_results.append(factorized_benchmark)\n",
    "\n",
    "# Create a comprehensive results table\n",
    "print(\"\\n=== BENCHMARK RESULTS ===\")\n",
    "print(f\"{'Model':<15} {'Memory (MB)':<12} {'Time (ms)':<12} {'Accuracy':<10} {'Parameters':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for result in benchmark_results:\n",
    "    print(f\"{result['name']:<15} {result['memory_mb']:<12.1f} \"\n",
    "          f\"{result['inference_time_ms']:<12.2f} {result['accuracy']:<10.1%} \"\n",
    "          f\"{result['parameter_count']:<12,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Performance benchmarking insight**: Notice how factorization primarily impacts memory usage and accuracy, while inference time slightly increases. This is because our matrices are relatively small, which means GPU kernel launch contributes to a large overhead. For medium-large models, latency is typically stable. \n",
    "> \n",
    "> The total number of operations (FLOPs) doesn't change dramatically—we're just reorganizing the computation across two matrix multiplications instead of one. This makes low-rank factorization particularly attractive for memory-constrained deployments where inference speed is also critical."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize trade-offs and make deployment decision\n",
    "\n",
    "The final step is analyzing our results to make an informed deployment decision. We'll create comprehensive visualizations that help us understand the trade-offs between compression, accuracy, and deployment feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualization\n",
    "print(\"Creating comprehensive trade-off analysis...\")\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Extract metrics from benchmark results for visualization\n",
    "model_names = [r['name'] for r in benchmark_results]\n",
    "memory_usage = [r['memory_mb'] for r in benchmark_results]\n",
    "inference_times = [r['inference_time_ms'] for r in benchmark_results]\n",
    "accuracies = [r['accuracy'] for r in benchmark_results]\n",
    "param_counts = [r['parameter_count'] for r in benchmark_results]\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#f39c12', '#9b59b6']\n",
    "\n",
    "# Create memory usage comparison chart with edge device memory budget line\n",
    "bars1 = ax1.bar(model_names, memory_usage, color=colors, alpha=0.8)\n",
    "ax1.axhline(y=60, color='red', linestyle='--', alpha=0.7, label='Edge Device Budget (60MB)')\n",
    "ax1.set_title('Model Memory Usage')\n",
    "ax1.set_ylabel('Memory (MB)')\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, mem in zip(bars1, memory_usage):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{mem:.1f}MB', ha='center', va='bottom')\n",
    "\n",
    "# Create inference time comparison\n",
    "bars2 = ax2.bar(model_names, inference_times, color=colors, alpha=0.8)\n",
    "ax2.set_title('Inference Time')\n",
    "ax2.set_ylabel('Time (ms)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, time_val in zip(bars2, inference_times):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{time_val:.2f}ms', ha='center', va='bottom')\n",
    "\n",
    "# Create accuracy comparison\n",
    "bars3 = ax3.bar(model_names, [a*100 for a in accuracies], color=colors, alpha=0.8)\n",
    "ax3.axhline(y=90, color='red', linestyle='--', alpha=0.7, label='Target Accuracy (90%)')\n",
    "ax3.set_title('Model Accuracy')\n",
    "ax3.set_ylabel('Accuracy (%)')\n",
    "ax3.legend()\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, acc in zip(bars3, accuracies):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{acc:.1%}', ha='center', va='bottom')\n",
    "\n",
    "# Create parameter count comparison\n",
    "bars4 = ax4.bar(model_names, param_counts, color=colors, alpha=0.8)\n",
    "ax4.set_title('Parameter Count')\n",
    "ax4.set_ylabel('Parameters')\n",
    "ax4.ticklabel_format(style='plain', axis='y')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, params in zip(bars4, param_counts):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
    "             f'{params:,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate efficiency metrics for deployment decision\n",
    "print(\"\\n=== DEPLOYMENT DECISION ANALYSIS ===\")\n",
    "\n",
    "# Extract baseline metrics for comparison\n",
    "original_memory = benchmark_results[0]['memory_mb']\n",
    "original_accuracy = benchmark_results[0]['accuracy']\n",
    "original_params = benchmark_results[0]['parameter_count']\n",
    "\n",
    "# Define deployment constraints\n",
    "edge_budget_mb = 60  # Practical memory budget\n",
    "accuracy_threshold = 0.90  # Minimum acceptable accuracy\n",
    "\n",
    "print(f\"Edge device constraints:\")\n",
    "print(f\"  Memory budget: {edge_budget_mb}MB\")\n",
    "print(f\"  Accuracy threshold: {accuracy_threshold:.0%}\")\n",
    "print()\n",
    "\n",
    "deployment_candidates = []\n",
    "\n",
    "# Analyze each factorized model against deployment constraints\n",
    "for result in benchmark_results[1:]:  # Skip original model\n",
    "    # Calculate compression and retention metrics\n",
    "    memory_reduction = (original_memory - result['memory_mb']) / original_memory\n",
    "    accuracy_retention = result['accuracy'] / original_accuracy\n",
    "    param_reduction = (original_params - result['parameter_count']) / original_params\n",
    "    \n",
    "    # Check constraint satisfaction\n",
    "    meets_memory = result['memory_mb'] <= edge_budget_mb\n",
    "    meets_accuracy = result['accuracy'] >= accuracy_threshold\n",
    "    deployable = meets_memory and meets_accuracy\n",
    "    \n",
    "    print(f\"{result['name']}:\")\n",
    "    print(f\"  Memory reduction: {memory_reduction:.1%}\")\n",
    "    print(f\"  Parameter reduction: {param_reduction:.1%}\")  \n",
    "    print(f\"  Accuracy retention: {accuracy_retention:.1%}\")\n",
    "    print(f\"  Meets memory constraint: {'PASS' if meets_memory else 'FAIL'}\")\n",
    "    print(f\"  Meets accuracy constraint: {'PASS' if meets_accuracy else 'FAIL'}\")\n",
    "    print(f\"  Deployable to edge: {'PASS' if deployable else 'FAIL'}\")\n",
    "    \n",
    "    if deployable:\n",
    "        deployment_candidates.append(result)\n",
    "    print()\n",
    "\n",
    "# Select the best deployable model\n",
    "if deployment_candidates:\n",
    "    best_model = max(deployment_candidates, key=lambda x: x['accuracy'])\n",
    "    print(f\"RECOMMENDED MODEL FOR DEPLOYMENT: {best_model['name']}\")\n",
    "    print(f\"   Memory: {best_model['memory_mb']:.1f}MB (fits in {edge_budget_mb}MB budget)\")\n",
    "    print(f\"   Accuracy: {best_model['accuracy']:.1%} (exceeds {accuracy_threshold:.0%} threshold)\")\n",
    "    print(f\"   Parameters: {best_model['parameter_count']:,}\")\n",
    "    print(f\"   Inference time: {best_model['inference_time_ms']:.2f}ms\")\n",
    "    \n",
    "    # Calculate and report the achieved savings\n",
    "    memory_saved = original_memory - best_model['memory_mb']\n",
    "    print(f\"   Memory savings: {memory_saved:.1f}MB ({(memory_saved/original_memory)*100:.1f}% reduction)\")\n",
    "else:\n",
    "    print(\"WARNING:  No factorized model meets both constraints!\")\n",
    "    print(\"   Consider these alternatives:\")\n",
    "    print(\"   1. Further compression (lower rank ratios)\")\n",
    "    print(\"   2. Factorizing additional layers\") \n",
    "    print(\"   3. Combining with other compression techniques (quantization, pruning)\")\n",
    "    print(\"   4. Reassessing deployment constraints with hardware team\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO: Analyze the trade-off results and identify the optimal deployment strategy**\n",
    "> \n",
    "> HINT: Focus on which model best balances memory constraints with accuracy requirements. Consider how different rank ratios affect the fundamental trade-off between model size and performance. What insights can you draw about the effectiveness of low-rank factorization for this specific deployment scenario? How relevant are the latency measurements?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've learned to implement and deploy low-rank matrix factorization as a strategic memory optimization technique for edge AI deployment. \n",
    "\n",
    "The systematic approach you've learned—analyze layers, implement factorization, benchmark comprehensively, make data-driven deployment decisions—directly transfers to production model optimization where resource constraints are non-negotiable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
