{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Transform a CNN for mobile-first deployment\n",
    "\n",
    "You have learned about mobile-first architectural patterns and efficient design principles in theory. Now it's time to apply these concepts by transforming a standard CNN into a mobile-optimized architecture that enables real-time inference on diverse mobile devices.\n",
    "\n",
    "> **Overview**: An app powered by Computer Vision suffers from poor performance on mid-range mobile devices. While the baseline CNN achieves excellent accuracy on server hardware, it struggles with memory limitations and slow inference times on budget smartphones, causing user frustration and poor app store reviews.\n",
    "> \n",
    "> **Scenario**: You work for a healthcare startup with a popular food recognition app that provides instant nutritional information from meal photos. Your app has 500K+ downloads but is receiving bad reviews because of a critical performance problem: the current ResNet-style CNN takes 3+ seconds on budget Android phones, takes >150MB storage, and drains 15% battery per session. \n",
    "> \n",
    "> **Goal**:  Transform the baseline CNN architecture using mobile-first design patterns to achieve <200ms inference time, <100MB storage usage, and preserve battery life while maintaining >92% accuracy across 50 food categories. Learn how architectural choices directly translate to mobile hardware performance.\n",
    "> \n",
    "> **Tools**: pytorch, torchvision, torchprofile (for FLOPS counting), datasets\n",
    "> \n",
    "> **Estimated Time**: 15 minutes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Let's establish our baseline environment and verify both T4 training capabilities and CPU edge simulation capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out the cell block again and restart the notebook\n",
    "# ! pip install torchprofile datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchprofile import profile_macs\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"assets/exercise3\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check hardware capabilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    \n",
    "    print(f\"Development GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_properties.total_memory / 1e9:.1f} GB\")\n",
    "    print(\"\\nIMPORTANT: Will train/analyze on GPU, simulate mobile deployment on CPU\")\n",
    "else:\n",
    "    print(\"CUDA not available - using CPU for both development and mobile simulation\")\n",
    "\n",
    "print(\"\\nEnvironment setup complete!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mobile hardware reality**: Unlike our development environments with 15.6GB of available GPU memory, mobile devices operate under severe constraints.\n",
    "Budget smartphones with 4GB RAM must share memory between the OS, apps, and your model. The ARM Cortex-A55 cores common in budget phones provide ~1/20th the computational power of your T4 GPU, making architectural efficiency crucial for acceptable performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the dataset\n",
    "\n",
    "For this exercise, we'll use a subset of the [Food-101](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) dataset hosted in HuggingFace, which contains only 100 samples per food clas. Also, we only select a subset of 10 classes out of the 101 available to speed up the exercise.\n",
    "\n",
    "We'll focus on architectural optimization rather than training, using the dataset primarily for inference performance measurement. **Bonus activity**: Why not add a training loop, and measure accuracy before / after optimization too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== LOADING THE SAMPLE DATASET ===\")\n",
    "\n",
    "# Load the pre-sampled Food-101 dataset from Hugging Face\n",
    "print(\"Loading Food-101 sample dataset from Hugging Face...\")\n",
    "dataset = load_dataset(\"JannikB/food101_sample_n100\")\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"  Train split: {len(dataset['train'])} samples\")\n",
    "print(f\"  Test split: {len(dataset['validation'])} samples\")\n",
    "\n",
    "# Check the dataset structure\n",
    "sample = dataset['train'][0]\n",
    "print(f\"  Sample keys: {list(sample.keys())}\")\n",
    "print(f\"  Image size: {sample['image'].size}\")\n",
    "all_classes = list(set(dataset[\"train\"][\"label\"]))\n",
    "print(\"All classes:\", all_classes)\n",
    "\n",
    "# Filter the dataset to only the first 10 classes for faster exercise completion\n",
    "selected_classes = all_classes[:10]\n",
    "def filter_classes(example):\n",
    "    return example[\"label\"] in selected_classes\n",
    "\n",
    "train_subset = dataset[\"train\"].filter(filter_classes)\n",
    "val_subset = dataset[\"validation\"].filter(filter_classes)\n",
    "\n",
    "print(f\"\\nFiltered to the first {len(selected_classes)} categories:\")\n",
    "print(f\"  Train samples: {len(train_subset)}\")\n",
    "print(f\"  Test samples: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class SampleDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Convert HuggingFace dataset to PyTorch dataset with mobile-friendly preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        \n",
    "        # Convert to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = item['label']\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = SampleDataset(train_subset, transform=transform)\n",
    "test_dataset = SampleDataset(val_subset, transform=transform)\n",
    "\n",
    "print(f\"PyTorch datasets created:\")\n",
    "print(f\"  Training: {len(train_dataset)} samples\")\n",
    "print(f\"  Testing: {len(test_dataset)} samples\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dataset considerations for mobile**: The 224x224 input size represents a balance between model accuracy and mobile processing constraints. Larger inputs (384x384+) would improve accuracy but significantly increase memory usage and computational requirements. \n",
    "> \n",
    "> Real mobile applications often use adaptive input sizing based on device capabilities - flagship phones might handle 256x256 while budget devices use 192x192."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create and analyze baseline CNN architecture\n",
    "\n",
    "Now let's create our baseline CNN - a standard architecture that works well on servers but struggles with mobile deployment due to inefficient parameter usage and memory access patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard CNN architecture that works on servers but struggles on mobile devices\n",
    "    Uses traditional convolution patterns with high parameter count and memory usage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(BaselineCNN, self).__init__()\n",
    "        \n",
    "        # Standard convolution blocks - memory intensive\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 224x224 -> 112x112\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 112x112 -> 56x56\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 56x56 -> 28x28\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 28x28 -> 14x14\n",
    "        )\n",
    "        \n",
    "        # Dense classification head - parameter heavy\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create baseline model\n",
    "baseline_model = BaselineCNN(num_classes=len(selected_classes)).to(device)\n",
    "\n",
    "print(\"Baseline CNN created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze baseline model's efficiency\n",
    "# Note that the same functions can be reused throughout the notebook\n",
    "\n",
    "def analyze_model_efficiency(model, model_name, input_size=(1, 3, 224, 224)):\n",
    "    \"\"\"\n",
    "    Perform a comprehensive efficiency analysis for mobile deployment.\n",
    "    \n",
    "    Measures:\n",
    "        - Parameter count\n",
    "        - Model size (MB)\n",
    "        - Peak activation memory (MB)\n",
    "        - FLOPs (via MACs profile)\n",
    "        - CPU inference time simulation\n",
    "        - Mobile deployment readiness flag\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to analyze.\n",
    "        model_name (str): A human-readable name for reporting.\n",
    "        input_size (tuple): Input tensor size.\n",
    "        device (str): Device for initial analysis (\"cpu\" recommended).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary statistics of the analysis.\n",
    "    \"\"\"\n",
    "    # Create model and dummy input on CPU\n",
    "    model = model.to(device)\n",
    "    model.cpu()\n",
    "    dummy_input = torch.randn(*input_size).to(\"cpu\")\n",
    "    dummy_input.cpu()\n",
    "\n",
    "    # Set model to eval model\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\\n=== {model_name.upper()} EFFICIENCY ANALYSIS ON CPU ===\")\n",
    "    \n",
    "    # Parameter analysis\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2\n",
    "    \n",
    "    print(f\"Parameters: {total_params:,}\")\n",
    "    print(f\"Model size: {model_size_mb:.1f} MB\")\n",
    "    \n",
    "    # Activation memory analysis\n",
    "    def _estimate_activation_memory(model, dummy_input):\n",
    "        \"\"\"\n",
    "        Estimate the peak activation memory usage during inference.\n",
    "        \n",
    "        Why: Activation memory is important for deployment on\n",
    "            resource-constrained devices (e.g., mobile, embedded).\n",
    "            \n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to analyze.\n",
    "        dummy_input (torch.Tensor): Example input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            float: Estimated peak activation memory in MB.\n",
    "        \"\"\"\n",
    "        activation_sizes = []   # Store size (in MB) for each activation\n",
    "        hooks = []              # Store hook references for removal later\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            \"\"\"Forward hook to record output tensor size.\"\"\"\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                size_mb = output.numel() * 4 / 1024**2  # float32 = 4 bytes\n",
    "                activation_sizes.append(size_mb)\n",
    "        \n",
    "        # Attach hooks to all \"leaf\" layers (no children)\n",
    "        for module in model.modules():\n",
    "            if len(list(module.children())) == 0:\n",
    "                hooks.append(module.register_forward_hook(hook_fn))\n",
    "    \n",
    "        # Run one forward pass with dummy input\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Peak memory: sum of the top 5 largest activations\n",
    "        peak_memory = sum(sorted(activation_sizes, reverse=True)[:5])\n",
    "        return peak_memory\n",
    "    \n",
    "    activation_memory = _estimate_activation_memory(model, dummy_input)\n",
    "    print(f\"Estimated activation memory: {activation_memory:.1f} MB\")\n",
    "    \n",
    "    # Computational complexity\n",
    "    macs = profile_macs(model, dummy_input)\n",
    "    flops = 2 * macs\n",
    "    print(f\"FLOPs: {flops / 1e9:.2f} G\")\n",
    "    \n",
    "    # Inference timing (mobile simulation on CPU) with warm-up\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            start = time.perf_counter()\n",
    "            _ = model(dummy_input)\n",
    "            end = time.perf_counter()\n",
    "            times.append(end - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "    print(f\"CPU inference time: {avg_time:.1f} ms (mobile simulation)\")\n",
    "    \n",
    "    # Mobile deployment assessment\n",
    "    mobile_ready = all([\n",
    "        model_size_mb < 50,  # <50MB model size for app store constraints\n",
    "        activation_memory < 100, # < 100MB total memory for 4GB devices\n",
    "        avg_time < 500  # <500ms on CPU for acceptable UX\n",
    "    ])\n",
    "    \n",
    "    print(f\"Mobile deployment ready: {'✓ Yes' if mobile_ready else '✗ No'}\")\n",
    "    \n",
    "    return {\n",
    "        'params': total_params,\n",
    "        'size_mb': model_size_mb,\n",
    "        'memory_mb': activation_memory,\n",
    "        'flops_g': flops / 1e9,\n",
    "        'cpu_time_ms': avg_time,\n",
    "        'mobile_ready': mobile_ready\n",
    "    }\n",
    "\n",
    "# Analyze baseline model efficiency\n",
    "baseline_stats = analyze_model_efficiency(baseline_model, \"Baseline CNN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Baseline architecture analysis**: The baseline CNN uses 3x3 convolutions throughout, creating significant parameter and memory overhead. \n",
    ">\n",
    "> Each 3x3 conv layer requires loading weight matrices and computing full convolutions across all input-output channel combinations. On mobile ARM processors, this creates memory bandwidth bottlenecks since each convolution requires multiple memory accesses per operation.\n",
    ">\n",
    "> The 30M+ parameters also exceed typical mobile app memory budgets, especially when combined with the OS and other applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Understand mobile architecture patterns\n",
    "\n",
    "Before transforming our model, let's review two fundamental architectural patterns that enable efficient mobile deployment: depthwise separable convolutions and mobile-friendly activations.\n",
    "\n",
    "These patterns address the core constraints of mobile hardware: limited memory bandwidth, constrained compute resources, and power efficiency requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_convolution_efficiency():\n",
    "    \"\"\"\n",
    "    Compare standard vs depthwise separable convolution patterns\n",
    "    to understand mobile optimization principles\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== CONVOLUTION PATTERN COMPARISON ===\")\n",
    "    \n",
    "    # Standard convolution parameters\n",
    "    in_channels, out_channels = 128, 256\n",
    "    kernel_size = 3\n",
    "    height, width = 56, 56\n",
    "    \n",
    "    print(f\"Input: {in_channels} channels, {height}x{width} feature map\")\n",
    "    print(f\"Output: {out_channels} channels\")\n",
    "    print()\n",
    "    \n",
    "    # Standard 3x3 convolution\n",
    "    std_params = out_channels * in_channels * kernel_size * kernel_size\n",
    "    std_flops = std_params * height * width\n",
    "    \n",
    "    print(\"STANDARD 3x3 CONVOLUTION:\")\n",
    "    print(f\"  Parameters: {std_params:,}\")\n",
    "    print(f\"  FLOPs: {std_flops / 1e6:.1f} M\")\n",
    "    print(f\"  Memory access pattern: Dense (all input-output channel combinations)\")\n",
    "    print()\n",
    "    \n",
    "    # Depthwise separable convolution\n",
    "    # Depthwise: each input channel gets its own 3x3 filter\n",
    "    dw_params = in_channels * kernel_size * kernel_size\n",
    "    dw_flops = dw_params * height * width\n",
    "    \n",
    "    # Pointwise: 1x1 convolution to combine channels\n",
    "    pw_params = in_channels * out_channels * 1 * 1\n",
    "    pw_flops = pw_params * height * width\n",
    "    \n",
    "    total_params = dw_params + pw_params\n",
    "    total_flops = dw_flops + pw_flops\n",
    "    \n",
    "    print(\"DEPTHWISE SEPARABLE CONVOLUTION:\")\n",
    "    print(f\"  Depthwise 3x3: {dw_params:,} params, {dw_flops / 1e6:.1f} M FLOPs\")\n",
    "    print(f\"  Pointwise 1x1: {pw_params:,} params, {pw_flops / 1e6:.1f} M FLOPs\")\n",
    "    print(f\"  Total: {total_params:,} params, {total_flops / 1e6:.1f} M FLOPs\")\n",
    "    print(f\"  Memory access pattern: Sparse (channel-wise then pointwise)\")\n",
    "    print()\n",
    "    \n",
    "    # Efficiency comparison\n",
    "    param_reduction = (std_params - total_params) / std_params * 100\n",
    "    flop_reduction = (std_flops - total_flops) / std_flops * 100\n",
    "    \n",
    "    print(\"EFFICIENCY GAINS:\")\n",
    "    print(f\"  Parameter reduction: {param_reduction:.1f}%\")\n",
    "    print(f\"  FLOP reduction: {flop_reduction:.1f}%\")\n",
    "    print(f\"  Speedup factor: {std_flops / total_flops:.1f}x\")\n",
    "    \n",
    "    return {\n",
    "        'standard': {'params': std_params, 'flops': std_flops},\n",
    "        'depthwise_sep': {'params': total_params, 'flops': total_flops},\n",
    "        'param_reduction_pct': param_reduction,\n",
    "        'flop_reduction_pct': flop_reduction\n",
    "    }\n",
    "\n",
    "# Demonstrate efficiency patterns\n",
    "efficiency_comparison = demonstrate_convolution_efficiency()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MOBILE ACTIVATION PATTERNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def compare_activation_functions():\n",
    "    \"\"\"Compare activation functions for mobile deployment\"\"\"\n",
    "    \n",
    "    x = torch.linspace(-3, 10, 1000)\n",
    "    \n",
    "    # Standard ReLU\n",
    "    relu_output = F.relu(x)\n",
    "    \n",
    "    # ReLU6 (mobile-friendly, quantization-robust)\n",
    "    relu6_output = F.relu6(x)\n",
    "    \n",
    "    # Swish/SiLU (modern, efficient)\n",
    "    swish_output = F.silu(x)\n",
    "    \n",
    "    # Hard-Swish (mobile-optimized version of Swish)\n",
    "    def hard_swish(x):\n",
    "        return x * F.relu6(x + 3) / 6\n",
    "    \n",
    "    hard_swish_output = hard_swish(x)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(x, relu_output, label='ReLU', linewidth=2, color='red')\n",
    "    plt.title('ReLU: Standard (unlimited output range)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(x, relu6_output, label='ReLU6', linewidth=2, color='blue')\n",
    "    plt.title('ReLU6: Mobile-friendly (clamped to [0,6])')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(x, swish_output, label='Swish/SiLU', linewidth=2, color='green')\n",
    "    plt.title('Swish: Smooth, non-monotonic')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(x, hard_swish_output, label='Hard-Swish', linewidth=2, color='orange')\n",
    "    plt.title('Hard-Swish: Mobile-optimized Swish')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'activation_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Activation function characteristics:\")\n",
    "    print(\"• ReLU: Unlimited range, potential overflow in quantization\")\n",
    "    print(\"• ReLU6: Bounded [0,6], quantization-friendly, mobile hardware optimized\")\n",
    "    print(\"• Swish: Smooth gradients, but computationally expensive (sigmoid)\")\n",
    "    print(\"• Hard-Swish: Mobile approximation of Swish, piecewise linear\")\n",
    "\n",
    "compare_activation_functions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mobile architecture insight**: \n",
    "> \n",
    "> Depthwise separable convolutions reduce parameters by ~8x and FLOPs by ~7x compared to standard convolutions by factorizing the spatial and channel-wise filtering operations. This dramatically reduces memory bandwidth requirements - instead of loading large weight tensors, mobile processors can work with smaller depthwise filters and 1x1 pointwise convolutions. \n",
    "> \n",
    "> Combined with bounded activations like ReLU6, these patterns enable quantization-robust inference on mobile hardware."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create mobile-friendly architectural blocks\n",
    "\n",
    "Now let's implement the fundamental building blocks for mobile-efficient architectures. These components form the foundation of modern mobile vision models like MobileNet, EfficientNet, and production mobile applications.\n",
    "\n",
    "Please note that this is just a subset of possible mobile optimizations which also include squeeze-and-excite blocks, grouped convolutions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose mobile-optimized activation function for all layers\n",
    "# HINT: Options include nn.ReLU(), nn.ReLU6(), nn.Hardswish(), nn.SiLU(), ...\n",
    "MOBILE_ACTIVATION =  # Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise Separable Convolution block - the fundamental mobile optimization pattern\n",
    "    Factorizes standard convolution into depthwise (spatial) + pointwise (channel) operations\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        \n",
    "        # TODO: Implement depthwise convolution\n",
    "        # HINT: Depthwise conv processes each input channel independently\n",
    "        # Use groups=in_channels to create separate filters for each input channel\n",
    "        # This reduces parameters from (in_ch × out_ch × k²) to (in_ch × k²)\n",
    "        # Reference: https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.depthwise =  # Add your code here\n",
    "        \n",
    "        # TODO: Implement pointwise convolution  \n",
    "        # HINT: Pointwise conv is 1x1 convolution that combines depthwise outputs\n",
    "        # Maps from in_channels to out_channels using 1x1 kernels\n",
    "        # Reference: https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.pointwise =  # Add your code here\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply the standard mobile convolution pattern: depthwise → BN → activation → pointwise → BN sequence\n",
    "        x = self.depthwise(x)\n",
    "        x = self.bn1(x)\n",
    "        x = MOBILE_ACTIVATION(x)\n",
    "        \n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Inverted Residual Block (MobileNetV2-style) - advanced mobile optimization pattern\n",
    "    Expands channels → applies depthwise conv → compresses back down\n",
    "    Inverted compared to ResNet: narrow → wide → narrow instead of wide → narrow → wide\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, expand_ratio=6):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "        \n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # TODO: Implement expansion phase (if needed)\n",
    "        # HINT: If expand_ratio > 1, add [1x1 conv + batchnorm + activation] to increase channels before depthwise        \n",
    "        # Example: 64 channels × 6 expand_ratio = 384 expanded channels\n",
    "        # Reference: https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        \n",
    "        # Add your code here\n",
    "        \n",
    "        # TODO: Add depthwise convolution phase by \n",
    "        # HINT: Apply depthwise conv on the expanded channels\n",
    "        # Use stride for downsampling if needed\n",
    "        # Reference: https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        depthwise_conv_layer =  # Add your code here\n",
    "        layers.extend([\n",
    "            depthwise_conv_layer,\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            MOBILE_ACTIVATION\n",
    "        ])\n",
    "        \n",
    "        # TODO: Add compression phase (pointwise linear)\n",
    "        # HINT: Compress back to output channels using 1x1 conv\n",
    "        pointwise_conv_layer =  # Add your code here\n",
    "        layers.extend([\n",
    "            pointwise_conv_layer,\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "            # No activation here - linear bottleneck\n",
    "        ])\n",
    "        \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = self.conv(x)\n",
    "        \n",
    "        # TODO: Add residual connection if applicable\n",
    "        # HINT: What do you need to return together with the result?\n",
    "        # Only add residual when input/output shapes match (stride=1, same channels)\n",
    "        \n",
    "        # Add your code here\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mobile building blocks insight**: These architectural patterns form the foundation of all modern mobile vision models (MobileNet, EfficientNet, MobileViT). \n",
    ">\n",
    "> Depthwise separable convolutions reduce memory bandwidth by ~8x by factorizing spatial and channel operations. Inverted residual blocks create an efficient information pathway: compress to narrow bottleneck → expand for rich processing → compress back down. This \"narrow-wide-narrow\" pattern maximizes representational capacity while minimizing parameter overhead.\n",
    ">\n",
    "> The key mobile design principle: **separate spatial processing from channel mixing** to leverage mobile hardware strengths while avoiding bandwidth bottlenecks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Transform model for mobile hardware\n",
    "\n",
    "Now assemble these mobile patterns into an optimized CNN architecture. You can follow the guided TODOs to create an efficient mobile-first design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile-optimized CNN that uses depthwise separable convolutions and inverted residuals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileCNN, self).__init__()\n",
    "        \n",
    "        # TODO: Create initial feature extraction layer\n",
    "        # HINT: Start with DepthwiseSeparableConv to immediately reduce parameters\n",
    "        # Use realistic mobile channel counts for practical deployment (not 256+ like servers)\n",
    "        self.initial_conv =  # Add your code here\n",
    "        \n",
    "        # TODO: Add mobile feature extraction blocks\n",
    "        # HINT: Use InvertedResidualBlock with progressive channel increase\n",
    "        # Typically channels are doubled with an expand_ratio of 4-6 for good accuracy/efficiency balance\n",
    "        self.block1 =  # Add your code here\n",
    "        \n",
    "        self.block2 =  # Add your code here\n",
    "        \n",
    "        self.block3 =  # Add your code here\n",
    "        \n",
    "        self.block4 =  # Add your code here\n",
    "        \n",
    "        # TODO: Create mobile-optimized classifier\n",
    "        # HINT: Use global average pooling to eliminate spatial dimensions\n",
    "        # Keep final linear layer reasonable for mobile deployment\n",
    "        self.global_pool =  # Add your code here\n",
    "        self.classifier =  # Add your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass through mobile architecture\n",
    "        # HINT: The pattern is initial_conv → blocks → global_pool → flatten → classifier\n",
    "        # What about mobile activations?\n",
    "\n",
    "        # Add your code here\n",
    "        \n",
    "        return x\n",
    "\n",
    "# TODO: Create mobile-optimized model\n",
    "# HINT: Which device should you typically set it to at initialization, CPU or GPU?\n",
    "mobile_model =  # Add your code here\n",
    "\n",
    "print(\"Mobile-Optimized CNN created:\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in mobile_model.parameters()):,}\")\n",
    "print(f\"Model size: {sum(p.numel() * p.element_size() for p in mobile_model.parameters()) / 1024**2:.1f} MB\")\n",
    "\n",
    "# Analyze mobile model efficiency\n",
    "mobile_stats = analyze_model_efficiency(mobile_model.to(\"cpu\"), \"Mobile CNN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Some typical platform-specific architectural considerations:**\n",
    ">\n",
    "> - **iOS Neural Engine**: \n",
    ">   - **Preferred**: Standard conv sizes (3×3, 1×1), channel counts divisible by 16, ReLU/ReLU6\n",
    ">   - **Avoid**: Custom activations, dynamic shapes, irregular tensor sizes\n",
    ">   - **Optimization**: Use Core ML tools, prefer Apple's optimized operation set\n",
    ">   <br><br>\n",
    "> - **Android NNAPI/GPU**:\n",
    ">   - **Preferred**: Depthwise convolutions, ReLU6, quantized operations, parallel-friendly patterns  \n",
    ">   - **Avoid**: Large kernels (5×5+), complex branching, FP32-only operations\n",
    ">   - **Optimization**: Design for GPU parallel execution, test across device diversity\n",
    ">   <br><br>\n",
    "> - **Qualcomm Hexagon DSP**:\n",
    ">   - **Preferred**: INT8 quantized operations, fixed-point math, regular memory patterns\n",
    ">   - **Avoid**: FP32 operations, variable tensor sizes, complex control flow\n",
    ">   - **Optimization**: Symmetric quantization schemes, Hexagon-specific delegates\n",
    ">\n",
    "> _Key insight_: Architecture choices that work well on one platform may underperform on others. Mobile-first design targets the common intersection of efficient patterns across all platforms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare architectures and assess mobile readiness\n",
    "\n",
    "Let's analyze the architectural transformation results and visualize the efficiency improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_comparison(baseline_stats, mobile_stats, device):\n",
    "    \"\"\"Create detailed comparison between baseline and mobile architectures\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ARCHITECTURAL TRANSFORMATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate improvement factors\n",
    "    param_reduction = (baseline_stats['params'] - mobile_stats['params']) / baseline_stats['params'] * 100\n",
    "    size_reduction = (baseline_stats['size_mb'] - mobile_stats['size_mb']) / baseline_stats['size_mb'] * 100\n",
    "    memory_reduction = (baseline_stats['memory_mb'] - mobile_stats['memory_mb']) / baseline_stats['memory_mb'] * 100 if baseline_stats['memory_mb'] > 0 else 0\n",
    "    flop_reduction = (baseline_stats['flops_g'] - mobile_stats['flops_g']) / baseline_stats['flops_g'] * 100\n",
    "    speed_improvement = baseline_stats['cpu_time_ms'] / mobile_stats['cpu_time_ms']\n",
    "    \n",
    "    print(f\"Parameter reduction: {param_reduction:.1f}% ({baseline_stats['params']:,} → {mobile_stats['params']:,})\")\n",
    "    print(f\"Model size reduction: {size_reduction:.1f}% ({baseline_stats['size_mb']:.1f}MB → {mobile_stats['size_mb']:.1f}MB)\")\n",
    "    if device==\"cuda\":\n",
    "        print(f\"Memory reduction: {memory_reduction:.1f}% ({baseline_stats['memory_mb']:.1f}MB → {mobile_stats['memory_mb']:.1f}MB)\")\n",
    "    print(f\"FLOP reduction: {flop_reduction:.1f}% ({baseline_stats['flops_g']:.2f}G → {mobile_stats['flops_g']:.2f}G)\")\n",
    "    print(f\"Speed improvement: {speed_improvement:.1f}x ({baseline_stats['cpu_time_ms']:.1f}ms → {mobile_stats['cpu_time_ms']:.1f}ms)\")\n",
    "    \n",
    "    print(f\"\\nMobile deployment readiness:\")\n",
    "    print(f\"  Baseline: {'✓ Ready' if baseline_stats['mobile_ready'] else '✗ Not Ready'}\")\n",
    "    print(f\"  Mobile CNN: {'✓ Ready' if mobile_stats['mobile_ready'] else '✗ Not Ready'}\")\n",
    "    \n",
    "    return {\n",
    "        'param_reduction_pct': param_reduction,\n",
    "        'size_reduction_pct': size_reduction,\n",
    "        'memory_reduction_pct': memory_reduction,\n",
    "        'flop_reduction_pct': flop_reduction,\n",
    "        'speed_improvement': speed_improvement\n",
    "    }\n",
    "\n",
    "def plot_results(baseline, mobile, stats, output_dir):\n",
    "    \"\"\"Create visualization of comparison results.\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    models = ['Baseline CNN', 'Mobile CNN']\n",
    "    colors = ['#e74c3c', '#2ecc71']\n",
    "\n",
    "    def _add_bar_labels(ax, bars, values, fmt=\"{:.1f}\", offset=1, bold=True):\n",
    "        \"\"\"Helper to add labels on top of bars.\"\"\"\n",
    "        for bar, val in zip(bars, values):\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                bar.get_height() + offset,\n",
    "                fmt.format(val),\n",
    "                ha='center', va='bottom',\n",
    "                fontweight='bold' if bold else 'normal'\n",
    "            )\n",
    "\n",
    "    # 1. Model Size\n",
    "    sizes = [baseline['size_mb'], mobile['size_mb']]\n",
    "    bars = ax1.bar(models, sizes, color=colors, alpha=0.7)\n",
    "    ax1.set_ylabel('Model Size (MB)')\n",
    "    ax1.set_title('Model Size Comparison')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    _add_bar_labels(ax1, bars, sizes, fmt=\"{:.1f}MB\")\n",
    "\n",
    "    # 2. Parameter Count\n",
    "    params = [baseline['params'] / 1e6, mobile['params'] / 1e6]\n",
    "    bars = ax2.bar(models, params, color=colors, alpha=0.7)\n",
    "    ax2.set_ylabel('Parameters (Millions)')\n",
    "    ax2.set_title('Parameter Count Comparison')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    _add_bar_labels(ax2, bars, params, fmt=\"{:.1f}M\", offset=0.2)\n",
    "\n",
    "    # 3. Inference Time\n",
    "    times = [baseline['cpu_time_ms'], mobile['cpu_time_ms']]\n",
    "    bars = ax3.bar(models, times, color=colors, alpha=0.7)\n",
    "    ax3.set_ylabel('Inference Time (ms)')\n",
    "    ax3.set_title('CPU Inference Time (Mobile Simulation)')\n",
    "    ax3.axhline(y=500, color='red', linestyle='--', alpha=0.7, label='Mobile Target (<500ms)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    _add_bar_labels(ax3, bars, times, fmt=\"{:.0f}ms\", offset=10)\n",
    "\n",
    "    # 4. Optimization Impact\n",
    "    categories = [\n",
    "        'Parameter\\nReduction', 'Size\\nReduction',\n",
    "        'Memory\\nReduction', 'FLOP\\nReduction', 'Speed\\nImprovement'\n",
    "    ]\n",
    "    values = [\n",
    "        stats['param_reduction_pct'],\n",
    "        stats['size_reduction_pct'],\n",
    "        stats['memory_reduction_pct'],\n",
    "        stats['flop_reduction_pct'],\n",
    "        stats['speed_improvement'] * 20  # scaled\n",
    "    ]\n",
    "    bars = ax4.bar(categories, values, color='#3498db', alpha=0.7)\n",
    "    ax4.set_ylabel('Improvement (%)')\n",
    "    ax4.set_title('Mobile Optimization Impact')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add values (handle speed separately)\n",
    "    for bar, cat, val in zip(bars, categories, values):\n",
    "        if cat == 'Speed\\nImprovement':\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                     f\"{stats['speed_improvement']:.1f}x\",\n",
    "                     ha='center', va='bottom', fontsize=9)\n",
    "        else:\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                     f\"{val:.0f}%\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Plot and save\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(output_dir, 'mobile_optimization_results.png')\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nVisualization saved to: {out_path}\")\n",
    "\n",
    "# Get comparison results\n",
    "comparison_results = create_model_comparison(baseline_stats, mobile_stats, device=\"cpu\")\n",
    "plot_results(baseline_stats, mobile_stats, comparison_results, output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO: Analyze metric-by-metric transformation results**: Through systematic application of mobile-first design patterns, you should see dramatic efficiency improvements while maintaining the model's core functionality. \n",
    "> \n",
    "> Examine your architectural transformation results and identify patterns:\n",
    "> \n",
    "> **1\\.** Which efficiency improvement surprised you most and why?\n",
    ">   \n",
    "> _Your answer:_ ________________________________________________________________\n",
    "> \n",
    "> **2\\.** Compare the magnitude of different improvements:\n",
    ">   - Which metrics showed the largest reductions/improvements?\n",
    ">   - Do the relative improvements match your expectations?\n",
    ">   - What might explain the relationship between parameter reduction and memory reduction?\n",
    "> \n",
    "> _Your analysis:_ ______________________________________________________________\n",
    "> \n",
    "> **3\\.** What's the difference between \"model size\" and \"runtime memory\" during mobile inference?\n",
    "> \n",
    "> *Hint: Consider what happens when your model processes an image...*\n",
    "> \n",
    "> _Your explanation:_ ___________________________________________________________\n",
    "> \n",
    "> **4\\.** Why might reducing model parameters not always solve mobile memory constraints?\n",
    "> \n",
    "> _Your reasoning:_ _____________________________________________________________\n",
    "> \n",
    "> _**IMPORTANT: Missing accuracy retention metric!**_ This exercise focused on architectural efficiency but didn't measure accuracy. In real mobile deployment, you must validate that optimization doesn't break model performance by finding architectures that maximize both effiency and accuracy simultaneously. Why not take accuracy validation as a bonus challenge?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you have transformed a standard CNN into a mobile-optimized architecture using systematic hardware-aware design patterns, then analyzed the efficiency trade-offs that determine real-world deployment success.\n",
    "\n",
    "Key architectural principles demonstrated include:\n",
    "\n",
    "- _**Hardware-software co-design**_: Architecture choices directly determine mobile deployment viability - post-hoc optimization alone cannot bridge fundamental design gaps\n",
    "\n",
    "- _**Multi-dimensional efficiency**_: Mobile optimization requires balancing parameters, memory, computation, and accuracy simultaneously, not just minimizing one metric\n",
    "\n",
    "- _**Platform-aware patterns**_: Depthwise separable convolutions, inverted residuals, and bounded activations align with mobile hardware strengths while avoiding bandwidth bottlenecks\n",
    "\n",
    "These design principles enable real-time visual AI experiences while respecting the power, memory, and computational limits of mobile devices.\n",
    "\n",
    "Through your analysis, you've uncovered why mobile deployment requires understanding the relationship between model architecture and runtime constraints. The efficiency patterns you observed——and any surprises in the metric comparisons——reflect fundamental hardware realities that shape production mobile AI systems.\n",
    "\n",
    "##### **Next mobile architecture challenges to explore:**\n",
    "\n",
    "- **Batch size optimization**: Compare mobile architecture performance with different batch sizes for deployment scenarios\n",
    "\n",
    "- **Dynamic inference**: Implement early-exit architectures for variable computational budgets\n",
    "\n",
    "- **Multi-resolution deployment**: Design single architectures that adapt to different input sizes based on device capabilities  \n",
    "\n",
    "- **Hardware-specific tuning**: Optimize channel counts and expand ratios for iOS Neural Engine vs Android NNAPI\n",
    "\n",
    "- **Advanced mobile patterns**: Implement squeeze-and-excitation blocks, neural architecture search for mobile constraints\n",
    "\n",
    "- **Memory-bandwidth co-design**: Optimize activation memory patterns for mobile memory hierarchies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
