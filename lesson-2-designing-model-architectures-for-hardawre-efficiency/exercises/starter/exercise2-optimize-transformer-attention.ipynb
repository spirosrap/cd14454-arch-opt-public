{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Optimize transformer attention for GPU throughput\n",
    "\n",
    "You have learned about hardware-aware architecture design and efficient patterns in theory. Now it's time to apply these concepts by implementing and comparing different attention architectures to maximize GPU throughput.\n",
    "\n",
    "> **Overview**: A transformer model is struggling with throughput bottlenecks during peak traffic. While the model achieves excellent accuracy, the attention mechanism is not effectively using the memory bandwidth and under-utilizing the T4's parallel processing capabilities.\n",
    "> \n",
    "> **Scenario**: You work for a social media platform that processes millions of posts for content moderation in real-time. Your current BERT-base model processes only 350 samples/second on T4 GPUs, but you need 1000+ samples/second to handle peak traffic loads. The DevOps team reports that GPU utilization is surprisingly low (15-25%), suggesting the architecture isn't designed for efficient parallel processing.\n",
    "> \n",
    "> **Goal**: Learn to design and implement memory-efficient attention patterns that leverage GPU parallelism, comparing Multi-Head Attention (MHA) vs Multi-Query Attention (MQA) architectures, and apply hardware-specific optimizations like mixed precision to maximize T4 throughput.\n",
    "> \n",
    "> **Tools**: pytorch, transformers, amp (mixed precision), datasets\n",
    "> \n",
    "> **Estimated Time**: 15 minutes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup \n",
    "\n",
    "Let's establish our baseline environment and verify T4 capabilities for attention optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out the cell block again and restart the notebook\n",
    "# ! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "# Create output directory, if it does not exists\n",
    "output_dir = \"assets/exercise2\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check T4 GPU capabilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    \n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "    print(f\"Total Memory: {gpu_properties.total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Multiprocessors: {gpu_properties.multi_processor_count}\")\n",
    "    print(f\"Memory Bandwidth: ~320 GB/s (theoretical)\")\n",
    "    \n",
    "    # Check for Tensor Core support (Compute Capability >= 7.0)\n",
    "    tensor_cores_available = gpu_properties.major >= 7\n",
    "    print(f\"Tensor Core Support: {'✓ Available' if tensor_cores_available else '✗ Not Available'}\")\n",
    "    \n",
    "    if tensor_cores_available:\n",
    "        print(\"  → Mixed precision (FP16) will show significant speedup\")\n",
    "        \n",
    "    print(\"Setup complete!\")\n",
    "else:\n",
    "    print(\"CUDA not available - exercise requires GPU\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **T4 hardware context**: T4 GPUs feature 2,560 CUDA cores, 320 tensor cores, and 320 GB/s memory bandwidth. \n",
    "> \n",
    "> The memory bandwidth often becomes the bottleneck for attention mechanisms, making architectural choices that reduce memory traffic crucial for performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Load and prepare the dataset\n",
    "\n",
    "For this exercise, we use the [tweet_eval](https://huggingface.co/datasets/cardiffnlp/tweet_eval) dataset, which classifies tweets with one or more of these seven different labels: irony, hate, offensive, stance, emoji, emotion, and sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TweetEval sentiment dataset\n",
    "print(\"Loading social media content for moderation...\")\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "# Initialize tokenizer (BERT-base)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def prepare_tweet_data(dataset_split, max_length=128, max_samples=2500):\n",
    "    \"\"\"Prepare and tokenize tweet data for transformer processing\"\"\"\n",
    "    \n",
    "    texts = dataset_split['text'][:max_samples]\n",
    "    labels = dataset_split['label'][:max_samples]\n",
    "    \n",
    "    # Tokenize with attention masks\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encoded['input_ids'],\n",
    "        'attention_mask': encoded['attention_mask'], \n",
    "        'labels': torch.tensor(labels, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# Prepare validation set for benchmarking\n",
    "val_data = prepare_tweet_data(dataset['validation'])\n",
    "\n",
    "print(f\"Tweet Eval dataset loaded:\")\n",
    "print(f\"Input shape: {val_data['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {val_data['attention_mask'].shape}\")\n",
    "print(f\"Labels shape: {val_data['labels'].shape}\")\n",
    "\n",
    "# Dataset analysis\n",
    "non_padded_lengths = (val_data['attention_mask'].sum(dim=1)).float()\n",
    "print(f\"Average content length: {non_padded_lengths.mean():.1f} tokens\")\n",
    "print(f\"Label distribution: {torch.bincount(val_data['labels']).float() / len(val_data['labels'])}\")\n",
    "print(f\"Classes: 0=negative, 1=neutral, 2=positive\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dataset optimization insight**: The 29.9 average token length vs 128 max sequence creates significant padding (77% of positions are empty).\n",
    ">\n",
    "> This means attention matrices are computing over mostly padding tokens, wasting GPU memory bandwidth. In production, dynamic sequence length batching or attention masking optimizations could provide additional 2-3x efficiency gains by avoiding computation on padding tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understand attention memory patterns\n",
    "\n",
    "Before implementing optimizations, let's understand how different attention architectures (MHA vs MQA) use GPU memory and bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mha_memory_stats(hidden_size, num_heads, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Calculate memory requirements for Multi-Head Attention (MHA) in megabytes.\n",
    "    \n",
    "    Args:\n",
    "        hidden_size (int): Hidden dimension size (e.g., 768 for BERT-base)\n",
    "        num_heads (int): Number of attention heads (e.g., 12 for BERT-base)\n",
    "        batch_size (int): Batch size for inference\n",
    "        seq_length (int): Sequence length (e.g., 128 tokens)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Memory statistics containing:\n",
    "            - qkv_memory_mb (float): Memory for Q, K, V projections in MB\n",
    "            - attention_memory_mb (float): Memory for attention matrices in MB  \n",
    "            - total_memory_mb (float): Total memory requirement in MB\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Calculate memory requirements for Multi-Head Attention (MQA) in bytes\n",
    "    # Hint: Q, K, and V all have hidden_size\n",
    "    # Reference: https://arxiv.org/abs/2006.16362 (Multi-Head Attention paper)\n",
    "    qkv_memory =  #  Add your code here\n",
    "    attention_memory =  # Add your code here\n",
    "    \n",
    "    total_memory_mb = (qkv_memory + attention_memory) / (1024 ** 2)  # The division transforms from bytes to MB\n",
    "\n",
    "    return {\n",
    "        'qkv_memory_mb': qkv_memory / (1024 ** 2),\n",
    "        'attention_memory_mb': attention_memory / (1024 ** 2),\n",
    "        'total_memory_mb': total_memory_mb\n",
    "    }\n",
    "    \n",
    "    \n",
    "def get_mqa_memory_stats(hidden_size, num_heads, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Calculate memory requirements for Multi-Query Attention (MQA) in megabytes.\n",
    "    \n",
    "    Args:\n",
    "        hidden_size (int): Hidden dimension size (e.g., 768 for BERT-base)\n",
    "        num_heads (int): Number of attention heads (e.g., 12 for BERT-base)\n",
    "        batch_size (int): Batch size for inference\n",
    "        seq_length (int): Sequence length (e.g., 128 tokens)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Memory statistics containing:\n",
    "            - qkv_memory_mb (float): Memory for Q, K, V projections in MB\n",
    "            - attention_memory_mb (float): Memory for attention matrices in MB  \n",
    "            - total_memory_mb (float): Total memory requirement in MB\n",
    "            \n",
    "    Note:\n",
    "        MQA reduces memory by sharing K,V matrices across heads while keeping\n",
    "        separate Q matrices per head, resulting in significant bandwidth savings.\n",
    "    \"\"\"\n",
    "    head_dim = hidden_size // num_heads\n",
    "    \n",
    "    # TODO: Calculate memory requirements for Multi-Query Attention (MQA) in bytes\n",
    "    # Hint: Think about what \"shared K,V across heads\" means for projection sizes.\n",
    "    # If K,V are shared, they only need single head_dim size instead of full hidden_size.\n",
    "    # Consider: Q (still multi-head) + K (shared, single head) + V (shared, single head)\n",
    "    # Reference: https://arxiv.org/abs/1911.02150 (Multi-Query Attention paper)\n",
    "    \n",
    "    q_memory =  # Add your code here\n",
    "    k_memory =  # Add your code here\n",
    "    v_memory =  # Add your code here\n",
    "    qkv_memory =  # Add your code here\n",
    "    \n",
    "    attention_memory =  # Add your code here (same as MHA)\n",
    "    \n",
    "    total_memory_mb = (qkv_memory + attention_memory) / (1024 ** 2)  # The division transforms from bytes to MB\n",
    "    \n",
    "    return {\n",
    "        'qkv_memory_mb': qkv_memory / (1024 ** 2),\n",
    "        'attention_memory_mb': attention_memory / (1024 ** 2),\n",
    "        'total_memory_mb': total_memory_mb\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_memory_patterns():\n",
    "    \"\"\"\n",
    "    Analyze memory usage patterns for different attention architectures\n",
    "    to understand why some patterns are more GPU-friendly than others.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration for analysis\n",
    "    batch_size = 32\n",
    "    seq_len = 128\n",
    "    hidden_size = 768\n",
    "    num_heads = 12\n",
    "    head_dim = hidden_size // num_heads  # 64\n",
    "    \n",
    "    print(\"=== ATTENTION MEMORY PATTERN ANALYSIS ===\")\n",
    "    print(f\"Configuration: batch={batch_size}, seq_len={seq_len}, hidden={hidden_size}, heads={num_heads}\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate memory requirements for Multi-Head Attention (MHA)\n",
    "    mha_memory_stats = get_mha_memory_stats(hidden_size, num_heads, batch_size, seq_len)\n",
    "    \n",
    "    print(\"MULTI-HEAD ATTENTION (MHA):\")\n",
    "    print(f\"Q, K, V tensors: {mha_memory_stats['qkv_memory_mb']:.1f} MB\")\n",
    "    print(f\"Attention matrices: {mha_memory_stats['attention_memory_mb']:.1f} MB\") \n",
    "    print(f\"Total memory: {mha_memory_stats['total_memory_mb']:.1f} MB\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate memory requirements for Multi-Query Attention (MQA)\n",
    "    mqa_memory_stats = get_mqa_memory_stats(hidden_size, num_heads, batch_size, seq_len)\n",
    "    \n",
    "    print(\"MULTI-QUERY ATTENTION (MQA):\")\n",
    "    print(f\"Q, K, V tensors: {mqa_memory_stats['qkv_memory_mb']:.1f} MB\")\n",
    "    print(f\"Attention matrices: {mqa_memory_stats['attention_memory_mb']:.1f} MB\") \n",
    "    print(f\"Total memory: {mqa_memory_stats['total_memory_mb']:.1f} MB\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate efficiency improvements\n",
    "    memory_reduction = (mha_memory_stats[\"total_memory_mb\"] - mqa_memory_stats[\"total_memory_mb\"]) / mha_memory_stats[\"total_memory_mb\"] * 100\n",
    "    bandwidth_reduction = (mha_memory_stats[\"qkv_memory_mb\"] - mqa_memory_stats[\"qkv_memory_mb\"]) / mha_memory_stats[\"qkv_memory_mb\"] * 100\n",
    "    \n",
    "    print(\"=== EFFICIENCY COMPARISON ===\")\n",
    "    print(f\"Memory reduction: {memory_reduction:.1f}%\")\n",
    "    print(f\"Memory bandwidth reduction: {bandwidth_reduction:.1f}%\")\n",
    "    print(f\"Parameter reduction: {(hidden_size * (hidden_size // num_heads) * 2) / 1024**2:.1f} MB\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate hardware impact on T4\n",
    "    t4_bandwidth_gbps = 320\n",
    "    inference_rate = 1000  # samples/sec target\n",
    "    \n",
    "    mha_bandwidth_usage = mha_memory_stats[\"qkv_memory_mb\"] * inference_rate / 1024  # GB/s\n",
    "    mqa_bandwidth_usage = mqa_memory_stats[\"qkv_memory_mb\"] * inference_rate / 1024  # GB/s\n",
    "    \n",
    "    print(\"=== T4 HARDWARE IMPACT ===\")\n",
    "    print(f\"MHA memory bandwidth need: {mha_bandwidth_usage:.1f} GB/s @ {inference_rate} samples/sec\")\n",
    "    print(f\"MQA memory bandwidth need: {mqa_bandwidth_usage:.1f} GB/s @ {inference_rate} samples/sec\")\n",
    "    print(f\"T4 memory bandwidth: {t4_bandwidth_gbps} GB/s\")\n",
    "    print(f\"MHA bandwidth utilization: {mha_bandwidth_usage/t4_bandwidth_gbps*100:.1f}%\")\n",
    "    print(f\"MQA bandwidth utilization: {mqa_bandwidth_usage/t4_bandwidth_gbps*100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'mha_memory_mb': mha_memory_stats['total_memory_mb'],\n",
    "        'mqa_memory_mb': mqa_memory_stats['total_memory_mb'],\n",
    "        'memory_reduction_percent': memory_reduction,\n",
    "        'bandwidth_reduction_percent': bandwidth_reduction\n",
    "    }\n",
    "\n",
    "# Run analysis\n",
    "memory_analysis = analyze_attention_memory_patterns()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Key MHA vs MQA insight**: Even though both architectures compute the same attention outputs, MQA shares the Key (K) and Value (V) projections across all heads. \n",
    "> \n",
    "> This dramatically reduces the memory needed for storing Q, K, V tensors to reduce the bandwidth bottleneck, without changing the attention matrices themselves."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement standard multi-head attention (MHA)\n",
    "Let's implement standard multi-head attention as our baseline to understand current memory and computational patterns.\n",
    "\n",
    "IMPORTANT: We are note implementing the feed-forward network like in BERT because we want to isolate and study the attention mechanism only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Multi-Head Attention implementation\n",
    "    Each head gets separate Q, K, V projections (memory intensive)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "        # TODO: Create separate projection layers for Q, K, V \n",
    "        # Hint: Each projection maps from hidden_size to hidden_size using a linear layer\n",
    "        # This creates separate matrices for each head, maximizing memory usage\n",
    "        self.query_proj =  # Add your code here\n",
    "        self.key_proj =  # Add your code here\n",
    "        self.value_proj =  # Add your code here\n",
    "\n",
    "        self.output_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_length, _ = hidden_states.shape\n",
    "        residual = hidden_states\n",
    "        \n",
    "        # Compute Q, K, V projections for all heads\n",
    "        query = self.query_proj(hidden_states)\n",
    "        key = self.key_proj(hidden_states)  \n",
    "        value = self.value_proj(hidden_states)\n",
    "        \n",
    "        # TODO: Reshape for multi-head attention\n",
    "        # Hint: Multi-head attention processes each head independently. You need to separate the hidden_size into num_heads × head_dim, then transpose heads to a separate dimension for parallel processing.\n",
    "        # PyTorch tensor manipulation recap: https://discuss.pytorch.org/t/whats-the-difference-between-torch-reshape-vs-torch-view/159172\n",
    "        query =  # Add your code here\n",
    "        key =  # Add your code here\n",
    "        value =  # Add your code here\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) * self.scale\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            # Expand mask for all heads\n",
    "            expanded_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "            attention_scores = attention_scores.masked_fill(expanded_mask == 0, float('-inf'))\n",
    "        \n",
    "        # Compute attention weights and apply to values\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        context = torch.matmul(attention_probs, value)\n",
    "        \n",
    "        # TODO: Reshape back to original dimensions\n",
    "        # Hint: Reverse the multi-head reshaping - merge heads back into the hidden dimension. Remember to ensure memory layout is contiguous for efficient operations.\n",
    "        # Contiguous tensors explanation: https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html\n",
    "        context =  # Add your code here\n",
    "        context =  # Add your code here\n",
    "        \n",
    "        # Final output projection\n",
    "        output = self.output_proj(context)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerWithMHA(nn.Module):\n",
    "    \"\"\"Complete transformer model using MHA\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30522, hidden_size=256, num_heads=8, num_layers=4, max_seq_length=128):\n",
    "        super(TransformerWithMHA, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Model components\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, hidden_size)\n",
    "        \n",
    "        \n",
    "        # Multi-head attention layers\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            MultiHeadAttention(hidden_size, num_heads) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head for content moderation\n",
    "        self.pooler = nn.Linear(hidden_size, hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, 3)  # 3 classes: safe, toxic, harmful\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        \n",
    "        # Create position embeddings\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
    "        \n",
    "        # Input embeddings\n",
    "        hidden_states = self.embeddings(input_ids) + self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Pass through attention layers\n",
    "        for attention_layer, layer_norm in zip(self.attention_layers, self.layer_norms):\n",
    "            # Apply attention with residual connection\n",
    "            attention_output = attention_layer(hidden_states, attention_mask)\n",
    "            hidden_states = layer_norm(hidden_states + attention_output)\n",
    "        \n",
    "        # Classification using [CLS] token (first token)\n",
    "        pooled_output = self.pooler(hidden_states[:, 0])\n",
    "        logits = self.classifier(torch.tanh(pooled_output))\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# TODO: Initialize the MHA model with architecture parameters aimed at a similar model size to Bert-base, without the feed-forward network (~=52M parameters)\n",
    "# Hint: Remember: hidden_size % num_heads == 0. \n",
    "# Consider that larger models will show bigger memory bandwidth differences.\n",
    "# PyTorch device selection reference: https://discuss.pytorch.org/t/is-there-any-difference-between-x-to-cuda-vs-x-cuda-which-one-should-i-use/20137\n",
    "mha_model =  # Add your code here\n",
    "\n",
    "print(\"Multi-Head Attention model created:\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in mha_model.parameters()):,}\")\n",
    "print(f\"Model size: {sum(p.numel() * p.element_size() for p in mha_model.parameters()) / 1024**2:.1f} MB\")\n",
    "\n",
    "# Calculate memory requirements for different batch sizes\n",
    "print(\"\\n=== MHA MEMORY REQUIREMENTS ===\")\n",
    "attention_layer = mha_model.attention_layers[0]\n",
    "for batch_size in [16, 32, 64, 128]:\n",
    "    memory_stats = get_mha_memory_stats(attention_layer.hidden_size, attention_layer.num_heads, batch_size, 128)\n",
    "    print(f\"Batch size {batch_size}: {memory_stats['total_memory_mb']:.1f} MB \"\n",
    "          f\"(QKV: {memory_stats['qkv_memory_mb']:.1f} MB, \"\n",
    "          f\"Attention: {memory_stats['attention_memory_mb']:.1f} MB)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **MHA memory analysis**: \n",
    "> \n",
    "> - Memory for Q, K, V tensors scales linearly with both batch size and sequence length. \n",
    "> - Attention matrices scale linearly with batch size but quadratically with sequence length. \n",
    "> \n",
    "> Each head learns independent attention patterns, so full projection matrices are required, which can make memory bandwidth a bottleneck on GPU hardware."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement Multi-Query Attention (MQA)\n",
    "Now let's implement MQA and compare its characteristics with standard MHA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"Multi-Query Attention implementation with shared key and value projections\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # TODO: Create separate projection layers for Q, K, V \n",
    "        # Hint: In MQA, each head has its own query, but all heads share the same key and value.\n",
    "        # What model self parameter defines a single head?\n",
    "        self.query_proj =  # Add your code here\n",
    "        self.key_proj =  # Add your code here\n",
    "        self.value_proj =  # Add your code here\n",
    "\n",
    "        self.output = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_length, _ = hidden_states.shape\n",
    "        \n",
    "        # Compute Q, K, V projections for all heads\n",
    "        query = self.query_proj(hidden_states)\n",
    "        key = self.key_proj(hidden_states)  \n",
    "        value = self.value_proj(hidden_states)\n",
    "        \n",
    "        # TODO: Reshape query for multi-head processing\n",
    "        # Hint: Only query needs multi-head reshaping, key and value remain single-head\n",
    "        query =   # Add your code here\n",
    "        # Key and value stay as [batch_size, seq_length, head_dim] - shared across heads\n",
    "        \n",
    "        # TODO: Expand key and value to work with all query heads\n",
    "        # Hint: Add head dimension to key and value tensors for broadcasting\n",
    "        key =  # Add your code here\n",
    "        value =  # Add your code here\n",
    "        \n",
    "        # Compute attention scores (key broadcasts across all heads)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) * self.scale\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            expanded_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "            attention_scores = attention_scores.masked_fill(expanded_mask == 0, float('-inf'))\n",
    "        \n",
    "        # Compute attention weights and apply to values\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        # Apply attention to values (value broadcasts across all heads)\n",
    "        context = torch.matmul(attention_probs, value)\n",
    "\n",
    "        # TODO: Reshape back to original dimensions (same as MHA)\n",
    "        context =  # Add your code here\n",
    "        context =  # Add your code here\n",
    "        \n",
    "        # Final output projection\n",
    "        output = self.output(context)\n",
    "        return output\n",
    "\n",
    "    def get_memory_stats(self, batch_size, seq_length):\n",
    "        \"\"\"Calculate memory requirements for MQA\"\"\"\n",
    "        # Memory for Q tensor (all heads)\n",
    "        query_memory = batch_size * seq_length * self.hidden_size * 4\n",
    "        \n",
    "        # Memory for K, V tensors (single head each, shared)\n",
    "        kv_memory = 2 * batch_size * seq_length * self.head_dim * 4\n",
    "        \n",
    "        # Memory for attention scores (same as MHA)\n",
    "        attention_memory = batch_size * self.num_heads * seq_length * seq_length * 4\n",
    "        \n",
    "        total_memory_mb = (query_memory + kv_memory + attention_memory) / (1024 ** 2)\n",
    "        return {\n",
    "            'query_memory_mb': query_memory / (1024 ** 2),\n",
    "            'kv_memory_mb': kv_memory / (1024 ** 2),\n",
    "            'attention_memory_mb': attention_memory / (1024 ** 2),\n",
    "            'total_memory_mb': total_memory_mb\n",
    "        }\n",
    "\n",
    "\n",
    "class MQAContentModerationTransformer(nn.Module):\n",
    "    \"\"\"Content moderation transformer using Multi-Query Attention\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30522, hidden_size=256, num_heads=8, num_layers=4, max_seq_length=128):\n",
    "        super(MQAContentModerationTransformer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Model components (same as MHA version)\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, hidden_size)\n",
    "        \n",
    "        # Multi-query attention layers\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            MultiQueryAttention(hidden_size, num_heads) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head (same as MHA version)\n",
    "        self.pooler = nn.Linear(hidden_size, hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        \n",
    "        # Create position embeddings\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
    "        \n",
    "        # Input embeddings\n",
    "        hidden_states = self.embeddings(input_ids) + self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Pass through attention layers\n",
    "        for attention_layer, layer_norm in zip(self.attention_layers, self.layer_norms):\n",
    "            attention_output = attention_layer(hidden_states, attention_mask)\n",
    "            hidden_states = layer_norm(hidden_states + attention_output)\n",
    "        \n",
    "        # Classification\n",
    "        pooled_output = self.pooler(hidden_states[:, 0])\n",
    "        logits = self.classifier(torch.tanh(pooled_output))\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "# TODO: Create MQA model and move to GPU\n",
    "# Hint: Use identical parameters to MHA model to ensure fair comparison of architectural differences only.\n",
    "mqa_model =  # Add your code here\n",
    "\n",
    "print(\"Multi-Query Attention model created:\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in mqa_model.parameters()):,}\")\n",
    "print(f\"Model size: {sum(p.numel() * p.element_size() for p in mqa_model.parameters()) / 1024**2:.1f} MB\")\n",
    "\n",
    "# Calculate memory requirements for different batch sizes\n",
    "print(\"\\n=== MQA MEMORY REQUIREMENTS ===\")\n",
    "attention_layer = mqa_model.attention_layers[0]\n",
    "for batch_size in [16, 32, 64, 128]:\n",
    "    memory_stats = get_mqa_memory_stats(attention_layer.hidden_size, attention_layer.num_heads, batch_size, 128)\n",
    "    print(f\"Batch size {batch_size}: {memory_stats['total_memory_mb']:.1f} MB \"\n",
    "          f\"(QKV: {memory_stats['qkv_memory_mb']:.1f} MB, \"\n",
    "          f\"Attention: {memory_stats['attention_memory_mb']:.1f} MB)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **MQA memory analysis**: Sharing K/V projections in MQA reduces both memory and memory bandwidth requirements, while maintaining the same attention computations. This makes MQA significantly more GPU-friendly than MHA, especially for larger batch sizes or longer sequences. \n",
    "> \n",
    "> Notice how the scaling patterns stay the same between MQA and MHA, but with fewer parameters!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Benchmark MHA vs MQA performance\n",
    "Let's measure the throughput improvements from Multi-Query Attention architecture.\n",
    "\n",
    "IMPORTANT: T4 GPUs have specialized Tensor Cores that accelerate FP16 operations, so we benchmark both with and without mixed precision support to study this enhancement too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for different batch sizes\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "dataloaders = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    dataset_tensors = TensorDataset(\n",
    "        val_data['input_ids'], \n",
    "        val_data['attention_mask'], \n",
    "        val_data['labels']\n",
    "    )\n",
    "    dataloaders[batch_size] = DataLoader(\n",
    "        dataset_tensors, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "print(f\"DataLoaders created for batch sizes: {batch_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark function for fair comparison\n",
    "def benchmark_model(model, dataloader, num_batches=20, use_mixed_precision=True):\n",
    "    \"\"\"Measure model throughput and memory usage\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    times = []\n",
    "    memory_stats = []\n",
    "    \n",
    "    # Warm up the GPU\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids, attention_mask, _) in enumerate(dataloader):\n",
    "            if i >= 3:  # 3 warmup iterations\n",
    "                break\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            _ = model(input_ids, attention_mask)\n",
    "    \n",
    "    # Benchmark inference\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids, attention_mask, _) in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            # Clear GPU cache\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            baseline_memory = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "            \n",
    "            # Synchronize and start timing\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            # TODO: Implement conditional mixed precision inference\n",
    "            # Hint: Use torch.cuda.amp.autocast() context manager for automatic FP16/FP32 selection\n",
    "            # This enables Tensor Cores on compatible hardware (T4, V100, A100, etc.)\n",
    "            # Documentation: https://pytorch.org/docs/stable/amp.html\n",
    "\n",
    "            # Add your code here\n",
    "\n",
    "            # Memory measurements\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "            memory_used = peak_memory - baseline_memory\n",
    "\n",
    "            # Synchronize and end timing\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            memory_stats.append(memory_used)\n",
    "\n",
    "    return times, memory_stats\n",
    "\n",
    "def comprehensive_benchmark():\n",
    "    \"\"\"Compare all model configurations across batch sizes\"\"\"\n",
    "    \n",
    "    configurations = [\n",
    "        (\"MHA FP32\", mha_model, False),\n",
    "        (\"MHA FP16\", mha_model, True), \n",
    "        (\"MQA FP32\", mqa_model, False),\n",
    "        (\"MQA FP16\", mqa_model, True)\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== COMPREHENSIVE THROUGHPUT BENCHMARK ===\")\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nBatch size {batch_size}:\")\n",
    "        results[batch_size] = {}\n",
    "        \n",
    "        for config_name, model, use_amp in configurations:\n",
    "            \n",
    "            times, memory_stats = benchmark_model(model, dataloaders[batch_size], use_mixed_precision=use_amp)\n",
    "            \n",
    "            avg_time = np.mean(times)\n",
    "            avg_memory = np.mean(memory_stats)\n",
    "            throughput = batch_size / avg_time  # samples/sec\n",
    "            \n",
    "            results[batch_size][config_name] = {\n",
    "                'avg_time': avg_time,\n",
    "                'throughput': throughput,\n",
    "                'latency_ms': avg_time * 1000,\n",
    "                'avg_memory_mb': avg_memory\n",
    "            }\n",
    "            \n",
    "            print(f\"  {config_name}: {throughput:.1f} samples/sec ({avg_time*1000:.1f}ms) | {avg_memory:.1f} MB\")\n",
    "    \n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "benchmark_results = comprehensive_benchmark()\n",
    "\n",
    "# Calculate improvement factors\n",
    "print(\"\\n=== OPTIMIZATION IMPACT ANALYSIS ===\")\n",
    "\n",
    "baseline_batch_64 = benchmark_results[64][\"MHA FP32\"]\n",
    "optimized_batch_64 = benchmark_results[64][\"MQA FP16\"]\n",
    "\n",
    "architecture_improvement = benchmark_results[64][\"MQA FP32\"]['throughput'] / baseline_batch_64['throughput']\n",
    "precision_improvement = benchmark_results[64][\"MHA FP16\"]['throughput'] / baseline_batch_64['throughput'] \n",
    "combined_improvement = optimized_batch_64['throughput'] / baseline_batch_64['throughput']\n",
    "\n",
    "# Memory efficiency analysis\n",
    "baseline_memory = baseline_batch_64['avg_memory_mb']\n",
    "mha_fp16_memory = benchmark_results[64][\"MHA FP16\"]['avg_memory_mb']\n",
    "mqa_fp32_memory = benchmark_results[64][\"MQA FP32\"]['avg_memory_mb']\n",
    "optimized_memory = optimized_batch_64['avg_memory_mb']\n",
    "\n",
    "precision_memory_reduction = (baseline_memory - mha_fp16_memory) / baseline_memory * 100\n",
    "architecture_memory_reduction = (baseline_memory - mqa_fp32_memory) / baseline_memory * 100\n",
    "combined_memory_reduction = (baseline_memory - optimized_memory) / baseline_memory * 100\n",
    "\n",
    "print(f\"Architecture optimization (MQA): {architecture_improvement:.2f}x improvement\")\n",
    "print(f\"Mixed precision (FP16): {precision_improvement:.2f}x improvement\")\n",
    "print(f\"Combined optimization: {combined_improvement:.2f}x improvement\")\n",
    "\n",
    "print(f\"\\n=== MEMORY EFFICIENCY ANALYSIS ===\")\n",
    "print(f\"Baseline memory usage (MHA FP32): {baseline_memory:.1f} MB\")\n",
    "print(f\"Mixed precision reduction: {precision_memory_reduction:.1f}%\")\n",
    "print(f\"Architecture reduction: {architecture_memory_reduction:.1f}%\")\n",
    "print(f\"Combined memory reduction: {combined_memory_reduction:.1f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mixed precision insight**: T4's Tensor Cores provide automatic acceleration for FP16 matrix operations, delivering on multiplicative improvements with minimal accuracy loss. The exact performance gain depends on model architecture, batch size, and memory access patterns, but typically shows significant improvements over FP32 operations. The primary benefit comes from computational acceleration rather than memory reduction, since activation memory and other factors can limit memory savings.\n",
    "> \n",
    "> When combined with memory-efficient architectures like MQA, these hardware-specific optimizations create compounding performance benefits that can transform production deployment feasibility.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.5: Visualize MQA bs MHA performance\n",
    "\n",
    "Let's visualize our results to get a more intuitive understanding of the impact of our optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_performance_comparison():\n",
    "    \"\"\"Create comprehensive visualization of optimization results\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    batch_sizes_list = list(benchmark_results.keys())\n",
    "    \n",
    "    # Throughput comparison - all configurations\n",
    "    mha_fp32_throughputs = [benchmark_results[bs][\"MHA FP32\"]['throughput'] for bs in batch_sizes_list]\n",
    "    mha_fp16_throughputs = [benchmark_results[bs][\"MHA FP16\"]['throughput'] for bs in batch_sizes_list]\n",
    "    mqa_fp32_throughputs = [benchmark_results[bs][\"MQA FP32\"]['throughput'] for bs in batch_sizes_list]\n",
    "    mqa_fp16_throughputs = [benchmark_results[bs][\"MQA FP16\"]['throughput'] for bs in batch_sizes_list]\n",
    "    \n",
    "    x = np.arange(len(batch_sizes_list))\n",
    "    width = 0.2\n",
    "    \n",
    "    ax1.bar(x - 1.5*width, mha_fp32_throughputs, width, label='MHA FP32', color='#e74c3c', alpha=0.7)\n",
    "    ax1.bar(x - 0.5*width, mha_fp16_throughputs, width, label='MHA FP16', color='#ff7f7f', alpha=0.7)\n",
    "    ax1.bar(x + 0.5*width, mqa_fp32_throughputs, width, label='MQA FP32', color='#27ae60', alpha=0.7)\n",
    "    ax1.bar(x + 1.5*width, mqa_fp16_throughputs, width, label='MQA FP16', color='#2ecc71', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Batch Size')\n",
    "    ax1.set_ylabel('Throughput (samples/sec)')\n",
    "    ax1.set_title('Throughput Comparison: All Configurations')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(batch_sizes_list)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Latency comparison - baseline vs optimized\n",
    "    baseline_latencies = [benchmark_results[bs][\"MHA FP32\"]['latency_ms'] for bs in batch_sizes_list]\n",
    "    optimized_latencies = [benchmark_results[bs][\"MQA FP16\"]['latency_ms'] for bs in batch_sizes_list]\n",
    "    \n",
    "    ax2.plot(batch_sizes_list, baseline_latencies, marker='o', linewidth=2, label='MHA FP32 (Baseline)', color='#e74c3c')\n",
    "    ax2.plot(batch_sizes_list, optimized_latencies, marker='s', linewidth=2, label='MQA FP16 (Optimized)', color='#2ecc71')\n",
    "    ax2.set_xlabel('Batch Size')\n",
    "    ax2.set_ylabel('Latency (ms)')\n",
    "    ax2.set_title('Latency Comparison: Baseline vs Optimized')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Improvement factors\n",
    "    improvements = [benchmark_results[bs][\"MQA FP16\"]['throughput'] / benchmark_results[bs][\"MHA FP32\"]['throughput'] \n",
    "                   for bs in batch_sizes_list]\n",
    "    \n",
    "    bars = ax3.bar(batch_sizes_list, improvements, color='#3498db', alpha=0.7)\n",
    "    ax3.set_xlabel('Batch Size')\n",
    "    ax3.set_ylabel('Throughput Improvement (x)')\n",
    "    ax3.set_title('Combined Optimization Improvement Factor')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add improvement values on bars\n",
    "    for i, v in enumerate(improvements):\n",
    "        ax3.text(batch_sizes_list[i], v + 0.05, f'{v:.2f}x', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Memory efficiency comparison (batch size 64)\n",
    "    batch_64_results = benchmark_results[64]\n",
    "    memory_configs = [\"MHA FP32\", \"MHA FP16\", \"MQA FP32\", \"MQA FP16\"]\n",
    "    memory_usage = [batch_64_results[config]['avg_memory_mb'] for config in memory_configs]\n",
    "    colors = ['#e74c3c', '#ff7f7f', '#27ae60', '#2ecc71']\n",
    "    \n",
    "    bars = ax4.bar(memory_configs, memory_usage, color=colors, alpha=0.7)\n",
    "    ax4.set_ylabel('Memory Usage (MB)')\n",
    "    ax4.set_title('Memory Usage Comparison (Batch Size 64)')\n",
    "    ax4.set_xticklabels(memory_configs, rotation=45, ha='right')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add memory values on bars\n",
    "    for bar, mem_val in zip(bars, memory_usage):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{mem_val:.0f}MB', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'comprehensive_optimization_results.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nVisualization saved to: {output_dir}/comprehensive_optimization_results.png\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "visualize_performance_comparison()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO: Architecture recommendation analysis**\n",
    "> \n",
    "> Using your benchmark results _(3.10x combined improvement, 1.47x architecture gain, 2.64x mixed precision gain, 22% memory reduction)_, recommend the optimal configuration for each scenario below, following the example scenario provided. \n",
    "> <br> Give a one-sentence justification based on the performance characteristics you measured.\n",
    "> \n",
    "> \n",
    "> **Example scenario:** High-volume social media monitoring\n",
    "> \n",
    "> - Requirements: Process 50,000 posts/minute across 10 T4 GPUs, minimize cloud costs\n",
    "> - Current bottleneck: Infrastructure costs too high with current throughput\n",
    "> \n",
    ">   Your recommendation: MQA + FP16\n",
    ">   <br>\n",
    ">   Reasoning: The 3.10x combined throughput improvement allows processing the same 50,000 posts/minute with just 3-4 T4 instances instead of 10, dramatically reducing cloud infrastructure costs.\n",
    "> \n",
    "> **Scenarios to analyze:**\n",
    "> \n",
    "> 1\\. Real-time gaming chat moderation\n",
    "> \n",
    "> - Requirements: <25ms response time, handle sudden traffic spikes during tournaments\n",
    "> - Current bottleneck: Latency spikes cause poor user experience during peak gaming hours\n",
    "> \n",
    ">   Your recommendation: ________________\n",
    ">   <br>\n",
    ">   Reasoning: ________________________________________________________________\n",
    "> \n",
    "> 2\\. Research lab experimental setup\n",
    "> \n",
    "> - Requirements: Maximum model accuracy for new NLP research, unlimited A100 GPU access\n",
    "> - Current focus: Exploring attention mechanisms, accuracy is paramount over efficiency\n",
    "> \n",
    ">   Your recommendation: ________________\n",
    ">   <br>\n",
    ">   Reasoning: ________________________________________________________________\n",
    "> \n",
    "> 3\\. Mobile content filtering app\n",
    "> \n",
    "> - Requirements: Run on smartphones with 4GB RAM, battery-efficient inference\n",
    "> - Current bottleneck: App crashes due to memory limits on mid-range devices\n",
    "> \n",
    ">   Your recommendation: ________________\n",
    ">   <br>\n",
    ">   Reasoning: ________________________________________________________________\n",
    "> \n",
    "> 4\\. Edge IoT device deployment\n",
    "> \n",
    "> - Requirements: Process sensor data on Jetson Nano (4GB RAM), 5W power budget\n",
    "> - Current bottleneck: Model too large and power-hungry for continuous operation\n",
    "> \n",
    ">   Your recommendation: ________________\n",
    ">   <br>\n",
    ">   Reasoning: ________________________________________________________________\n",
    "> \n",
    "> 5\\. Enterprise API with SLA requirements\n",
    "> \n",
    "> - Requirements: 99.9% uptime, <100ms response time, cost-efficient scaling\n",
    "> - Current bottleneck: Current 170ms latency violates SLA, causing contract penalties\n",
    "> \n",
    ">   Your recommendation: ________________\n",
    ">   <br>\n",
    ">   Reasoning: ________________________________________________________________\n",
    "> \n",
    "> **Reflection questions:**\n",
    "> \n",
    "> - Which optimization (architecture vs mixed precision) provides the larger performance gain?\n",
    "> - How does the 22% memory reduction impact deployment decisions?\n",
    "> - When might you choose MQA FP32 over MHA FP16 despite lower throughput?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've implemented and compared attention architectures to gain a hands-on understanding of the relationship between transformer architecture choices and GPU hardware efficiency. \n",
    "\n",
    "**Key insight**: Modern transformer performance is often limited by memory bandwidth rather than computational capacity. Architecture choices like MQA that reduce memory traffic while preserving computational patterns can unlock dramatic performance improvements without requiring additional hardware.\n",
    "\n",
    "The architectural principles you've learned—shared projections, memory-efficient attention patterns, and systematic hardware impact analysis—plus the use of hardware acceleration such as tensor cores, apply broadly to transformer optimization and are essential for cost-effective deployment of language models in production environments.\n",
    "\n",
    "##### **Next optimization challenges to explore:**\n",
    "\n",
    "- **Grouped Query Attention (GQA)**: Implement the middle ground between MHA and MQA with configurable key/value sharing\n",
    "\n",
    "- **Advanced profiling techniques:** Use PyTorch Profiler to identify micro-optimizations in attention kernels\n",
    "\n",
    "- **Memory optimization strategies**: Explore gradient checkpointing and activation recomputation for longer sequences\n",
    "\n",
    "- **Multi-GPU attention scaling**: Implement tensor parallelism for attention across multiple GPUs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
