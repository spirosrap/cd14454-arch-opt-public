{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Combine architecture and hardware optimizations for GPU acceleration\n",
    "\n",
    "You have learned about hardware-aware architecture design and hardware acceleration tools in theory. Now it's time to apply these concepts by combining architectural modifications with hardware optimizations to discover how they interact in practice.\n",
    "\n",
    "**Overview:** Explore how architectural choices and GPU acceleration can interact in complex ways, requiring careful coordination rather than simple stacking of optimizations to unlock expected performance gains.\n",
    "\n",
    "**Scenario:** You work for a visual content moderation platform that processes millions of images in real-time. Although your model delivers strong accuracy, its current throughput falls far short of peak demand: you need to nearly 10x performance to keep up. To close the gap, you turn to both TensorRT-based hardware acceleration and architectural adjustments, especially since the DevOps team reports highly unstable GPU utilization (25–60%), suggesting inefficiencies in how the model and hardware interact.\n",
    "\n",
    "**Goal:** Apply at least one architectural modification and one hardware acceleration technique to DenseNet121, then measure their interaction to see that the combined gains are not strictly additive and may have a positive or a negative effect.\n",
    "\n",
    "**Tools:** PyTorch, TensorRT, ONNX, CUDA tools\n",
    "\n",
    "**Estimated Time:** 20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Let's establish our baseline environment and verify T4 capabilities for integrated optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out and restart notebook\n",
    "# ! pip install torchinfo tensorrt onnx onnxruntime-gpu cuda-python datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "import tensorrt as trt\n",
    "import onnx\n",
    "from cuda import cudart\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"assets/exercise1\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check T4 GPU capabilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    \n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "    print(f\"Total Memory: {gpu_properties.total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Multiprocessors: {gpu_properties.multi_processor_count}\")\n",
    "    print(f\"Memory Bandwidth: ~320 GB/s (theoretical)\")\n",
    "    \n",
    "    # Check for Tensor Core support (Compute Capability >= 7.0)\n",
    "    tensor_cores_available = gpu_properties.major >= 7\n",
    "    print(f\"Tensor Core Support: {'✓ Available' if tensor_cores_available else '✗ Not Available'}\")\n",
    "    \n",
    "    if tensor_cores_available:\n",
    "        print(\"  → Mixed precision (FP16) will show significant speedup\")\n",
    "        print(\"  → Kernel fusion opportunities available\")\n",
    "        \n",
    "    print(\"Setup complete!\")\n",
    "else:\n",
    "    print(\"CUDA not available - exercise requires GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **T4 hardware context:** T4 GPUs feature 2,560 CUDA cores, 320 Tensor Cores, and 320 GB/s memory bandwidth. The Tensor Cores are specifically designed to accelerate mixed precision (FP16) operations, but their effectiveness depends on the types of operations in your model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create benchmark dataset and model\n",
    "\n",
    "Let's establish our baseline DenseNet121 model and prepare consistent benchmarking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic ImageNet-like data for consistent benchmarking\n",
    "def create_benchmark_dataset(num_samples=1000, image_size=224):\n",
    "    \"\"\"\n",
    "    Create synthetic dataset for controlled benchmarking\n",
    "    \n",
    "    Args:\n",
    "        num_samples (int): Number of samples to generate\n",
    "        image_size (int): Size of square images (224x224 for ImageNet)\n",
    "    \n",
    "    Returns:\n",
    "        TensorDataset: Synthetic images and labels for benchmarking\n",
    "    \"\"\"\n",
    "    # Generate random images with ImageNet statistics\n",
    "    images = torch.randn(num_samples, 3, image_size, image_size)\n",
    "    labels = torch.randint(0, 1000, (num_samples,))\n",
    "    \n",
    "    dataset = TensorDataset(images, labels)\n",
    "    return dataset\n",
    "\n",
    "# Create benchmark dataset and dataloaders\n",
    "benchmark_dataset = create_benchmark_dataset()\n",
    "print(f\"Benchmark dataset created: {len(benchmark_dataset)} samples\")\n",
    "\n",
    "batch_sizes = [16, 32, 64]\n",
    "dataloaders = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    dataloaders[batch_size] = DataLoader(\n",
    "        benchmark_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        pin_memory=True  # Enables faster GPU transfer\n",
    "    )\n",
    "\n",
    "print(f\"DataLoaders created for batch sizes: {batch_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Synthetic dataset considerations:** Using synthetic data eliminates dataset loading and preprocessing bottlenecks, allowing us to isolate and measure the pure model optimization effects. Real-world deployments would show additional complexities from data pipeline optimization, but this controlled approach reveals the core architectural and hardware interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline DenseNet121 model\n",
    "baseline_model = models.densenet121()\n",
    "baseline_model = baseline_model.to(device)\n",
    "baseline_model.eval()\n",
    "\n",
    "# Get model information\n",
    "summary(baseline_model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Understanding DenseNet's model:** The DenseNet model is based on *dense connectivity*, where each layer receives inputs from ALL preceding layers within a block, creating progressively larger feature maps through concatenation rather than element-wise addition. This leads to this memory usage characteristics:\n",
    "> \n",
    "> - *Memory growth* - feature maps grow linearly with depth (growth_rate × num_layers), making later layers process significantly more channels than earlier ones. \n",
    "> - *Memory efficiency trade-offs* - while concatenation enables better gradient flow and feature reuse, it creates higher memory bandwidth requirements and different GPU utilization patterns. \n",
    "> \n",
    "> This baseline analysis can help you understand how architectural modifications interact with DenseNet's inherent dense connectivity when combined with hardware acceleration techniques. You can find even more details about DenseNet in the original [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993) paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Choose your hardware<>architecture optimization path\n",
    "\n",
    "To optimize the model on GPU, you need to implement at least **one architectural modification** and **one hardware acceleration technique**, then measure their interaction effects. Choose your path!\n",
    "\n",
    "### Architecture modification options (Choose >=1):\n",
    "\n",
    "**Option A: Early Exit Architecture**\n",
    "- Add intermediate classifier after dense block 3 for confident predictions\n",
    "- Reduces average computation by skipping expensive final block for easy samples\n",
    "- Adaptive computation based on prediction confidence\n",
    "\n",
    "**Option B: Reduced Dense Layers**  \n",
    "- Decrease growth rate from 32 to 24 channels per layer\n",
    "- Reduces memory bandwidth and computation while maintaining architecture\n",
    "- Direct efficiency improvement through less feature concatenation\n",
    "\n",
    "**Option C: Low-Rank Classifier Factorization**\n",
    "- Decompose final classifier using SVD (7M→2M parameters)\n",
    "- Maintains dense matrix operations while reducing computational load\n",
    "- Preserves patterns that work well with hardware acceleration\n",
    "\n",
    "**Option D: Grouped Convolutions**\n",
    "- Replace standard 1×1 bottleneck convolutions with grouped versions\n",
    "- Reduces FLOPs while maintaining regular convolution patterns\n",
    "- Better hardware utilization than depthwise separable approaches\n",
    "\n",
    "**Option E: Structured Channel Pruning**\n",
    "- Remove least important channels from dense block connections\n",
    "- Creates structured sparsity that maintains computational efficiency\n",
    "- Reduces both parameters and memory bandwidth requirements\n",
    "\n",
    "### Hardware acceleration options (Choose >=1):\n",
    "\n",
    "**Option A: Mixed Precision (FP16) Optimization**\n",
    "- Leverages Tensor Cores for automatic FP16/FP32 selection\n",
    "- Accelerates compatible operations while maintaining numerical stability\n",
    "- Most effective with dense matrix operations\n",
    "\n",
    "**Option B: Dynamic Batching Optimization**\n",
    "- Optimizes batch processing and memory utilization\n",
    "- Changes GPU occupancy patterns and memory coalescing\n",
    "- Can reveal memory bandwidth vs compute trade-offs\n",
    "\n",
    "**Option C: Kernel Fusion + Graph Optimization**  \n",
    "- Combines multiple operations into single GPU kernels\n",
    "- Reduces memory bandwidth requirements\n",
    "- Effectiveness depends on operation compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO: Summarize your optimization strategy**\n",
    "> \n",
    "> Now that you've made your strategic choice on the combination of architecture and hardware optimizations to implement, briefly explain your reasoning _(1-2 sentences)_\n",
    "> <br>HINT: Consider DenseNet121's characteristics when it comes to memory usage, compute complexity, and fusion opportunities.\n",
    "> \n",
    "> _Add your answer here_: __________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement architecture optimization(s)\n",
    "\n",
    "Now it's time to translate your chosen architectural strategy into code. \n",
    "\n",
    "This step involves modifying the model's structure in PyTorch to change how it computes results. By altering the model's forward pass or its layers, you can introduce efficiencies like adaptive computation or reduced parameter counts, directly impacting its performance profile before any hardware-specific compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_densenet(base_model, device):\n",
    "    \"\"\"Create DenseNet with optimizations\"\"\"\n",
    "\n",
    "    # TODO: Create your optimization logic as you wish (modularization is recommended), and define the optimized model in the optimized_model variable\n",
    "    # HINT: You can refer to the exercises in lesson 2 for some implementations\n",
    "    # Or find inspiration at discuss.pytorch.org\n",
    "    optimized_model = # Add your code here\n",
    "\n",
    "    optimized_model = optimized_model.to(device)\n",
    "    optimized_model.eval()\n",
    "    \n",
    "    return optimized_model\n",
    "\n",
    "# Create architecture-optimized model\n",
    "print(\"Creating optimized DenseNet...\")\n",
    "arch_model = create_optimized_densenet(baseline_model, device)\n",
    "\n",
    "# Get optimized model information\n",
    "arch_reduction = sum(p.numel() for p in baseline_model.parameters()) - sum(p.numel() for p in arch_model.parameters())\n",
    "print(f\"\\nParameter reduction vs. baseline: {arch_reduction:,} ({arch_reduction/sum(p.numel() for p in baseline_model.parameters())*100:.1f}%)\")\n",
    "summary(arch_model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **On aligning architecture with hardware**: Every architectural modification changes the model's computational pattern in a specific way, whether through parameter reduction, operation reordering, or conditional execution. \n",
    "> \n",
    "> The key is understanding how these changes will interact with your target hardware's strengths and limitations. Some architectural patterns align naturally with GPU acceleration (dense matrix operations), while others introduce complexity that static compilers cannot optimize (dynamic branching). Measuring the individual architectural effect first provides a baseline to understand interaction effects later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement hardware acceleration with TensorRT\n",
    "\n",
    "With the architecture modified, you can now turn to hardware acceleration. \n",
    "\n",
    "This step involves converting our high-level PyTorch model into a format that a specialized compiler, [TensorRT](https://developer.nvidia.com/tensorrt), can optimize. [ONNX (Open Neural Network Exchange)](https://onnx.ai/) acts as a bridge: TensorRT takes this ONNX graph and applies powerful GPU-specific optimizations.\n",
    "\n",
    "**Remember that building the TRT engine could take up to a few minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's export the model to the ONNX format preferred by TensorRT\n",
    "def export_to_onnx(model, sample_input, onnx_path):\n",
    "    \"\"\"\n",
    "    Export PyTorch model to ONNX format for TensorRT optimization\n",
    "    \n",
    "    ONNX (Open Neural Network Exchange) is an intermediate representation\n",
    "    that TensorRT uses to analyze and optimize model architectures.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Export model to ONNX format\n",
    "        # TODO: Make sure the export supports chosen optimizations, if applicable parameters exist\n",
    "        # HINT: How does ONNX handle batches? Should you set up fp16 precision here or in TensorRT?\n",
    "        # Reference: https://docs.pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.export\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            sample_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=17,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output']\n",
    "        )\n",
    "\n",
    "        # Verify ONNX model\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(f\"ONNX export successful: {onnx_path}\")\n",
    "        print(f\"   Total nodes: {len(onnx_model.graph.node)}\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ONNX export failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondly, let's create the optimized TensorRT engine \n",
    "def optimize_with_tensorrt(onnx_path, engine_path):\n",
    "    \"\"\"\n",
    "    Convert ONNX model to TensorRT engine with optional optimizations\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create TensorRT logger and builder\n",
    "        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "        builder = trt.Builder(TRT_LOGGER)\n",
    "        config = builder.create_builder_config()\n",
    "\n",
    "        # TODO: Implement your TensorRT optimizations\n",
    "        # HINT: You can refer to the exercise 1 in lesson 3 for an example implementation\n",
    "        # Reference: https://developer.nvidia.com/docs/drive/drive-os/6.0.7/public/drive-os-tensorrt/api-reference/docs/python/infer/Core/BuilderConfig.html#tensorrt.IBuilderConfig\n",
    "\n",
    "        # Add your code here\n",
    "        \n",
    "        # Parse ONNX model\n",
    "        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "        parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "        \n",
    "        with open(onnx_path, 'rb') as model_file:\n",
    "            if not parser.parse(model_file.read()):\n",
    "                print(\"Failed to parse ONNX model\")\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return False\n",
    "\n",
    "        # Build optimized engine\n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "        \n",
    "        if serialized_engine is None:\n",
    "            print(\"Failed to build TensorRT engine\")\n",
    "            return False\n",
    "            \n",
    "        # Save optimized engine\n",
    "        with open(engine_path, 'wb') as f:\n",
    "            f.write(serialized_engine)\n",
    "            \n",
    "        print(f\"TensorRT engine saved: {engine_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"TensorRT optimization failed: {e}\")\n",
    "\n",
    "# Export models to ONNX format\n",
    "sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "baseline_onnx_path = os.path.join(output_dir, \"baseline_densenet121.onnx\")\n",
    "arch_onnx_path = os.path.join(output_dir, \"optimized_densenet121.onnx\")\n",
    "\n",
    "print(\"Exporting models to ONNX...\")\n",
    "baseline_onnx_success = export_to_onnx(baseline_model, sample_input, baseline_onnx_path)\n",
    "arch_onnx_success = export_to_onnx(arch_model, sample_input, arch_onnx_path)\n",
    "\n",
    "onnx.checker.check_model(baseline_onnx_path)\n",
    "onnx.checker.check_model(arch_onnx_path)\n",
    "\n",
    "# Optimize with TensorRT\n",
    "baseline_engine_path = os.path.join(output_dir, \"baseline_densenet_fused.engine\")\n",
    "arch_engine_path = os.path.join(output_dir, \"optimized_densenet_fused.engine\")\n",
    "\n",
    "if baseline_onnx_success and arch_onnx_success:\n",
    "    print(\"\\nOptimizing with TensorRT...\")\n",
    "    baseline_trt_success = optimize_with_tensorrt(\n",
    "        baseline_onnx_path, \n",
    "        baseline_engine_path\n",
    "    )\n",
    "    arch_trt_success = optimize_with_tensorrt(\n",
    "        arch_onnx_path,\n",
    "        arch_engine_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Understanding ONNX export and TensorRT compilation patterns**: \n",
    "> 1. The _ONNX model_ is not guaranteed to have a lower total node count. Early exit architectures, parameter sharing, or layer removal can reduce the graph complexity that ONNX captures. However, some optimizations like attention mechanisms or complex branching might actually increase node counts. \n",
    "> \n",
    "> 2. _TensorRT's compilation_ process typically runs quietly by default, but you might see warnings about unsupported operations, precision conversions, or optimization choices. Some of these warnings can be ignored (e.g., layer fusion opportunities taken) while others require investigation (e.g., unsupported operation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Perform systematic performance benchmarking\n",
    "\n",
    "Now let's measure the impact. \n",
    "\n",
    "This systematic approach lets us measure individual technique effects, combined effects, and quantify whether optimizations amplify each other, conflict, or remain independent.\n",
    "\n",
    "A systematic benchmark is crucial to understand not just if our changes made the model faster, but why. You will compare four configurations to isolate the effects of each optimization and, most importantly, to reveal how they interact when combined. _Does 1+1 equal 2, or do you see an unexpected result?_\n",
    "\n",
    "⎯⎯⎯⎯⎯⎯⎯\n",
    "\n",
    "You only have **one TODO in step 6.2** to prepare the inputs for inference. <ins>If you want to scroll quickly through the rest of the code</ins>, here's a high-level summary:\n",
    "\n",
    "- *6.1*: Build the TensorRT inference class for consistent benchmarking\n",
    "- *6.2*: Create a fair comparison methodology across PyTorch and TensorRT models  \n",
    "- *6.3*: Execute the core experiment and calculate optimizations' interaction effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Create the TensorRT inference class\n",
    "\n",
    "To run systematic comparisons, you need a unified interface for benchmarking TensorRT engines. This helper class handles the low-level GPU memory management and inference execution, allowing us to focus on measuring performance rather than implementation details.\n",
    "\n",
    "**Note:** For this exercise, we use synchronous CUDA operations (`execute_v2`, `cudaMemcpy`) rather than async variants to ensure accurate timing measurements. Async operations would improve throughput but make it harder to isolate individual inference latency for benchmarking purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class for running inference with a TensorRT engine\n",
    "class TensorRTInfer:\n",
    "    def __init__(self, engine_path):\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        with open(engine_path, \"rb\") as f, trt.Runtime(self.logger) as runtime:\n",
    "            self.engine = runtime.deserialize_cuda_engine(f.read())\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "        # Assume 1 input, 1 output\n",
    "        self.in_name = [self.engine.get_tensor_name(i)\n",
    "                        for i in range(self.engine.num_io_tensors)\n",
    "                        if self.engine.get_tensor_mode(self.engine.get_tensor_name(i)) == trt.TensorIOMode.INPUT][0]\n",
    "        self.out_name = [self.engine.get_tensor_name(i)\n",
    "                         for i in range(self.engine.num_io_tensors)\n",
    "                         if self.engine.get_tensor_mode(self.engine.get_tensor_name(i)) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "        # Match engine dtypes to numpy\n",
    "        self.np_in_dtype = np.float16 if self.engine.get_tensor_dtype(self.in_name) == trt.float16 else np.float32\n",
    "        self.np_out_dtype = np.float16 if self.engine.get_tensor_dtype(self.out_name) == trt.float16 else np.float32\n",
    "\n",
    "    def infer(self, input_tensor: np.ndarray):\n",
    "        # Ensure correct dtype and contiguous memory\n",
    "        inp = np.ascontiguousarray(input_tensor).astype(self.np_in_dtype)\n",
    "\n",
    "        # Handle dynamic shapes\n",
    "        if -1 in self.engine.get_tensor_shape(self.in_name):\n",
    "            self.context.set_input_shape(self.in_name, inp.shape)\n",
    "\n",
    "        # Get concrete output shape\n",
    "        out_shape = tuple(self.context.get_tensor_shape(self.out_name))\n",
    "\n",
    "        # Allocate device memory\n",
    "        in_bytes = inp.nbytes\n",
    "        out_bytes = np.prod(out_shape) * np.dtype(self.np_out_dtype).itemsize\n",
    "\n",
    "        _, d_in = cudart.cudaMalloc(in_bytes)\n",
    "        _, d_out = cudart.cudaMalloc(out_bytes)\n",
    "\n",
    "        # Host output buffer\n",
    "        h_out = np.empty(np.prod(out_shape), dtype=self.np_out_dtype)\n",
    "\n",
    "        # Bind device pointers\n",
    "        self.context.set_tensor_address(self.in_name, int(d_in))\n",
    "        self.context.set_tensor_address(self.out_name, int(d_out))\n",
    "\n",
    "        # H2D copy\n",
    "        cudart.cudaMemcpy(d_in, inp.ctypes.data, in_bytes,\n",
    "                          cudart.cudaMemcpyKind.cudaMemcpyHostToDevice)\n",
    "        \n",
    "        # Prepare bindings array\n",
    "        bindings = [int(d_in), int(d_out)]  # device pointers in the order of engine bindings\n",
    "\n",
    "        # Run synchronously\n",
    "        if not self.context.execute_v2(bindings):\n",
    "            raise RuntimeError(\"TensorRT inference failed\")\n",
    "\n",
    "        # D2H copy\n",
    "        cudart.cudaMemcpy(h_out.ctypes.data, d_out, out_bytes,\n",
    "                          cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost)\n",
    "\n",
    "        # Free device memory\n",
    "        cudart.cudaFree(d_in)\n",
    "        cudart.cudaFree(d_out)\n",
    "\n",
    "        return h_out.reshape(out_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Define benchmarking logic for each model\n",
    "\n",
    "With our infrastructure in place, you can now create consistent benchmarking functions that can handle both PyTorch and TensorRT models. The key challenge is ensuring fair comparisons by controlling for differences in input preprocessing, warmup procedures, and timing methodology across all four configurations you'll test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_inputs(inputs, framework=\"pytorch\", variant=\"baseline\", device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Prepare inputs depending on framework and optimization variant.\n",
    "    \n",
    "    Args:\n",
    "        inputs: batch from dataloader\n",
    "        framework: \"pytorch\" or \"tensorrt\"\n",
    "        variant: \"baseline\" or \"optimized\" (e.g., fp16)\n",
    "        device: \"cuda\" or \"cpu\"\n",
    "    \"\"\"\n",
    "    # TODO: Define your input\n",
    "    # HINT: Your baseline and optimized models may expected different formats depending on the chosen optimization\n",
    "    # Don't forget to place the input of the expected device too\n",
    "\n",
    "    # Add your code here\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def benchmark_trt_engine(engine_path, dataloader, num_batches=20, model_name=\"Model\"):\n",
    "    \"\"\"Benchmark a TensorRT engine.\"\"\"\n",
    "    trt_infer = TensorRTInfer(engine_path)\n",
    "    times = []\n",
    "    \n",
    "    print(f\"Benchmarking {model_name}...\")\n",
    "\n",
    "    model_variant = \"baseline\" if \"baseline\" in model_name.lower() else \"optimized\"\n",
    "    \n",
    "    # Warmup\n",
    "    for i, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = _prepare_inputs(inputs, framework=\"tensorrt\", variant=model_variant)\n",
    "        if i >= 3: break\n",
    "        _ = trt_infer.infer(inputs)\n",
    "\n",
    "    # Benchmark\n",
    "    for i, (inputs, _) in enumerate(dataloader):\n",
    "        if i >= num_batches: break\n",
    "        inputs = _prepare_inputs(inputs, framework=\"tensorrt\", variant=model_variant)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.perf_counter()\n",
    "        _ = trt_infer.infer(inputs)\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        times.append(end_time - start_time)\n",
    "        \n",
    "    return times\n",
    "\n",
    "def benchmark_pytorch_model(model, dataloader, num_batches=20, model_name=\"Model\"):\n",
    "    \"\"\"Benchmark a PyTorch model.\"\"\"\n",
    "    model.eval().to(device)\n",
    "    times = []\n",
    "    \n",
    "    print(f\"Benchmarking {model_name}...\")\n",
    "    \n",
    "    model_variant = \"baseline\" if \"baseline\" in model_name.lower() else \"optimized\"\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(dataloader):   \n",
    "            inputs = _prepare_inputs(inputs, framework=\"pytorch\", variant=model_variant)\n",
    "            if i >= 3: break\n",
    "            _ = model(inputs)\n",
    "\n",
    "    # Benchmark\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            if i >= num_batches: break\n",
    "            inputs = _prepare_inputs(inputs, framework=\"pytorch\", variant=model_variant)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "            _ = model(inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            times.append(end_time - start_time)\n",
    "            \n",
    "    return times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Analyze optimizations' interactions and results\n",
    "\n",
    "This is the core experiment: measuring your different configurations to isolate individual optimization effects and discover how they interact when combined. \n",
    "\n",
    "You'll benchmark baseline PyTorch, architecture-only PyTorch, hardware-only TensorRT, and combined TensorRT to calculate the interaction factor and determine whether techniques amplify, conflict, or remain independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_technique_interactions():\n",
    "    \"\"\"Systematic analysis of the effect of optimizations.\"\"\"\n",
    "    print(\"\\n=== TECHNIQUE INTERACTION ANALYSIS ===\")\n",
    "    results = {}\n",
    "    batch_size = 32\n",
    "    dataloader = dataloaders[batch_size]\n",
    "\n",
    "    # --- 1. Baseline PyTorch ---\n",
    "    times = benchmark_pytorch_model(baseline_model, dataloader, model_name=\"Baseline DenseNet (PyTorch)\")\n",
    "    results['baseline'] = {'throughput': batch_size / np.mean(times), 'name': \"Baseline (PyTorch)\"}\n",
    "    print(f\"  Throughput: {results['baseline']['throughput']:.1f} samples/sec\\n\")\n",
    "\n",
    "    # --- 2. Architecture-Optimized PyTorch ---\n",
    "    print(\"Benchmarking Arch-Optimized (PyTorch)...\")\n",
    "    total_samples = 0\n",
    "    arch_times = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            if i >= 15: break\n",
    "            inputs = inputs.to(device)\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "            _, exit_type = arch_model(inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            arch_times.append(end_time - start_time)\n",
    "            total_samples += inputs.size(0)\n",
    "    results['architecture_only'] = {'throughput': batch_size / np.mean(arch_times), 'name': \"Arch-Optimized (PyTorch)\"}\n",
    "    print(f\"  Throughput: {results['architecture_only']['throughput']:.1f} samples/sec\\n\")\n",
    "\n",
    "    # --- 3. Hardware-Optimized TensorRT ---\n",
    "    if baseline_trt_success:\n",
    "        times = benchmark_trt_engine(baseline_engine_path, dataloader, model_name=\"Baseline DenseNet (TensorRT)\")\n",
    "        results['hardware_only'] = {'throughput': batch_size / np.mean(times), 'name': \"Baseline (TensorRT)\"}\n",
    "        print(f\"  Throughput: {results['hardware_only']['throughput']:.1f} samples/sec\\n\")\n",
    "    else:\n",
    "        results['hardware_only'] = {'throughput': 0, 'name': \"Baseline (TensorRT)\"}\n",
    "\n",
    "    # --- 4. Combined Optimization TensorRT ---\n",
    "    if arch_trt_success:\n",
    "        times = benchmark_trt_engine(arch_engine_path, dataloader, model_name=\"Arch-Optimized (TensorRT)\")\n",
    "        results['combined'] = {'throughput': batch_size / np.mean(times), 'name': \"Combined (TensorRT)\"}\n",
    "        print(f\"  Throughput: {results['combined']['throughput']:.1f} samples/sec\\n\")\n",
    "    else:\n",
    "        results['combined'] = {'throughput': 0, 'name': \"Combined (TensorRT)\"}\n",
    "\n",
    "    return results\n",
    "\n",
    "interaction_results = analyze_technique_interactions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print analysis results\n",
    "baseline = interaction_results['baseline']\n",
    "hardware_only = interaction_results.get('hardware_only', {'throughput': 0})\n",
    "architecture_only = interaction_results['architecture_only']\n",
    "combined = interaction_results.get('combined', {'throughput': 0})\n",
    "\n",
    "# Avoid division by zero if a benchmark failed\n",
    "if baseline['throughput'] > 0 and hardware_only['throughput'] > 0 and combined['throughput'] > 0:\n",
    "    architecture_gain = architecture_only['throughput'] / baseline['throughput']\n",
    "    hardware_gain = hardware_only['throughput'] / baseline['throughput']\n",
    "    combined_gain = combined['throughput'] / baseline['throughput']\n",
    "\n",
    "    # Analyze interaction effects\n",
    "    theoretical_combined = architecture_gain * hardware_gain\n",
    "    actual_combined = combined_gain\n",
    "    interaction_factor = actual_combined / theoretical_combined\n",
    "\n",
    "    print(\"=== INTEGRATION ANALYSIS ===\")\n",
    "    print(\"INDIVIDUAL TECHNIQUE EFFECTS:\")\n",
    "    print(f\"Architecture optimization: {architecture_gain:.2f}x throughput\")\n",
    "    print(f\"Hardware optimization:     {hardware_gain:.2f}x throughput\")\n",
    "    print()\n",
    "\n",
    "    print(\"INTEGRATION EFFECTS:\")\n",
    "    print(f\"Theoretical combined improvement (if independent): {theoretical_combined:.2f}x over baseline\")\n",
    "    print(f\"Actual combined improvement:                       {actual_combined:.2f}x over baseline\")\n",
    "    print(f\"Interaction factor: {interaction_factor:.2f}\")\n",
    "\n",
    "    if interaction_factor > 1.05:\n",
    "        print(\"→ POSITIVE INTERACTION: Techniques amplify each other beyond expectation.\")\n",
    "    elif interaction_factor < 0.95:\n",
    "        print(\"→ NEGATIVE INTERACTION: Techniques do not fully combine, but still improve over baseline.\")\n",
    "    else:\n",
    "        print(\"→ NEUTRAL INTERACTION: Effects are roughly independent.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nOne or more benchmarks failed, cannot compute interaction analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why 1+1 ≠ 2 in AI optimization**: You likely observed that the architecture and hardware gains did not simply multiply. \n",
    "> \n",
    "> This is because optimizations are not independent variables and, following [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law), the combined effect is different than _architecture_gain × hardware_gain_. Individual techniques create specific performance improvements, but their combination depends on how they interact at the computational level. \n",
    "> - Positive interactions occur when one optimization creates opportunities for another (e.g., parameter reduction enabling better cache utilization)\n",
    "> - Negative interactions happen when optimizations conflict (e.g., dynamic logic defeating static compilation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "> #### **TODO: Test your strategic optimization mindset** \n",
    "> \n",
    "> Add your answers to these analysis questions in the space below.\n",
    "> \n",
    "> 1. **Analyze the bottleneck**: Based on the individual gains, which optimization (architectural or hardware) was more effective for DenseNet121 on its own? Why do you think that was the primary bottleneck for this model?\n",
    "> \n",
    "> _Add your answer here_:  __________________\n",
    "> \n",
    "> 2. **Explain the optimizations' interaction**: Describe the interaction effect you observed (positive, negative, or neutral). Was it expected?\n",
    "> \n",
    "> _Add your answer here_:  __________________\n",
    "> \n",
    "> 3. **Propose a final combination**: Imagine your goal was purely maximum throughput, and accuracy could drop slightly. Based on the options in Step 3, propose your final combination of architectural and hardware optimizations that you hypothesize would create a positive interaction (synergy). Justify your choice.\n",
    "> \n",
    "> _Add your answer here_:  __________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have now implemented a combined hardware-architecture optimization strategy, measuring their interaction effects with the DenseNet121 model on GPU. \n",
    "\n",
    "**Key takeaway**: _Optimizations are not composable by default_. Simply applying the best architectural trick and the best hardware tool does not guarantee multiplicative benefits. Effective performance engineering requires a holistic approach, considering how the model’s computational patterns and the hardware’s capabilities interact—sometimes synergistically, sometimes destructively.\n",
    "\n",
    "##### **Next challenge -> Explore other architectures**\n",
    "\n",
    "How would these same techniques apply to a different architecture, such as a Vision Transformer (ViT)? ViTs are dominated by large, dense matrix multiplications rather than sequential convolutions and concatenations. This structural difference may lead to very different interactions with mixed precision, kernel fusion, and architectural optimizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
