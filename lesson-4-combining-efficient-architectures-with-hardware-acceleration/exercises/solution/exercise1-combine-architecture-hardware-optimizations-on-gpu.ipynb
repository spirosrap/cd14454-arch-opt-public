{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Combine architecture and hardware optimizations for GPU acceleration\n",
    "\n",
    "You have learned about hardware-aware architecture design and hardware acceleration tools in theory. Now it's time to apply these concepts by combining architectural modifications with hardware optimizations to discover how they interact in practice.\n",
    "\n",
    "**Overview:** Explore how architectural choices and GPU acceleration can interact in complex ways, requiring careful coordination rather than simple stacking of optimizations to unlock expected performance gains.\n",
    "\n",
    "**Scenario:** You work for a visual content moderation platform that processes millions of images in real-time. Although your model delivers strong accuracy, its current throughput falls far short of peak demand: you need to nearly 10x performance to keep up. To close the gap, you turn to both TensorRT-based hardware acceleration and architectural adjustments, especially since the DevOps team reports highly unstable GPU utilization (25–60%), suggesting inefficiencies in how the model and hardware interact.\n",
    "\n",
    "**Goal:** Apply at least one architectural modification and one hardware acceleration technique to DenseNet121, then measure their interaction to see that the combined gains are not strictly additive and may have a positive or a negative effect.\n",
    "\n",
    "**Tools:** PyTorch, TensorRT, ONNX, CUDA tools\n",
    "\n",
    "**Estimated Time:** 20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Let's establish our baseline environment and verify T4 capabilities for integrated optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out and restart notebook\n",
    "# ! pip install torchinfo tensorrt onnx onnxruntime-gpu cuda-python datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "import tensorrt as trt\n",
    "import onnx\n",
    "from cuda import cudart\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"assets/exercise1\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla T4\n",
      "Compute Capability: 7.5\n",
      "Total Memory: 15.6 GB\n",
      "Multiprocessors: 40\n",
      "Memory Bandwidth: ~320 GB/s (theoretical)\n",
      "Tensor Core Support: ✓ Available\n",
      "  → Mixed precision (FP16) will show significant speedup\n",
      "  → Kernel fusion opportunities available\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check T4 GPU capabilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    \n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "    print(f\"Total Memory: {gpu_properties.total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Multiprocessors: {gpu_properties.multi_processor_count}\")\n",
    "    print(f\"Memory Bandwidth: ~320 GB/s (theoretical)\")\n",
    "    \n",
    "    # Check for Tensor Core support (Compute Capability >= 7.0)\n",
    "    tensor_cores_available = gpu_properties.major >= 7\n",
    "    print(f\"Tensor Core Support: {'✓ Available' if tensor_cores_available else '✗ Not Available'}\")\n",
    "    \n",
    "    if tensor_cores_available:\n",
    "        print(\"  → Mixed precision (FP16) will show significant speedup\")\n",
    "        print(\"  → Kernel fusion opportunities available\")\n",
    "        \n",
    "    print(\"Setup complete!\")\n",
    "else:\n",
    "    print(\"CUDA not available - exercise requires GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **T4 hardware context:** T4 GPUs feature 2,560 CUDA cores, 320 Tensor Cores, and 320 GB/s memory bandwidth. The Tensor Cores are specifically designed to accelerate mixed precision (FP16) operations, but their effectiveness depends on the types of operations in your model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create benchmark dataset and model\n",
    "\n",
    "Let's establish our baseline DenseNet121 model and prepare consistent benchmarking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark dataset created: 1000 samples\n",
      "DataLoaders created for batch sizes: [16, 32, 64]\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic ImageNet-like data for consistent benchmarking\n",
    "def create_benchmark_dataset(num_samples=1000, image_size=224):\n",
    "    \"\"\"\n",
    "    Create synthetic dataset for controlled benchmarking\n",
    "    \n",
    "    Args:\n",
    "        num_samples (int): Number of samples to generate\n",
    "        image_size (int): Size of square images (224x224 for ImageNet)\n",
    "    \n",
    "    Returns:\n",
    "        TensorDataset: Synthetic images and labels for benchmarking\n",
    "    \"\"\"\n",
    "    # Generate random images with ImageNet statistics\n",
    "    images = torch.randn(num_samples, 3, image_size, image_size)\n",
    "    labels = torch.randint(0, 1000, (num_samples,))\n",
    "    \n",
    "    dataset = TensorDataset(images, labels)\n",
    "    return dataset\n",
    "\n",
    "# Create benchmark dataset and dataloaders\n",
    "benchmark_dataset = create_benchmark_dataset()\n",
    "print(f\"Benchmark dataset created: {len(benchmark_dataset)} samples\")\n",
    "\n",
    "batch_sizes = [16, 32, 64]\n",
    "dataloaders = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    dataloaders[batch_size] = DataLoader(\n",
    "        benchmark_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        pin_memory=True  # Enables faster GPU transfer\n",
    "    )\n",
    "\n",
    "print(f\"DataLoaders created for batch sizes: {batch_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Synthetic dataset considerations:** Using synthetic data eliminates dataset loading and preprocessing bottlenecks, allowing us to isolate and measure the pure model optimization effects. Real-world deployments would show additional complexities from data pipeline optimization, but this controlled approach reveals the core architectural and hardware interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DenseNet                                 [1, 1000]                 --\n",
       "├─Sequential: 1-1                        [1, 1024, 7, 7]           --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 112, 112]         9,408\n",
       "│    └─BatchNorm2d: 2-2                  [1, 64, 112, 112]         128\n",
       "│    └─ReLU: 2-3                         [1, 64, 112, 112]         --\n",
       "│    └─MaxPool2d: 2-4                    [1, 64, 56, 56]           --\n",
       "│    └─_DenseBlock: 2-5                  [1, 256, 56, 56]          --\n",
       "│    │    └─_DenseLayer: 3-1             [1, 32, 56, 56]           45,440\n",
       "│    │    └─_DenseLayer: 3-2             [1, 32, 56, 56]           49,600\n",
       "│    │    └─_DenseLayer: 3-3             [1, 32, 56, 56]           53,760\n",
       "│    │    └─_DenseLayer: 3-4             [1, 32, 56, 56]           57,920\n",
       "│    │    └─_DenseLayer: 3-5             [1, 32, 56, 56]           62,080\n",
       "│    │    └─_DenseLayer: 3-6             [1, 32, 56, 56]           66,240\n",
       "│    └─_Transition: 2-6                  [1, 128, 28, 28]          --\n",
       "│    │    └─BatchNorm2d: 3-7             [1, 256, 56, 56]          512\n",
       "│    │    └─ReLU: 3-8                    [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-9                  [1, 128, 56, 56]          32,768\n",
       "│    │    └─AvgPool2d: 3-10              [1, 128, 28, 28]          --\n",
       "│    └─_DenseBlock: 2-7                  [1, 512, 28, 28]          --\n",
       "│    │    └─_DenseLayer: 3-11            [1, 32, 28, 28]           53,760\n",
       "│    │    └─_DenseLayer: 3-12            [1, 32, 28, 28]           57,920\n",
       "│    │    └─_DenseLayer: 3-13            [1, 32, 28, 28]           62,080\n",
       "│    │    └─_DenseLayer: 3-14            [1, 32, 28, 28]           66,240\n",
       "│    │    └─_DenseLayer: 3-15            [1, 32, 28, 28]           70,400\n",
       "│    │    └─_DenseLayer: 3-16            [1, 32, 28, 28]           74,560\n",
       "│    │    └─_DenseLayer: 3-17            [1, 32, 28, 28]           78,720\n",
       "│    │    └─_DenseLayer: 3-18            [1, 32, 28, 28]           82,880\n",
       "│    │    └─_DenseLayer: 3-19            [1, 32, 28, 28]           87,040\n",
       "│    │    └─_DenseLayer: 3-20            [1, 32, 28, 28]           91,200\n",
       "│    │    └─_DenseLayer: 3-21            [1, 32, 28, 28]           95,360\n",
       "│    │    └─_DenseLayer: 3-22            [1, 32, 28, 28]           99,520\n",
       "│    └─_Transition: 2-8                  [1, 256, 14, 14]          --\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 512, 28, 28]          1,024\n",
       "│    │    └─ReLU: 3-24                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-25                 [1, 256, 28, 28]          131,072\n",
       "│    │    └─AvgPool2d: 3-26              [1, 256, 14, 14]          --\n",
       "│    └─_DenseBlock: 2-9                  [1, 1024, 14, 14]         --\n",
       "│    │    └─_DenseLayer: 3-27            [1, 32, 14, 14]           70,400\n",
       "│    │    └─_DenseLayer: 3-28            [1, 32, 14, 14]           74,560\n",
       "│    │    └─_DenseLayer: 3-29            [1, 32, 14, 14]           78,720\n",
       "│    │    └─_DenseLayer: 3-30            [1, 32, 14, 14]           82,880\n",
       "│    │    └─_DenseLayer: 3-31            [1, 32, 14, 14]           87,040\n",
       "│    │    └─_DenseLayer: 3-32            [1, 32, 14, 14]           91,200\n",
       "│    │    └─_DenseLayer: 3-33            [1, 32, 14, 14]           95,360\n",
       "│    │    └─_DenseLayer: 3-34            [1, 32, 14, 14]           99,520\n",
       "│    │    └─_DenseLayer: 3-35            [1, 32, 14, 14]           103,680\n",
       "│    │    └─_DenseLayer: 3-36            [1, 32, 14, 14]           107,840\n",
       "│    │    └─_DenseLayer: 3-37            [1, 32, 14, 14]           112,000\n",
       "│    │    └─_DenseLayer: 3-38            [1, 32, 14, 14]           116,160\n",
       "│    │    └─_DenseLayer: 3-39            [1, 32, 14, 14]           120,320\n",
       "│    │    └─_DenseLayer: 3-40            [1, 32, 14, 14]           124,480\n",
       "│    │    └─_DenseLayer: 3-41            [1, 32, 14, 14]           128,640\n",
       "│    │    └─_DenseLayer: 3-42            [1, 32, 14, 14]           132,800\n",
       "│    │    └─_DenseLayer: 3-43            [1, 32, 14, 14]           136,960\n",
       "│    │    └─_DenseLayer: 3-44            [1, 32, 14, 14]           141,120\n",
       "│    │    └─_DenseLayer: 3-45            [1, 32, 14, 14]           145,280\n",
       "│    │    └─_DenseLayer: 3-46            [1, 32, 14, 14]           149,440\n",
       "│    │    └─_DenseLayer: 3-47            [1, 32, 14, 14]           153,600\n",
       "│    │    └─_DenseLayer: 3-48            [1, 32, 14, 14]           157,760\n",
       "│    │    └─_DenseLayer: 3-49            [1, 32, 14, 14]           161,920\n",
       "│    │    └─_DenseLayer: 3-50            [1, 32, 14, 14]           166,080\n",
       "│    └─_Transition: 2-10                 [1, 512, 7, 7]            --\n",
       "│    │    └─BatchNorm2d: 3-51            [1, 1024, 14, 14]         2,048\n",
       "│    │    └─ReLU: 3-52                   [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-53                 [1, 512, 14, 14]          524,288\n",
       "│    │    └─AvgPool2d: 3-54              [1, 512, 7, 7]            --\n",
       "│    └─_DenseBlock: 2-11                 [1, 1024, 7, 7]           --\n",
       "│    │    └─_DenseLayer: 3-55            [1, 32, 7, 7]             103,680\n",
       "│    │    └─_DenseLayer: 3-56            [1, 32, 7, 7]             107,840\n",
       "│    │    └─_DenseLayer: 3-57            [1, 32, 7, 7]             112,000\n",
       "│    │    └─_DenseLayer: 3-58            [1, 32, 7, 7]             116,160\n",
       "│    │    └─_DenseLayer: 3-59            [1, 32, 7, 7]             120,320\n",
       "│    │    └─_DenseLayer: 3-60            [1, 32, 7, 7]             124,480\n",
       "│    │    └─_DenseLayer: 3-61            [1, 32, 7, 7]             128,640\n",
       "│    │    └─_DenseLayer: 3-62            [1, 32, 7, 7]             132,800\n",
       "│    │    └─_DenseLayer: 3-63            [1, 32, 7, 7]             136,960\n",
       "│    │    └─_DenseLayer: 3-64            [1, 32, 7, 7]             141,120\n",
       "│    │    └─_DenseLayer: 3-65            [1, 32, 7, 7]             145,280\n",
       "│    │    └─_DenseLayer: 3-66            [1, 32, 7, 7]             149,440\n",
       "│    │    └─_DenseLayer: 3-67            [1, 32, 7, 7]             153,600\n",
       "│    │    └─_DenseLayer: 3-68            [1, 32, 7, 7]             157,760\n",
       "│    │    └─_DenseLayer: 3-69            [1, 32, 7, 7]             161,920\n",
       "│    │    └─_DenseLayer: 3-70            [1, 32, 7, 7]             166,080\n",
       "│    └─BatchNorm2d: 2-12                 [1, 1024, 7, 7]           2,048\n",
       "├─Linear: 1-2                            [1, 1000]                 1,025,000\n",
       "==========================================================================================\n",
       "Total params: 7,978,856\n",
       "Trainable params: 7,978,856\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.83\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 180.54\n",
       "Params size (MB): 31.92\n",
       "Estimated Total Size (MB): 213.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create baseline DenseNet121 model\n",
    "baseline_model = models.densenet121()\n",
    "baseline_model = baseline_model.to(device)\n",
    "baseline_model.eval()\n",
    "\n",
    "# Get model information\n",
    "summary(baseline_model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Understanding DenseNet's model:** The DenseNet model is based on *dense connectivity*, where each layer receives inputs from ALL preceding layers within a block, creating progressively larger feature maps through concatenation rather than element-wise addition. This leads to this memory usage characteristics:\n",
    "> \n",
    "> - *Memory growth* - feature maps grow linearly with depth (growth_rate × num_layers), making later layers process significantly more channels than earlier ones. \n",
    "> - *Memory efficiency trade-offs* - while concatenation enables better gradient flow and feature reuse, it creates higher memory bandwidth requirements and different GPU utilization patterns. \n",
    "> \n",
    "> This baseline analysis can help you understand how architectural modifications interact with DenseNet's inherent dense connectivity when combined with hardware acceleration techniques. You can find even more details about DenseNet in the original [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993) paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Choose your hardware<>architecture optimization path\n",
    "\n",
    "To optimize the model on GPU, you need to implement at least **one architectural modification** and **one hardware acceleration technique**, then measure their interaction effects. Choose your path!\n",
    "\n",
    "### Architecture modification options (Choose >=1):\n",
    "\n",
    "**Option A: Early Exit Architecture**\n",
    "- Add intermediate classifier after dense block 3 for confident predictions\n",
    "- Reduces average computation by skipping expensive final block for easy samples\n",
    "- Adaptive computation based on prediction confidence\n",
    "\n",
    "**Option B: Reduced Dense Layers**  \n",
    "- Decrease growth rate from 32 to 24 channels per layer\n",
    "- Reduces memory bandwidth and computation while maintaining architecture\n",
    "- Direct efficiency improvement through less feature concatenation\n",
    "\n",
    "**Option C: Low-Rank Classifier Factorization**\n",
    "- Decompose final classifier using SVD (7M→2M parameters)\n",
    "- Maintains dense matrix operations while reducing computational load\n",
    "- Preserves patterns that work well with hardware acceleration\n",
    "\n",
    "**Option D: Grouped Convolutions**\n",
    "- Replace standard 1×1 bottleneck convolutions with grouped versions\n",
    "- Reduces FLOPs while maintaining regular convolution patterns\n",
    "- Better hardware utilization than depthwise separable approaches\n",
    "\n",
    "**Option E: Structured Channel Pruning**\n",
    "- Remove least important channels from dense block connections\n",
    "- Creates structured sparsity that maintains computational efficiency\n",
    "- Reduces both parameters and memory bandwidth requirements\n",
    "\n",
    "### Hardware acceleration options (Choose >=1):\n",
    "\n",
    "**Option A: Mixed Precision (FP16) Optimization**\n",
    "- Leverages Tensor Cores for automatic FP16/FP32 selection\n",
    "- Accelerates compatible operations while maintaining numerical stability\n",
    "- Most effective with dense matrix operations\n",
    "\n",
    "**Option B: Dynamic Batching Optimization**\n",
    "- Optimizes batch processing and memory utilization\n",
    "- Changes GPU occupancy patterns and memory coalescing\n",
    "- Can reveal memory bandwidth vs compute trade-offs\n",
    "\n",
    "**Option C: Kernel Fusion + Graph Optimization**  \n",
    "- Combines multiple operations into single GPU kernels\n",
    "- Reduces memory bandwidth requirements\n",
    "- Effectiveness depends on operation compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO: Summarize your optimization strategy**\n",
    "> \n",
    "> Now that you've made your strategic choice on the combination of architecture and hardware optimizations to implement, briefly explain your reasoning _(1-2 sentences)_\n",
    "> <br>HINT: Consider DenseNet121's characteristics when it comes to memory usage, compute complexity, and fusion opportunities.\n",
    "> \n",
    "> _Add your answer here_: \n",
    "> \n",
    "> **_A: COMBO WITH POSITIVE EFFECTS_**: **Low-Rank Classifier Factorization _(Architecture)_ + Mixed Precision _(Hardware)_** is an ideal optimization combo:\n",
    ">   - *Low-Rank Factorization* replaces the model's final, large classifier layer (`nn.Linear`) with two smaller sequential matrix multiplications, maintaining the model’s mathematical structure while reducing computation\n",
    ">   - *Mixed Precision (FP16)* uses FP16 where possible, effectively halving memory usage and compute per operation, which can roughly double GPU throughput while maintaining model accuracy\n",
    ">   - *Reduced Dense Layers (optional)* leverages the possible accuracy margin to further lower memory bandwidth and computation, enhancing the benefits of early exits and kernel fusion\n",
    "> <br><br>Tensor Cores benefit from low-rank factorization because they efficiently multiply smaller matrices, achieving maximum performance when combined with FP16.\n",
    "> \n",
    "> **_B: COMBO WITH NEGATIVE EFFECTS_**: **Early Exit Architecture _(Architecture)_ + Kernel Fusion _(Hardware)_** could unlock adaptive computation with hardware-level operation fusion:\n",
    "> - *Early Exit Architecture* reduces average computation per sample by allowing easy inputs to bypass expensive final blocks, which is useful for DenseNet’s progressive channel growth.\n",
    "> - *Kernel Fusion + Graph Optimization* improves GPU utilization by fusing consecutive operations\n",
    "> <br><br>In practice, this combination does not unlock significant throughput gains on DenseNet121. Can you think why before looking at the solution below?\n",
    "> \n",
    "> **⚠️ IMPORTANT!** This exercise implements the “negative effect” combo to illustrate how architectural and hardware optimizations can interact in non-obvious ways. <br>_For implementations of low-rank factorization, mixed precision, and other optimizations, please refer to the other exercises in the course._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement architecture optimization(s)\n",
    "\n",
    "Now it's time to translate your chosen architectural strategy into code. \n",
    "\n",
    "This step involves modifying the model's structure in PyTorch to change how it computes results. By altering the model's forward pass or its layers, you can introduce efficiencies like adaptive computation or reduced parameter counts, directly impacting its performance profile before any hardware-specific compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating optimized DenseNet...\n",
      "Sample batch exit behavior: early_exit\n",
      "\n",
      "Parameter reduction vs. baseline: -1,025,000 (-12.8%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "EarlyExitDenseNet                        [1, 1000]                 --\n",
       "├─Sequential: 1-1                        [1, 1024, 14, 14]         --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 112, 112]         9,408\n",
       "│    └─BatchNorm2d: 2-2                  [1, 64, 112, 112]         128\n",
       "│    └─ReLU: 2-3                         [1, 64, 112, 112]         --\n",
       "│    └─MaxPool2d: 2-4                    [1, 64, 56, 56]           --\n",
       "│    └─_DenseBlock: 2-5                  [1, 256, 56, 56]          --\n",
       "│    │    └─_DenseLayer: 3-1             [1, 32, 56, 56]           45,440\n",
       "│    │    └─_DenseLayer: 3-2             [1, 32, 56, 56]           49,600\n",
       "│    │    └─_DenseLayer: 3-3             [1, 32, 56, 56]           53,760\n",
       "│    │    └─_DenseLayer: 3-4             [1, 32, 56, 56]           57,920\n",
       "│    │    └─_DenseLayer: 3-5             [1, 32, 56, 56]           62,080\n",
       "│    │    └─_DenseLayer: 3-6             [1, 32, 56, 56]           66,240\n",
       "│    └─_Transition: 2-6                  [1, 128, 28, 28]          --\n",
       "│    │    └─BatchNorm2d: 3-7             [1, 256, 56, 56]          512\n",
       "│    │    └─ReLU: 3-8                    [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-9                  [1, 128, 56, 56]          32,768\n",
       "│    │    └─AvgPool2d: 3-10              [1, 128, 28, 28]          --\n",
       "│    └─_DenseBlock: 2-7                  [1, 512, 28, 28]          --\n",
       "│    │    └─_DenseLayer: 3-11            [1, 32, 28, 28]           53,760\n",
       "│    │    └─_DenseLayer: 3-12            [1, 32, 28, 28]           57,920\n",
       "│    │    └─_DenseLayer: 3-13            [1, 32, 28, 28]           62,080\n",
       "│    │    └─_DenseLayer: 3-14            [1, 32, 28, 28]           66,240\n",
       "│    │    └─_DenseLayer: 3-15            [1, 32, 28, 28]           70,400\n",
       "│    │    └─_DenseLayer: 3-16            [1, 32, 28, 28]           74,560\n",
       "│    │    └─_DenseLayer: 3-17            [1, 32, 28, 28]           78,720\n",
       "│    │    └─_DenseLayer: 3-18            [1, 32, 28, 28]           82,880\n",
       "│    │    └─_DenseLayer: 3-19            [1, 32, 28, 28]           87,040\n",
       "│    │    └─_DenseLayer: 3-20            [1, 32, 28, 28]           91,200\n",
       "│    │    └─_DenseLayer: 3-21            [1, 32, 28, 28]           95,360\n",
       "│    │    └─_DenseLayer: 3-22            [1, 32, 28, 28]           99,520\n",
       "│    └─_Transition: 2-8                  [1, 256, 14, 14]          --\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 512, 28, 28]          1,024\n",
       "│    │    └─ReLU: 3-24                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-25                 [1, 256, 28, 28]          131,072\n",
       "│    │    └─AvgPool2d: 3-26              [1, 256, 14, 14]          --\n",
       "│    └─_DenseBlock: 2-9                  [1, 1024, 14, 14]         --\n",
       "│    │    └─_DenseLayer: 3-27            [1, 32, 14, 14]           70,400\n",
       "│    │    └─_DenseLayer: 3-28            [1, 32, 14, 14]           74,560\n",
       "│    │    └─_DenseLayer: 3-29            [1, 32, 14, 14]           78,720\n",
       "│    │    └─_DenseLayer: 3-30            [1, 32, 14, 14]           82,880\n",
       "│    │    └─_DenseLayer: 3-31            [1, 32, 14, 14]           87,040\n",
       "│    │    └─_DenseLayer: 3-32            [1, 32, 14, 14]           91,200\n",
       "│    │    └─_DenseLayer: 3-33            [1, 32, 14, 14]           95,360\n",
       "│    │    └─_DenseLayer: 3-34            [1, 32, 14, 14]           99,520\n",
       "│    │    └─_DenseLayer: 3-35            [1, 32, 14, 14]           103,680\n",
       "│    │    └─_DenseLayer: 3-36            [1, 32, 14, 14]           107,840\n",
       "│    │    └─_DenseLayer: 3-37            [1, 32, 14, 14]           112,000\n",
       "│    │    └─_DenseLayer: 3-38            [1, 32, 14, 14]           116,160\n",
       "│    │    └─_DenseLayer: 3-39            [1, 32, 14, 14]           120,320\n",
       "│    │    └─_DenseLayer: 3-40            [1, 32, 14, 14]           124,480\n",
       "│    │    └─_DenseLayer: 3-41            [1, 32, 14, 14]           128,640\n",
       "│    │    └─_DenseLayer: 3-42            [1, 32, 14, 14]           132,800\n",
       "│    │    └─_DenseLayer: 3-43            [1, 32, 14, 14]           136,960\n",
       "│    │    └─_DenseLayer: 3-44            [1, 32, 14, 14]           141,120\n",
       "│    │    └─_DenseLayer: 3-45            [1, 32, 14, 14]           145,280\n",
       "│    │    └─_DenseLayer: 3-46            [1, 32, 14, 14]           149,440\n",
       "│    │    └─_DenseLayer: 3-47            [1, 32, 14, 14]           153,600\n",
       "│    │    └─_DenseLayer: 3-48            [1, 32, 14, 14]           157,760\n",
       "│    │    └─_DenseLayer: 3-49            [1, 32, 14, 14]           161,920\n",
       "│    │    └─_DenseLayer: 3-50            [1, 32, 14, 14]           166,080\n",
       "├─Sequential: 1-2                        [1, 1000]                 --\n",
       "│    └─AdaptiveAvgPool2d: 2-10           [1, 1024, 1, 1]           --\n",
       "│    └─Flatten: 2-11                     [1, 1024]                 --\n",
       "│    └─Linear: 2-12                      [1, 1000]                 1,025,000\n",
       "├─Sequential: 1-3                        [1, 1024, 7, 7]           --\n",
       "│    └─_Transition: 2-13                 [1, 512, 7, 7]            --\n",
       "│    │    └─BatchNorm2d: 3-51            [1, 1024, 14, 14]         2,048\n",
       "│    │    └─ReLU: 3-52                   [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-53                 [1, 512, 14, 14]          524,288\n",
       "│    │    └─AvgPool2d: 3-54              [1, 512, 7, 7]            --\n",
       "│    └─_DenseBlock: 2-14                 [1, 1024, 7, 7]           --\n",
       "│    │    └─_DenseLayer: 3-55            [1, 32, 7, 7]             103,680\n",
       "│    │    └─_DenseLayer: 3-56            [1, 32, 7, 7]             107,840\n",
       "│    │    └─_DenseLayer: 3-57            [1, 32, 7, 7]             112,000\n",
       "│    │    └─_DenseLayer: 3-58            [1, 32, 7, 7]             116,160\n",
       "│    │    └─_DenseLayer: 3-59            [1, 32, 7, 7]             120,320\n",
       "│    │    └─_DenseLayer: 3-60            [1, 32, 7, 7]             124,480\n",
       "│    │    └─_DenseLayer: 3-61            [1, 32, 7, 7]             128,640\n",
       "│    │    └─_DenseLayer: 3-62            [1, 32, 7, 7]             132,800\n",
       "│    │    └─_DenseLayer: 3-63            [1, 32, 7, 7]             136,960\n",
       "│    │    └─_DenseLayer: 3-64            [1, 32, 7, 7]             141,120\n",
       "│    │    └─_DenseLayer: 3-65            [1, 32, 7, 7]             145,280\n",
       "│    │    └─_DenseLayer: 3-66            [1, 32, 7, 7]             149,440\n",
       "│    │    └─_DenseLayer: 3-67            [1, 32, 7, 7]             153,600\n",
       "│    │    └─_DenseLayer: 3-68            [1, 32, 7, 7]             157,760\n",
       "│    │    └─_DenseLayer: 3-69            [1, 32, 7, 7]             161,920\n",
       "│    │    └─_DenseLayer: 3-70            [1, 32, 7, 7]             166,080\n",
       "│    └─BatchNorm2d: 2-15                 [1, 1024, 7, 7]           2,048\n",
       "├─Sequential: 1-4                        [1, 1000]                 --\n",
       "│    └─AdaptiveAvgPool2d: 2-16           [1, 1024, 1, 1]           --\n",
       "│    └─Flatten: 2-17                     [1, 1024]                 --\n",
       "│    └─Linear: 2-18                      [1, 1000]                 1,025,000\n",
       "==========================================================================================\n",
       "Total params: 9,003,856\n",
       "Trainable params: 9,003,856\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.84\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 180.55\n",
       "Params size (MB): 36.02\n",
       "Estimated Total Size (MB): 217.17\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EarlyExitDenseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    DenseNet121 with early exit capability and reduced dense layers.\n",
    "    \n",
    "    Architectural modifications:\n",
    "    1. Early exit after dense block 3 for confident predictions\n",
    "    2. Slightly reduced growth rate in dense layers to lower computation and memory\n",
    "    \"\"\"\n",
    "\n",
    "    import torch.nn.utils.prune as prune\n",
    "    from torch.linalg import svd\n",
    "\n",
    "    def __init__(self, base_densenet, confidence_threshold=0.85):\n",
    "        super(EarlyExitDenseNet, self).__init__()\n",
    "        \n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Split DenseNet features for early exit capability\n",
    "        # Note that DenseNet121 has features organized as [conv0, norm0, relu0, pool0, denseblock1, transition1, denseblock2, transition2, denseblock3, transition3, denseblock4, norm5]\n",
    "        # You can split after denseblock3 (index 8) to create early exit point after most computation is done\n",
    "        # Reference: https://pytorch.org/vision/stable/models.html#densenet\n",
    "        features = list(base_densenet.features.children())\n",
    "        self.early_features = nn.Sequential(*features[:9])  # Through denseblock3 and transition3\n",
    "        self.late_features = nn.Sequential(*features[9:])   # denseblock4 and final norm\n",
    "\n",
    "        # Create intermediate classifier for early exit\n",
    "        self.early_classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 1000)\n",
    "        )\n",
    "\n",
    "        # Copy pretrained classifier weights to early classifier\n",
    "        with torch.no_grad():\n",
    "            self.early_classifier[2].weight.copy_(base_densenet.classifier.weight)\n",
    "            self.early_classifier[2].bias.copy_(base_densenet.classifier.bias)\n",
    "        \n",
    "        # Final classifier (keep standard DenseNet classifier, optionally reduced)\n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 1000)\n",
    "        )\n",
    "        \n",
    "        # Copy pretrained classifier weights to final classifier\n",
    "        with torch.no_grad():\n",
    "            self.final_classifier[2].weight.copy_(base_densenet.classifier.weight)\n",
    "            self.final_classifier[2].bias.copy_(base_densenet.classifier.bias)\n",
    "        \n",
    "    def forward(self, x, training=False):\n",
    "        # Extract early features\n",
    "        early_features = self.early_features(x)\n",
    "\n",
    "        # Generate early predictions and check confidence\n",
    "\n",
    "        # If max confidence exceeds threshold and not training, return early prediction\n",
    "        # Reference: https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html\n",
    "        early_logits = self.early_classifier(early_features)\n",
    "        if not training and not self.training:\n",
    "            confidence = F.softmax(early_logits, dim=1).max(dim=1)[0]\n",
    "\n",
    "            if confidence.mean() > self.confidence_threshold:\n",
    "                return early_logits, 'early_exit'\n",
    "\n",
    "        # Continue to final layers for uncertain predictions\n",
    "        final_features = self.late_features(early_features)\n",
    "        final_logits = self.final_classifier(final_features)\n",
    "\n",
    "        return final_logits, 'full_forward'\n",
    "\n",
    "def create_optimized_densenet(base_model, device):\n",
    "    \"\"\"Create DenseNet with optimizations\"\"\"\n",
    "\n",
    "    # TODO: Create your optimization logic as you wish (modularization is recommended), and define the optimized model in the optimized_model variable\n",
    "    # HINT: You can refer to the exercises in lesson 2 for some implementations\n",
    "    # Or find inspiration at discuss.pytorch.org\n",
    "    optimized_model = EarlyExitDenseNet(\n",
    "        baseline_model, \n",
    "        confidence_threshold=0.01  # Setting this to a very low confidence for demonstration for demo; in practice, threshold is typically in [0.5, 0.9]\n",
    "    )  # Add your code here\n",
    "\n",
    "    optimized_model = optimized_model.to(device)\n",
    "    optimized_model.eval()\n",
    "\n",
    "    # Test early exit behavior\n",
    "    sample_batch = next(iter(dataloaders[16]))\n",
    "    sample_input = sample_batch[0].to(device)\n",
    "    with torch.no_grad():\n",
    "        output, exit_type = optimized_model(sample_input)\n",
    "        print(f\"Sample batch exit behavior: {exit_type}\")\n",
    "    \n",
    "    return optimized_model\n",
    "\n",
    "# Create architecture-optimized model\n",
    "print(\"Creating optimized DenseNet...\")\n",
    "arch_model = create_optimized_densenet(baseline_model, device)\n",
    "\n",
    "# Get optimized model information\n",
    "arch_reduction = sum(p.numel() for p in baseline_model.parameters()) - sum(p.numel() for p in arch_model.parameters())\n",
    "print(f\"\\nParameter reduction vs. baseline: {arch_reduction:,} ({arch_reduction/sum(p.numel() for p in baseline_model.parameters())*100:.1f}%)\")\n",
    "summary(arch_model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **On aligning architecture with hardware**: Every architectural modification changes the model's computational pattern in a specific way, whether through parameter reduction, operation reordering, or conditional execution. \n",
    "> \n",
    "> The key is understanding how these changes will interact with your target hardware's strengths and limitations. Some architectural patterns align naturally with GPU acceleration (dense matrix operations), while others introduce complexity that static compilers cannot optimize (dynamic branching). Measuring the individual architectural effect first provides a baseline to understand interaction effects later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement hardware acceleration with TensorRT\n",
    "\n",
    "With the architecture modified, you can now turn to hardware acceleration. \n",
    "\n",
    "This step involves converting our high-level PyTorch model into a format that a specialized compiler, [TensorRT](https://developer.nvidia.com/tensorrt), can optimize. [ONNX (Open Neural Network Exchange)](https://onnx.ai/) acts as a bridge: TensorRT takes this ONNX graph and applies powerful GPU-specific optimizations.\n",
    "\n",
    "**Remember that building the TRT engine could take up to a few minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's export the model to the ONNX format preferred by TensorRT\n",
    "def export_to_onnx(model, sample_input, onnx_path):\n",
    "    \"\"\"\n",
    "    Export PyTorch model to ONNX format for TensorRT optimization\n",
    "    \n",
    "    ONNX (Open Neural Network Exchange) is an intermediate representation\n",
    "    that TensorRT uses to analyze and optimize model architectures.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Export model to ONNX format\n",
    "        # TODO: Make sure the export supports chosen optimizations, if applicable parameters exist\n",
    "        # HINT: How does ONNX handle batches? Should you set up fp16 precision here or in TensorRT?\n",
    "        # Reference: https://docs.pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.export\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            sample_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=17,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output']\n",
    "        )\n",
    "\n",
    "        # Verify ONNX model\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(f\"ONNX export successful: {onnx_path}\")\n",
    "        print(f\"   Total nodes: {len(onnx_model.graph.node)}\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ONNX export failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting models to ONNX...\n",
      "ONNX export successful: assets/exercise1/baseline_densenet121.onnx\n",
      "   Total nodes: 617\n",
      "ONNX export successful: assets/exercise1/optimized_densenet121.onnx\n",
      "   Total nodes: 428\n",
      "\n",
      "Optimizing with TensorRT...\n",
      "TensorRT engine saved: assets/exercise1/baseline_densenet_fused.engine\n",
      "TensorRT engine saved: assets/exercise1/optimized_densenet_fused.engine\n"
     ]
    }
   ],
   "source": [
    "# Secondly, let's create the optimized TensorRT engine \n",
    "def optimize_with_tensorrt(onnx_path, engine_path):\n",
    "    \"\"\"\n",
    "    Convert ONNX model to TensorRT engine with optional optimizations\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create TensorRT logger and builder\n",
    "        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "        builder = trt.Builder(TRT_LOGGER)\n",
    "        config = builder.create_builder_config()\n",
    "\n",
    "        # TODO: Implement your TensorRT optimizations\n",
    "        # HINT: You can refer to the exercise 1 in lesson 3 for an example implementation\n",
    "        # Reference: https://developer.nvidia.com/docs/drive/drive-os/6.0.7/public/drive-os-tensorrt/api-reference/docs/python/infer/Core/BuilderConfig.html#tensorrt.IBuilderConfig\n",
    "\n",
    "        # Add your code here\n",
    "\n",
    "        # 1. Configure memory pool for aggressive optimization\n",
    "        # Why? Larger workspace enables more fusion opportunities\n",
    "        # Set to 4GB for maximum fusion potential on T4\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)\n",
    "        \n",
    "        # 2. Kernel fusion is enabled by default - nothing to do!\n",
    "        \n",
    "        # Parse ONNX model\n",
    "        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "        parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "        \n",
    "        with open(onnx_path, 'rb') as model_file:\n",
    "            if not parser.parse(model_file.read()):\n",
    "                print(\"Failed to parse ONNX model\")\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return False\n",
    "\n",
    "        # Build optimized engine\n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "        \n",
    "        if serialized_engine is None:\n",
    "            print(\"Failed to build TensorRT engine\")\n",
    "            return False\n",
    "            \n",
    "        # Save optimized engine\n",
    "        with open(engine_path, 'wb') as f:\n",
    "            f.write(serialized_engine)\n",
    "            \n",
    "        print(f\"TensorRT engine saved: {engine_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"TensorRT optimization failed: {e}\")\n",
    "\n",
    "# Export models to ONNX format\n",
    "sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "baseline_onnx_path = os.path.join(output_dir, \"baseline_densenet121.onnx\")\n",
    "arch_onnx_path = os.path.join(output_dir, \"optimized_densenet121.onnx\")\n",
    "\n",
    "print(\"Exporting models to ONNX...\")\n",
    "baseline_onnx_success = export_to_onnx(baseline_model, sample_input, baseline_onnx_path)\n",
    "arch_onnx_success = export_to_onnx(arch_model, sample_input, arch_onnx_path)\n",
    "\n",
    "onnx.checker.check_model(baseline_onnx_path)\n",
    "onnx.checker.check_model(arch_onnx_path)\n",
    "\n",
    "# Optimize with TensorRT\n",
    "baseline_engine_path = os.path.join(output_dir, \"baseline_densenet_fused.engine\")\n",
    "arch_engine_path = os.path.join(output_dir, \"optimized_densenet_fused.engine\")\n",
    "\n",
    "if baseline_onnx_success and arch_onnx_success:\n",
    "    print(\"\\nOptimizing with TensorRT...\")\n",
    "    baseline_trt_success = optimize_with_tensorrt(\n",
    "        baseline_onnx_path, \n",
    "        baseline_engine_path\n",
    "    )\n",
    "    arch_trt_success = optimize_with_tensorrt(\n",
    "        arch_onnx_path,\n",
    "        arch_engine_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Understanding ONNX export and TensorRT compilation patterns**: \n",
    "> 1. The _ONNX model_ is not guaranteed to have a lower total node count. Early exit architectures, parameter sharing, or layer removal can reduce the graph complexity that ONNX captures. However, some optimizations like attention mechanisms or complex branching might actually increase node counts. \n",
    "> \n",
    "> 2. _TensorRT's compilation_ process typically runs quietly by default, but you might see warnings about unsupported operations, precision conversions, or optimization choices. Some of these warnings can be ignored (e.g., layer fusion opportunities taken) while others require investigation (e.g., unsupported operation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Perform systematic performance benchmarking\n",
    "\n",
    "Now let's measure the impact. \n",
    "\n",
    "This systematic approach lets us measure individual technique effects, combined effects, and quantify whether optimizations amplify each other, conflict, or remain independent.\n",
    "\n",
    "A systematic benchmark is crucial to understand not just if our changes made the model faster, but why. You will compare four configurations to isolate the effects of each optimization and, most importantly, to reveal how they interact when combined. _Does 1+1 equal 2, or do you see an unexpected result?_\n",
    "\n",
    "⎯⎯⎯⎯⎯⎯⎯\n",
    "\n",
    "You only have **one TODO in step 6.2** to prepare the inputs for inference. <ins>If you want to scroll quickly through the rest of the code</ins>, here's a high-level summary:\n",
    "\n",
    "- *6.1*: Build the TensorRT inference class for consistent benchmarking\n",
    "- *6.2*: Create a fair comparison methodology across PyTorch and TensorRT models  \n",
    "- *6.3*: Execute the core experiment and calculate optimizations' interaction effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Create the TensorRT inference class\n",
    "\n",
    "To run systematic comparisons, you need a unified interface for benchmarking TensorRT engines. This helper class handles the low-level GPU memory management and inference execution, allowing us to focus on measuring performance rather than implementation details.\n",
    "\n",
    "**Note:** For this exercise, we use synchronous CUDA operations (`execute_v2`, `cudaMemcpy`) rather than async variants to ensure accurate timing measurements. Async operations would improve throughput but make it harder to isolate individual inference latency for benchmarking purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class for running inference with a TensorRT engine\n",
    "class TensorRTInfer:\n",
    "    def __init__(self, engine_path):\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        with open(engine_path, \"rb\") as f, trt.Runtime(self.logger) as runtime:\n",
    "            self.engine = runtime.deserialize_cuda_engine(f.read())\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "        # Assume 1 input, 1 output\n",
    "        self.in_name = [self.engine.get_tensor_name(i)\n",
    "                        for i in range(self.engine.num_io_tensors)\n",
    "                        if self.engine.get_tensor_mode(self.engine.get_tensor_name(i)) == trt.TensorIOMode.INPUT][0]\n",
    "        self.out_name = [self.engine.get_tensor_name(i)\n",
    "                         for i in range(self.engine.num_io_tensors)\n",
    "                         if self.engine.get_tensor_mode(self.engine.get_tensor_name(i)) == trt.TensorIOMode.OUTPUT][0]\n",
    "\n",
    "        # Match engine dtypes to numpy\n",
    "        self.np_in_dtype = np.float16 if self.engine.get_tensor_dtype(self.in_name) == trt.float16 else np.float32\n",
    "        self.np_out_dtype = np.float16 if self.engine.get_tensor_dtype(self.out_name) == trt.float16 else np.float32\n",
    "\n",
    "    def infer(self, input_tensor: np.ndarray):\n",
    "        # Ensure correct dtype and contiguous memory\n",
    "        inp = np.ascontiguousarray(input_tensor).astype(self.np_in_dtype)\n",
    "\n",
    "        # Handle dynamic shapes\n",
    "        if -1 in self.engine.get_tensor_shape(self.in_name):\n",
    "            self.context.set_input_shape(self.in_name, inp.shape)\n",
    "\n",
    "        # Get concrete output shape\n",
    "        out_shape = tuple(self.context.get_tensor_shape(self.out_name))\n",
    "\n",
    "        # Allocate device memory\n",
    "        in_bytes = inp.nbytes\n",
    "        out_bytes = np.prod(out_shape) * np.dtype(self.np_out_dtype).itemsize\n",
    "\n",
    "        _, d_in = cudart.cudaMalloc(in_bytes)\n",
    "        _, d_out = cudart.cudaMalloc(out_bytes)\n",
    "\n",
    "        # Host output buffer\n",
    "        h_out = np.empty(np.prod(out_shape), dtype=self.np_out_dtype)\n",
    "\n",
    "        # Bind device pointers\n",
    "        self.context.set_tensor_address(self.in_name, int(d_in))\n",
    "        self.context.set_tensor_address(self.out_name, int(d_out))\n",
    "\n",
    "        # H2D copy\n",
    "        cudart.cudaMemcpy(d_in, inp.ctypes.data, in_bytes,\n",
    "                          cudart.cudaMemcpyKind.cudaMemcpyHostToDevice)\n",
    "        \n",
    "        # Prepare bindings array\n",
    "        bindings = [int(d_in), int(d_out)]  # device pointers in the order of engine bindings\n",
    "\n",
    "        # Run synchronously\n",
    "        if not self.context.execute_v2(bindings):\n",
    "            raise RuntimeError(\"TensorRT inference failed\")\n",
    "\n",
    "        # D2H copy\n",
    "        cudart.cudaMemcpy(h_out.ctypes.data, d_out, out_bytes,\n",
    "                          cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost)\n",
    "\n",
    "        # Free device memory\n",
    "        cudart.cudaFree(d_in)\n",
    "        cudart.cudaFree(d_out)\n",
    "\n",
    "        return h_out.reshape(out_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Define benchmarking logic for each model\n",
    "\n",
    "With our infrastructure in place, you can now create consistent benchmarking functions that can handle both PyTorch and TensorRT models. The key challenge is ensuring fair comparisons by controlling for differences in input preprocessing, warmup procedures, and timing methodology across all four configurations you'll test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_inputs(inputs, framework=\"pytorch\", variant=\"baseline\", device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Prepare inputs depending on framework and optimization variant.\n",
    "    \n",
    "    Args:\n",
    "        inputs: batch from dataloader\n",
    "        framework: \"pytorch\" or \"tensorrt\"\n",
    "        variant: \"baseline\" or \"optimized\" (e.g., fp16)\n",
    "        device: \"cuda\" or \"cpu\"\n",
    "    \"\"\"\n",
    "    # TODO: Define your input\n",
    "    # HINT: Your baseline and optimized models may expected different formats depending on the chosen optimization\n",
    "    # Don't forget to place the input of the expected device too\n",
    "\n",
    "    # Add your code here\n",
    "\n",
    "    if framework == \"pytorch\":\n",
    "        inputs = inputs.to(device)\n",
    "        if variant == \"optimized\":  # e.g., fp16 model\n",
    "            inputs = inputs.half()\n",
    "\n",
    "    elif framework == \"tensorrt\":\n",
    "        inputs = inputs.numpy()\n",
    "        if variant == \"optimized\":\n",
    "            # Example: cast to fp16 numpy for TRT engines\n",
    "            inputs = inputs.astype(\"float16\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown framework: {framework}\")\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def benchmark_trt_engine(engine_path, dataloader, num_batches=20, model_name=\"Model\"):\n",
    "    \"\"\"Benchmark a TensorRT engine.\"\"\"\n",
    "    trt_infer = TensorRTInfer(engine_path)\n",
    "    times = []\n",
    "    \n",
    "    print(f\"Benchmarking {model_name}...\")\n",
    "\n",
    "    model_variant = \"baseline\" if \"baseline\" in model_name.lower() else \"optimized\"\n",
    "    \n",
    "    # Warmup\n",
    "    for i, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = _prepare_inputs(inputs, framework=\"tensorrt\", variant=model_variant)\n",
    "        if i >= 3: break\n",
    "        _ = trt_infer.infer(inputs)\n",
    "\n",
    "    # Benchmark\n",
    "    for i, (inputs, _) in enumerate(dataloader):\n",
    "        if i >= num_batches: break\n",
    "        inputs = _prepare_inputs(inputs, framework=\"tensorrt\", variant=model_variant)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.perf_counter()\n",
    "        _ = trt_infer.infer(inputs)\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        times.append(end_time - start_time)\n",
    "        \n",
    "    return times\n",
    "\n",
    "def benchmark_pytorch_model(model, dataloader, num_batches=20, model_name=\"Model\"):\n",
    "    \"\"\"Benchmark a PyTorch model.\"\"\"\n",
    "    model.eval().to(device)\n",
    "    times = []\n",
    "    \n",
    "    print(f\"Benchmarking {model_name}...\")\n",
    "    \n",
    "    model_variant = \"baseline\" if \"baseline\" in model_name.lower() else \"optimized\"\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(dataloader):   \n",
    "            inputs = _prepare_inputs(inputs, framework=\"pytorch\", variant=model_variant)\n",
    "            if i >= 3: break\n",
    "            _ = model(inputs)\n",
    "\n",
    "    # Benchmark\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            if i >= num_batches: break\n",
    "            inputs = _prepare_inputs(inputs, framework=\"pytorch\", variant=model_variant)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "            _ = model(inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            times.append(end_time - start_time)\n",
    "            \n",
    "    return times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Analyze optimizations' interactions and results\n",
    "\n",
    "This is the core experiment: measuring your different configurations to isolate individual optimization effects and discover how they interact when combined. \n",
    "\n",
    "You'll benchmark baseline PyTorch, architecture-only PyTorch, hardware-only TensorRT, and combined TensorRT to calculate the interaction factor and determine whether techniques amplify, conflict, or remain independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TECHNIQUE INTERACTION ANALYSIS ===\n",
      "Benchmarking Baseline DenseNet (PyTorch)...\n",
      "  Throughput: 365.3 samples/sec\n",
      "\n",
      "Benchmarking Arch-Optimized (PyTorch)...\n",
      "  Throughput: 395.5 samples/sec\n",
      "\n",
      "Benchmarking Baseline DenseNet (TensorRT)...\n",
      "  Throughput: 2320.2 samples/sec\n",
      "\n",
      "Benchmarking Arch-Optimized (TensorRT)...\n",
      "  Throughput: 1445.6 samples/sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_technique_interactions():\n",
    "    \"\"\"Systematic analysis of the effect of optimizations.\"\"\"\n",
    "    print(\"\\n=== TECHNIQUE INTERACTION ANALYSIS ===\")\n",
    "    results = {}\n",
    "    batch_size = 32\n",
    "    dataloader = dataloaders[batch_size]\n",
    "\n",
    "    # --- 1. Baseline PyTorch ---\n",
    "    times = benchmark_pytorch_model(baseline_model, dataloader, model_name=\"Baseline DenseNet (PyTorch)\")\n",
    "    results['baseline'] = {'throughput': batch_size / np.mean(times), 'name': \"Baseline (PyTorch)\"}\n",
    "    print(f\"  Throughput: {results['baseline']['throughput']:.1f} samples/sec\\n\")\n",
    "\n",
    "    # --- 2. Architecture-Optimized PyTorch ---\n",
    "    print(\"Benchmarking Arch-Optimized (PyTorch)...\")\n",
    "    total_samples = 0\n",
    "    arch_times = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            if i >= 15: break\n",
    "            inputs = inputs.to(device)\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "            _, exit_type = arch_model(inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            arch_times.append(end_time - start_time)\n",
    "            total_samples += inputs.size(0)\n",
    "    results['architecture_only'] = {'throughput': batch_size / np.mean(arch_times), 'name': \"Arch-Optimized (PyTorch)\"}\n",
    "    print(f\"  Throughput: {results['architecture_only']['throughput']:.1f} samples/sec\\n\")\n",
    "\n",
    "    # --- 3. Hardware-Optimized TensorRT ---\n",
    "    if baseline_trt_success:\n",
    "        times = benchmark_trt_engine(baseline_engine_path, dataloader, model_name=\"Baseline DenseNet (TensorRT)\")\n",
    "        results['hardware_only'] = {'throughput': batch_size / np.mean(times), 'name': \"Baseline (TensorRT)\"}\n",
    "        print(f\"  Throughput: {results['hardware_only']['throughput']:.1f} samples/sec\\n\")\n",
    "    else:\n",
    "        results['hardware_only'] = {'throughput': 0, 'name': \"Baseline (TensorRT)\"}\n",
    "\n",
    "    # --- 4. Combined Optimization TensorRT ---\n",
    "    if arch_trt_success:\n",
    "        times = benchmark_trt_engine(arch_engine_path, dataloader, model_name=\"Arch-Optimized (TensorRT)\")\n",
    "        results['combined'] = {'throughput': batch_size / np.mean(times), 'name': \"Combined (TensorRT)\"}\n",
    "        print(f\"  Throughput: {results['combined']['throughput']:.1f} samples/sec\\n\")\n",
    "    else:\n",
    "        results['combined'] = {'throughput': 0, 'name': \"Combined (TensorRT)\"}\n",
    "\n",
    "    return results\n",
    "\n",
    "interaction_results = analyze_technique_interactions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INTEGRATION ANALYSIS ===\n",
      "INDIVIDUAL TECHNIQUE EFFECTS:\n",
      "Architecture optimization: 1.08x throughput\n",
      "Hardware optimization:     6.35x throughput\n",
      "\n",
      "INTEGRATION EFFECTS:\n",
      "Theoretical combined improvement (if independent): 6.87x over baseline\n",
      "Actual combined improvement:                       3.96x over baseline\n",
      "Interaction factor: 0.58\n",
      "→ NEGATIVE INTERACTION: Techniques do not fully combine, but still improve over baseline.\n"
     ]
    }
   ],
   "source": [
    "# Print analysis results\n",
    "baseline = interaction_results['baseline']\n",
    "hardware_only = interaction_results.get('hardware_only', {'throughput': 0})\n",
    "architecture_only = interaction_results['architecture_only']\n",
    "combined = interaction_results.get('combined', {'throughput': 0})\n",
    "\n",
    "# Avoid division by zero if a benchmark failed\n",
    "if baseline['throughput'] > 0 and hardware_only['throughput'] > 0 and combined['throughput'] > 0:\n",
    "    architecture_gain = architecture_only['throughput'] / baseline['throughput']\n",
    "    hardware_gain = hardware_only['throughput'] / baseline['throughput']\n",
    "    combined_gain = combined['throughput'] / baseline['throughput']\n",
    "\n",
    "    # Analyze interaction effects\n",
    "    theoretical_combined = architecture_gain * hardware_gain\n",
    "    actual_combined = combined_gain\n",
    "    interaction_factor = actual_combined / theoretical_combined\n",
    "\n",
    "    print(\"=== INTEGRATION ANALYSIS ===\")\n",
    "    print(\"INDIVIDUAL TECHNIQUE EFFECTS:\")\n",
    "    print(f\"Architecture optimization: {architecture_gain:.2f}x throughput\")\n",
    "    print(f\"Hardware optimization:     {hardware_gain:.2f}x throughput\")\n",
    "    print()\n",
    "\n",
    "    print(\"INTEGRATION EFFECTS:\")\n",
    "    print(f\"Theoretical combined improvement (if independent): {theoretical_combined:.2f}x over baseline\")\n",
    "    print(f\"Actual combined improvement:                       {actual_combined:.2f}x over baseline\")\n",
    "    print(f\"Interaction factor: {interaction_factor:.2f}\")\n",
    "\n",
    "    if interaction_factor > 1.05:\n",
    "        print(\"→ POSITIVE INTERACTION: Techniques amplify each other beyond expectation.\")\n",
    "    elif interaction_factor < 0.95:\n",
    "        print(\"→ NEGATIVE INTERACTION: Techniques do not fully combine, but still improve over baseline.\")\n",
    "    else:\n",
    "        print(\"→ NEUTRAL INTERACTION: Effects are roughly independent.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nOne or more benchmarks failed, cannot compute interaction analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why 1+1 ≠ 2 in AI optimization**: You likely observed that the architecture and hardware gains did not simply multiply. \n",
    "> \n",
    "> This is because optimizations are not independent variables and, following [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law), the combined effect is different than _architecture_gain × hardware_gain_. Individual techniques create specific performance improvements, but their combination depends on how they interact at the computational level. \n",
    "> - Positive interactions occur when one optimization creates opportunities for another (e.g., parameter reduction enabling better cache utilization)\n",
    "> - Negative interactions happen when optimizations conflict (e.g., dynamic logic defeating static compilation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "> #### **TODO: Test your strategic optimization mindset** \n",
    "> \n",
    "> Add your answers to these analysis questions in the space below.\n",
    "> \n",
    "> 1. **Analyze the bottleneck**: Based on the individual gains, which optimization (architectural or hardware) was more effective for DenseNet121 on its own? Why do you think that was the primary bottleneck for this model?\n",
    "> \n",
    "> _Add your answer here_: Typically, TensorRT's kernel fusion provides a larger individual gain. This suggests that for DenseNet121, a major bottleneck is memory bandwidth, not just raw computation (FLOPs). DenseNet's architecture involves many small, sequential operations (concatenations, 1x1 convolutions, BatchNorm, ReLU) that lead to frequent reads/writes from GPU memory. Kernel fusion directly addresses this by merging these operations, reducing memory traffic and keeping the compute cores fed more effectively.\n",
    "> \n",
    "> 2. **Explain the optimizations' interaction**: Describe the interaction effect you observed (positive, negative, or neutral). Was it expected?\n",
    "> \n",
    "> _Add your answer here_: We observed a negative interaction. The Early Exit architecture introduces dynamic, data-dependent branching in Python. However, TensorRT is a static compiler that traces the model once to create a fixed, optimized computation graph. It could not capture the `if...else` logic and therefore only optimized the full, worst-case execution path. As a result, when we benchmarked the combined TensorRT engine, we lost the primary benefit of the architectural change (skipping computation), leading to a combined gain that was significantly less than the theoretical product of the individual gains.\n",
    "> \n",
    "> 3. **Propose a final combination**: Imagine your goal was purely maximum throughput, and accuracy could drop slightly. Based on the options in Step 3, propose your final combination of architectural and hardware optimizations that you hypothesize would create a positive interaction (synergy). Justify your choice.\n",
    "> \n",
    "> _Add your answer here_: A potentially synergistic combination would be Low-Rank Classifier Factorization (C) + Mixed Precision (A). Low-rank factorization replaces the final large matrix multiplication with two smaller ones. This creates dense, regular computations that are ideal for acceleration by the GPU's Tensor Cores, which are activated by mixed precision (FP16). The architectural change (factorization) creates work that is perfectly suited for the hardware optimization (Tensor Cores via FP16), likely resulting in a performance gain greater than the sum of their parts—a positive interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have now implemented a combined hardware-architecture optimization strategy, measuring their interaction effects with the DenseNet121 model on GPU. \n",
    "\n",
    "**Key takeaway**: _Optimizations are not composable by default_. Simply applying the best architectural trick and the best hardware tool does not guarantee multiplicative benefits. Effective performance engineering requires a holistic approach, considering how the model’s computational patterns and the hardware’s capabilities interact—sometimes synergistically, sometimes destructively.\n",
    "\n",
    "##### **Next challenge -> Explore other architectures**\n",
    "\n",
    "How would these same techniques apply to a different architecture, such as a Vision Transformer (ViT)? ViTs are dominated by large, dense matrix multiplications rather than sequential convolutions and concatenations. This structural difference may lead to very different interactions with mixed precision, kernel fusion, and architectural optimizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
